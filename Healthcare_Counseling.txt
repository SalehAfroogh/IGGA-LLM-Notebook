Should health systems regulate the use of ChatGPT?

As hospitals and health systems begin to experiment and pilot generative AI such as ChatGPT, many CIOs and IT leaders said CIOs must be the ones to develop policies around appropriate use cases and must evaluate frameworks and regulations to stay on top of industry standards on generative AI. 

Becker's asked four IT leaders: Should health systems begin to regulate the use of ChatGPT?

Robert Eardley. CIO of University Hospitals (Cleveland): Generative AI capabilities such as ChatGPT that are embedded into core computer applications should follow a governance process to other net-new technologies. 

Organizations should acknowledge the importance that any "drafted" responses are subject to human review and approval. Most important early on is to be aware of and inventory the use of any generative AI explorations within the organization. Automated responses based upon generative AI should be tightly reviewed and approved for use within that context. Automated generative AI capabilities should undergo a stringent review process for accuracy in ways that other technologies have been embedded into an organization’s workflow over time (such as drug-drug interaction alerts to prevent medication errors). 

Darrell Bodnar. CIO of North Country Healthcare (Whitefield, N.H.): I think that CIOs must take a position and develop policies around the appropriate use of ChatGPT and all AI language models and services. 

North Country Healthcare has already taken a position and provided a detailed policy and framework for the use and adoption of ChatGPT "like" models. 

Guidelines and regulated access need to be documented around all use, including clinical and nonclinical scenarios and the potential to share [protected health information] strictly defined. There also needs to be consideration for relying upon any such services for guidance and decision-making and the liabilities that accompany the process. 

There is no doubt that AI language models like ChatGPT are going to revolutionize the way we all work. The potential benefits in healthcare at a time when labor markets are stretched so thin in every service and vertical are tremendous; we just need to proceed with the same caution we embrace any new technology.

Sunil Dadlani. Executive Vice President and Chief Information and Digital Transformation Officer of Atlantic Health System (Morristown, N.J.): The use of ChatGPT and similar technologies is rapidly expanding. But before widely adopting them across a healthcare organization, there are several steps CIOs must take to ensure they are integrated in productive and secure ways.

Safely adopting any new technology is tied directly to a solid understanding of the regulatory landscape, particularly when it comes to the governing rules around data security including HIPAA and GDPR.  Additionally because healthcare technology environments are interconnected, leaders must have full visibility into the access and usage agreements in place with third-party vendors and others to ensure data protection. When possible, try to avoid the danger of "building the car while driving it" and have proper policies and guidelines governing the use of generative artificial intelligence and machine learning technology in place. 

Educating team members about the capabilities and risks associated with these technologies will help ensure they are used properly. Always be ready to conduct risk assessments and perform continuous monitoring and evaluations. 

Maintaining cross-functional internal and external partnerships will also help you gain those insights more comprehensively. 

Finally, make sure you are staying current with industry advancements, regulations, compliance and ethical frameworks. This field is evolving quickly, and staying a step ahead will help avoid mistakes in the future.

Paul Conocenti. CIO of Montage Health (Monterey, Calif.): At Montage Health, we are closely monitoring this new technology. We are cautiously optimistic of its value and equally concerned about its misuse and accuracy. Before moving any technology into our production environment, the use case must be validated and monitored for accuracy and security compliance — ChatGPT is no exception

2. OpenAI and ChatGPT: A Primer for Healthcare Leaders 

Russell Group principles on the use of generative AI tools in education Our universities are committed to the ethical and responsible use of generative AI and to preparing our staff and students to be leaders in an increasingly AI-enabled world. The rise of generative artificial intelligence (AI) has the potential for a profound impact on the ways in which we teach, learn, assess, and access education. Our universities wish to ensure that generative AI tools can be used for the benefit of students and staff – enhancing teaching practices and student learning experiences, ensuring students develop skills for the future within an ethical framework, and enabling educators to benefit from efficiencies to develop innovative methods of teaching. Valuable work undertaken by organisations such as the Quality Assurance Agency for Higher Education (QAA) and Jisc has helped develop the sector’s understanding of the opportunities and considerations of generative AI 12 , and the Department for Education (DfE) has set out its position on the use of generative AI in the pre-university education sector 3 . Russell Group universities have contributed sector-wide insight and have been proactively working with experts to revise and develop policies that provide guidance to students and staff. Collaboration, coordination, and consistency on this issue across the education and professional sectors – including professional bodies, schools, FE colleges and employers – will be crucial. In recognition of this, Russell Group universities have collectively developed the following principles that will guide the approach to generative AI tools across our universities and, we hope, beyond: 1. Universities will support students and staff to become AI-literate. 2. Staff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience. 3. Universities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access. 4. Universities will ensure academic rigour and integrity is upheld. 5. Universities will work collaboratively to share best practice as the technology and its application in education evolves. 1. Universities will support students and staff to become AI-literate. 1.1 Generative AI tools are capable of processing vast amounts of information to generate responses but they have significant limitations. It is important that all students and staff understand the opportunities, limitations and ethical issues associated with the use of these tools and can apply what they have learned as the capabilities of generative AI develop. These include: (a) Privacy and data considerations: whether a generative AI tool is designed to learn directly from its users’ inputs or not, there are risks to privacy and intellectual property associated with the information that students and staff may enter. (b) Potential for bias: generative AI tools produce answers based on information generated by humans which may contain societal biases and stereotypes which, in-turn, may be replicated in the generative AI tool’s response.

(c) Inaccuracy and misinterpretation of information: data and information contained within generative AI tools is garnered from a wide range of sources, including those that are poorly referenced or incorrect. Similarly, unclear commands or information may be misinterpreted by generative AI tools and produce incorrect, irrelevant or out-of-date information. This means that accountability for the accuracy of information generated by these tools when transferred to another context lies with the user. (d) Ethics codes: users of generative AI tools should be aware that while ethics codes exist, they may not be embedded within all generative AI tools and that their incorporation, or otherwise, may not be something that users can easily verify. (e) Plagiarism: generative AI tools re-present information developed by others and so there is the risk of plagiarised content and/or copyright infringement being submitted by a user as their own, and artwork used by image generators may have been included without the creator’s consent or licence. (f) Exploitation: the process by which generative AI tools are built can present ethical issues. For example, some developers have outsourced data labelling to low-wage workers in poor conditions4 . 1.2 Our universities will provide guidance and training to help students and staff understand how generative AI tools work, where they can add value and personalise learning, as well as their limitations. By increasing AI-literacy, our universities will equip students with the skills needed to use these tools appropriately throughout their studies and future careers, and ensure staff have the necessary skills and knowledge to deploy these tools to support student learning and adapt teaching pedagogies. 2. Staff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience. 2.1 Our universities will develop resources and training opportunities, so that staff are able to provide students with clear guidance on how to use generative AI to support their learning, assignments, and research. 2.2 The appropriate uses of generative AI tools are likely to differ between academic disciplines and will be informed by policies and guidance from subject associations, therefore universities will encourage academic departments to apply institution-wide policies within their own context. Universities will also be encouraged to consider how these tools might be applied appropriately for different student groups or those with specific learning needs. 2.3 Engagement and dialogue between academic staff and students will be important to establish a shared understanding of the appropriate use of generative AI tools. Ensuring this dialogue is regular and ongoing will be vital given the pace at which generative AI is evolving. 3. Universities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access. 3.1 Universities continually update and enhance their pedagogies and assessment methods in response to drivers including new research, technological developments and workforce needs – adapting to the use of generative AI technology is no different. Incorporating the use of generative AI tools into teaching methods and assessments has the potential to enhance

the student learning experience, improve critical reasoning skills and prepare students for the real-world applications of the generative AI technologies they will encounter beyond university. 3.2 Appropriate adaptations to teaching and assessment methods will vary by university and discipline, and protecting this autonomy is vital. All staff who support student learning should be empowered to design teaching sessions, materials and assessments that incorporate the creative use of generative AI tools where appropriate. Professional bodies will also have an important role in supporting universities to adapt their practices, particularly in relation to accreditation. 3.3 As the technologies develop and new generative tools become available, elements of generative AI used within universities may reside behind paywalls or be restricted to paying subscribers. Universities will need to consider how best to respond to a potential proliferation of such subscription tools and attempt to ensure fairness of access so that students and staff can access the generative AI tools and computing resources they need in support of their teaching and learning practices. 4. Universities will ensure academic rigour and integrity is upheld. 4.1 All 24 Russell Group universities have reviewed their academic conduct policies and guidance to reflect the emergence of generative AI. These policies make it clear to students and staff where the use generative AI is inappropriate, and are intended to support them in making informed decisions and to empower them to use these tools appropriately and acknowledge their use where necessary. 4.2 Such clear and transparent policies are critical to maintaining consistent and high standards of learning, teaching and assessment across Russell Group universities. 4.3 Ensuring academic integrity and the ethical use of generative AI can also be achieved by cultivating an environment where students can ask questions about specific cases of their use and discuss the associated challenges openly and without fear of penalisation. 5. Universities will work collaboratively to share best practice as the technology and its application in education evolves. 5.1 Navigating this ever-changing landscape will require collaboration between universities, students, schools, FE colleges, employers, sector and professional bodies, with the ongoing review and evaluation of policies, principles and their practical implementation. 5.2 Our universities will regularly evaluate policies and guidance for staff and students relating to generative AI tools and their impact on teaching, learning, and assessment practices. This will include monitoring the effectiveness, fairness, and ethical implications of the integration of generative AI tools into academic life, and adapting policies and procedures to ensure they remain valid as generative AI technologies evolve. 5.3 Fostering relationships between higher education institutions, schools, employers, professional bodies who accredit degrees, AI experts, leading academics and researchers, as well as ensuring an inter-disciplinary approach to addressing emerging challenges and promoting the ethical use of generative AI, will be crucial. Russell Group universities recognise the challenges that lie ahead and will continue to value the input of others, along with contributing expertise to the national and international discussions around generative AI and its applications within teaching, learning, assessment and support.

3. Chugai DX Meeting

1. Progress in CHUGAI DIGITAL (Satoko Shisai: Executive Vice President, Head of Digital Transformation Unit) 2. Three Initiatives to Strengthen the Digital Platform (Keisuke Ohara, Head of IT Solution Department) (Kazumitsu Kanatani, Head of Digital Strategy Department) (1) Multicloud strategy/Cyber security strategy (2) Initiative toward utilization of generative AI (3) Initiative on healthcare × Web 3.0 3. Status of Progress in the Insight Business (Dr. Nobuya Ishii, Head of Science & Technology Intelligence Department)

Top Innovator 2030  Realization of Chugai’s “Envisioned Future” in 2030 With world-class drug discovery capabilities, patients around the world expect that “Chugai will surely create new treatments." Attract passionate talent from all over the world, and inspire players in globally to think they can create something new by partnering with Chugai Recognized for its ESG initiatives through its business activities, Chugai will become a global role model as a leader in resolving social issues Our definition of “Top Innovator in the healthcare industry” In collaboration with Roche, we will continue to place “innovative new drugs” at the core of our business, while aiming to become a leading innovator in the global healthcare field, where a diverse range of players, not limited to pharmaceutical companies, are taking on the challenge of innovation.

Growth Strategy to Become a Top Innovator 2030 *RED: Research & Early Development Expansion of existing technological bases and building a new technological foundation to materialize unique drug discovery ideas Accelerating innovation opportunities by strengthening collaboration with leading global players and leveraging digital technologies Launch in-house global products every year by doubling R&D output Dramatic improvement in product / patient value by restructuring business model, having digital utilization as a core Improve productivity of entire value chain by leveraging digital technologies Commercialization of insight business with the aim of maximizing the value of pharmaceuticals and having a new business pillar Key Drivers ▶ DX ▶ RED SHIFT ▶ Open Innovation “Double R&D output” & “Launch global in-house products every year”

Reform Chugai’s business through digital technology and become a top innovator that provides healthcare solutions that transform society Transform our business by using digital technologies to make Chugai a top innovator in the provision of society-changing healthcare solutions Society-changing healthcare solutions  Provide optimal personalized healthcare suited to individuals.  Produce high QoL throughout life through ultra-early diagnosis, prevention, and treatment.  Bring about social assurance programs sustainable even in shrinking and aging societies. Transform our business  Provide innovative drug products continuously by leveraging digital technologies.  Greatly streamline all value chains.  Create frameworks for providing innovative services.  Change employee awareness and organizational structure and customs at Chugai.

Key Achievements So Far (1) 7 DX for drug discovery and development  Accelerate drug discovery research DX by utilizing AI, robotics, etc.  Select candidate antibodies for development through the use of AI technology in generating antibody molecular sequences and optimization (MALEXA)  Improve molecular design and screening methods for small/mid-size molecules through AI technology  Improve productivity in pathology through the use of image analysis technology. Use quantitative evaluation to perform integrated analyses  Develop robots to support work in complex experiments  Promotion in development of Lab Automation System and digital infrastructure  Promote development of digital biomarkers  Enter into new partnership agreement with Biofourmis for the continued development and practical clinical application of digital solutions to objectively evaluate pain in patients with endometriosis  Promote utilization of RWD in filing of regulatory application and internal decision-making  Filing of regulatory application: Use of RWD as evaluation data and reference data in the filing package for HER2-positive colorectal cancer  Utilization in internal decision-making: to survey actual treatment status of diseases, investigate prognosis predictions, and consider clinical study design, etc.

Key Achievements So Far (2) 8 Optimize all value chains  Promoting initiatives towards smart factories  Equip Ukima Plant with the following functions. (1) Automation and visualization of work plan drafting (2) Efficient assignment and utilization of human resources throughout the entire plant (3) Use of smartphones to enable remote support and tamper-proof image recording tools  Promote deployment to Fujieda Plant and Utsunomiya Plant  Promote updating of customer engagement model  Support sales, safety, and MA activities with a comprehensive platform that integrates customer databases and information on various solutions, and a decision-making support engine that utilizes AI. Accelerate improvement of operation results  Digitization of clinical trials: Promotion of DCTs  Start decentralized clinical trials (DCTs) incorporating visiting nursing and telemedicine into ordinary clinical trials in the US  Promote Reconsider Productive Approach (RPA) efforts  Initially aimed at reducing workload by 100,000 hours by 2023, but achieved 150,000 hours workload reduction by the end of 2022

Key Achievements So Far (3) 9 Strengthening the digital platform  Operation of the Chugai Cloud Infrastructure (CCI)  Unify the data provision functions in a multicloud environment (AWS/Azure/Google Cloud, etc.) to promote standardization, strengthen security governance, and achieve efficient integrated operations management  While using AWS as the main platform, use Google for AI/machine learning, etc.  Cyber Security  Formulate a cyber security vision and clarify issues to enhance cyder security and countermeasures. In addition to raising the overall level of security, various risk-based initiatives will be implemented, and security checks and security monitoring of business partners will be enhanced.  Acceleration of external collaborations and open innovation  Establishment of CVC: Begin approach to digital AI technology to support drug discovery and translational research  Digital Innovation Lab: Promoted more than 450 ideas and 80 PoCs in 3 years, and shifted more than 20 projects to actual development  Innovation Pitch (C-DIP): A pitch event will be held on 9/22 (scheduled to be held within the year in October and December) with the aim of promoting collaboration with start-up companies and revitalizing the entire industry

Key Achievements So Far (4) 10 Strengthening the digital platform  Launch & promote ASPIRE project  Introduction of state-of-the-art global standard processes and next-generation core business infrastructure (ERP), and promotion of company-wide operational process and organizational reforms  Promotion of data strategy  Accelerate the establishment of a governance/control system whereby anyone who needs to use data can easily find and obtain that data thanks to the establishment of an environment in which the desired analysis can be performed conveniently, together with the FAIR + culture/system  Launch of Web 3.0 initiatives  In October 2022, our company’s philosophy on Web 3.0 was announced as a Point of View. We will aim for new innovations by utilizing “DAO,” “DID,” “NFT/FT,” etc., based on blockchain technology  Promotion of utilization of generative AI  ChatGPT: Company-wide use started in August after various risk evaluations and guidelines were formulated  Acceleration of utilization for more advanced operations and promotion of evaluation of various types of generative AI

DX Brand: Selected as Grand Prix and Platinum Company  The only pharmaceutical company selected for 4 consecutive years since 2020  In 2022, Chugai was selected to receive the “DX Grand Prix” as “a company that leads the digital era in a manner that transcends the framework of its industry”  In 2023, Chugai was selected for the “DX Platinum Company 2023-2025” as a company that has continued to pursue outstanding DX initiatives since the inception of the system

1. Progress in CHUGAI DIGITAL (Satoko Shisai: Executive Vice President, Head of Digital Transformation Unit) 2. Three Initiatives to Strengthen the Digital Platform (Keisuke Ohara, Head of IT Solution Department) (Kazumitsu Kanatani, Head of Digital Strategy Department) (1) Multicloud strategy/Cyber security strategy (2) Initiative toward utilization of generative AI (3) Initiative on healthcare × Web 3.0 3. Status of Progress in the Insight Business (Nobuya Ishii, Head of Science & Technology Intelligence Department) 	

Cyber Security Vision 14 CHUGAI CYBER SECURITY VISION 2030 Become a cyber security leader to support the achievement of top innovator status in the healthcare industry  Foster a security culture so that all employees throughout Chugai Global consider “Security First” to be a matter that concerns them personally  Disclose a commitment to security in an outward-facing, proactive manner  Construct a companywide governance system with a robust monitoring/escalation and feedback loop  Construct a system that supports business and enhances security by having business and IT coordinate while retaining high degrees of specialization  Construct a highly secure IT infrastructure base that enables secure use of data  Achieve flexible and robust IT security to accommodate ecosystem expansion and change People/Culture Organizational management Technology  Through internal and external environmental analyses, including CHUGAI DIGITAL VISION 2030, create a "vision and strategy" for cyber security by 2030

Establish a Cyber Security Management System 15  Construct a system to grasp security management information and implement vulnerability management in a timely and comprehensive manner  Assign an officer to act as coordinator of the headquarters and bases and issue instructions and requests related to security  Report the results of security monitoring to the management every quarter and connect them to additional improvement activities Governance layer Management layer Implementation layer Stakeholders (customers, consumers, management, business, etc ...) Formulation of cyber security plans and rules  Reflect plans and rules in business processes, documented procedures, and manuals  Cooperation in operation and implementation of cyber security management measures Reporting Monitoring Evaluation Ongoing monitoring and cyber security assessment Management report Formulation of Security Vision and Mission Formulation of cyber security strategy Report the status of implementation Present plans and rules Instructions to concretize Head office and departments System user System owner Digital Strategy Committee Chairperson: DX Supervisory Officer Deputy Chair: Head of DX Unit IT Solution Department Cyber security management roles  Role of hub between each headquarters/unit and ISOL Dept.  Coordinators for implementation of security measures Governance Cycle Organizational chart of each layer Plant/research base System user System owner Overseas sites Decision-making meetings attended by management Security specialist EUC Promotion C

Approach to Security Issues 16 + • Create rules referring to best practices • Regularly monitor compliance with rules • Activities to correct gaps Baseline approach Risk-based approach • Grasp risks specific to each case and system • Grasp the latest trends in cyber attacks • Implement individual measures for identified risks Raise the overall security level Address residual risk in the baseline approach

Baseline Approach 17  Each site's security level is scored annually against the best practice collection  Elevate the security level of the entire Group by establishing a baseline (security goals for each fiscal year) for plants, research laboratories, and overseas bases and planning and implementing various measures to achieve it

Risk-based Approach 18 Check security risks at the time of system planning and modification (Catch up with cases in the budget drafting process) (1) Conduct risk assessment for information assets (Annual inventory of personal information ledger, occasional risk evaluation of human-derived data, etc.) (2) (3) Identify existential risks through regular penetration tests Continuous acquisition of information on the movements of advanced attackers (Using threat intelligence) (4) Systematically implement individual measures for risks identified at each level Respond appropriately to individual risks that cannot be grasped by the baseline approach (rules) alone  Combine multiple approaches to proactively grasp overall risk  Take appropriate individual actions according to the identified risks in a planned manner

Business Partner Security Initiatives 19  Check response to the security risks of business partners from 2 viewpoints: "security system/maturity level" and "technical vulnerability"  On a trial basis, implement for about 70 of our important business partners Check of security system and maturity level Check of technical vulnerabilities Details of check Frequency of checking • Confirmation of the status of acquisition of certification such as ISMS* • Audit by our company, using checklist At the start of business + Periodically (e.g., annually) • Use of security rating services • Use of threat intelligence services • Use OSINT** to check from the perspective of attackers Occasional checks, as needed (Routine continuous monitoring) Business Partner Security Initiatives (Results) 20  Take action based on the results for "security system/maturity level" and "technical vulnerability"  It will be necessary to establish a system for checking in cooperation with the business unit, risk management unit, procurement unit, and IT unit Upgrading of Security Monitoring 21  Promote to upgrade SOC/SIEM that is central to proactive security response Cyber Security Response System 22  Cooperate and share information within and outside the company in preparation for emergency  In the event of an emergency, promptly set up a task team for early response and take company-wide action with the aim of minimizing damage Future Theme: Digital Service Security 23  It will be necessary to establish a security system that anticipates the provision of digital services to patients and healthcare providers (automatic drug delivery devices, digital biomarkers, smartphone applications, etc.). Why will the Cloud be Necessary? 24 Cost reduction Pay-as-you-go system that does not require an initial investment Elasticity No need for scale capacity forecast dependent on demand Higher security Robust security by specialist organizations Acquisition of various third-party international standards Agility Hundreds of thousands of servers can be deployed in minutes and shut down at any time Global-scale deployment Deploy worldwide in just a few minutes Broad range of functions Can utilize hundreds of state-ofthe-art services, which are constantly evolving 6 + 1 be Why is a Multi-cloud Strategy Necessary? 25 Niche services/ Strengths and weaknesses Measures to simplify operations Mitigation of vendor lock-in and geopolitical risks Solution for increased costs A mixture of public cloud services that are carefully selected allows us to access featured services of the respective cloud vendors and niche or advanced services that are not available from other cloud services. A mixture of public cloud services that are carefully selected can help mitigate or reduce the risks of vendor lock-in. These risks include potential future price increases, service discontinuation by cloud vendors, as well as geopolitical risks that may lead to service outages and delivery delays. Multi-cloud environments do come with disadvantages, such as increased costs from not being eligible for volume discounts. However, by taking advantage of the Roche Group’s blanket policy, we have reduced the costs. Also, by adopting the integrated cost management (FinOps products), we can collectively manage costs across multiple clouds, minimize expenses by adjusting, suspending, or deleting resources while comparing costs among cloud services. Multi-cloud environments are often criticized for their complexity and increased operational burdens. However, when the same vendor handles both the design and operation, it allows for standardized and consistent operations without adding unnecessary complexity. Also, the use of integrated automation management (IaC products) allows us to reduce operational burdens. Leveraging the advantages of multi-cloud Managing the drawbacks of multi-cloud Utilization and Challenges of our Cloud Infrastructure  While we’ve pursued various measures for DX individually, further advancement of DX at an expedited pace should consider aspects like faster environment provisioning, enhanced security, standardized service utilization, and integrated infrastructure. Infrastructure silos Each platform has its own infrastructure operation team resulting in operational inefficiency Environment provisioning has a long lead time Different procedures per domain and individual optimization make the lead time longer and the request process more complex. Inconsistent security levels The platforms are being run individually, and governance is not easily attained due to their inconsistent security levels. Service standardization is difficult It is difficult to standardize the commonly used infrastructure services because of their individualized configuration optimized for each (such as backup, patch, monitoring) Overview of the CCI (Chugai Cloud Infrastructure)  Start building the next-generation Chugai cloud infrastructure (CCI) by utilizing the knowledge of CSI construction  Efficiently integrate the enterprise system platforms that have been configured on third-party cloud environments  Centralize cloud infrastructure features for enhanced standardization and security governance  Continuously improve and extend functionality to keep pace with technological advancements Usage Guidelines for the CCI (Chugai Cloud Infrastructure) Chugai Cloud Infrastructure (CCI) uses AWS as the main cloud platform, with Microsoft Azure and Google Cloud as the sub cloud platforms. Effectiveness of the CCI (Chugai Cloud Infrastructure) Speed (Agility) Governance (Security/Control) Growing operation costs Environment provisioning that used to take one month to complete can be done in 2 to 3 business days Governance and speed achieved together through ‘Detective Control’ Operation cost reduction of 36% through consolidation and automation/self-service Promotion Policy for 2023 31 Introduction of ChatGPT and promotion of its utilization Promotion of utilization of generative AI (external solution) in each unit's operations Generative AI utilization strategy for the creation of new value, and drafting of use cases Direction 1 Direction 2 Direction 3  3 directions to be promoted for the time being ChatGPT Utilization Status 32 Implementation status  Construct the Chugai version of ChatGPT on Azure and implement PoC from May onward, with the objective of investigating use cases and risks  Start company-wide deployment in August after establishing rules and procedures Examples of utilization  Create infrastructure within the internal environment, confirm use cases through trials, and promote company-wide utilization from August onward after formulating guidelines  Identify the following 6 risks and formulate guidelines Risk response Infringement of intellectual property and copyrights Leakage of personal information and confidential data Lack of credibility Biased output Use for unintended purpose Shadow AI Preparation of meeting minutes and action list Creation of various scenarios Questionnaire analysis Proofreading of various regulatory documents UMN initial search Extraction of information on laws and regulations RWD tabulation efficiency Code creation and programming explanation Initial Use Cases 33  Auto-search on scholarly paper databases using specific keywords  Summarize the abstract of paper  Share the abstract along with paper details (such as title, author, sources, abstract, link) Use case details  Automatically generate code in Python or R to refine coding and streamline programming tasks  Input error-containing code in Python or other languages and display suggestions to correct the identified error.  Input code in Python, VBA, or other languages to identify the content and structure  Input survey results, including those from Google Forms, into GPT for analysis, which covers the entire process from comment aggregation/analysis to solution drafting  Based on knowledge compiled internally along with written text for a task, create structured data (e.g., duration, root cause classification, impact, solutions, etc.)  Promote various initiatives for tasks that can be done with Chugai’s ChatGPT  Develop a platform for utilizing internal and external data and encourage diverse usages of data beyond text Retrieval of paper and abstract summary Streamlined programming Analysis of various text data Idea Types Frequently Requested in ChatGPT and Direction of Realization Preparation of draft internal and external explanatory materials and educational materials Create drafts of recruitment guidelines and questions Collection of regulations and trends at other companies Draft proposals for various applications and GxP documents, and proposals for modifications Cognitive searches Preparation of draft meeting minutes Preparation of draft emails Preparation of draft codes Translation/proofreading Summaries of papers Responding to various inquiries (Chatbot) Enable with “Add your data” function Realize by combining ChatGPT with other systems Streamline with ChatGPT and Microsoft Copilot Simplification of SOP search and document preparation Confirmation of how to use systems, equipment, software, etc. Already feasible Handle by DXU + each unit Infrastructure to be considered by DXU  Promote utilization of ideas that can already be implemented  Cognitive searches, etc., where the infrastructure is best considered by the entire company should be executed chiefly by DXU Organizing the Generative AI Development Structure 35 Value Creation/ Optimization Generative AI platform Promotion structure Knowledge acquisition through trial Preparation of guidelines for the generative AI usage and development Establishment of in-house development/ promotion structure Promotion/development support structure Enhanced capability for the development of generative AI Identification and collection of usable internal and external data Configuration of the app development platform Platform for data aggregation, document generation, molecular design and more Research Div. TR Div. Clinical Development Div. Development of generative AI that leads to new value creation Pharmaceutical Technology Div.  Promote the development of generative AI to expedite R&D upon organizing a well-structured developer team and the generative AI platform Our Focus in the Generative AI 36 Time to value 4 drug discovery fields1 Identification of insights and support for decision-making Utilization in drug discovery Mining and utilization of latent knowledge within the company Research TR Clinical Development CMC Quickly streamline operations Comparison and proposal of multiple candidates … … … Entire company Consistency between strategy and assets Other companies can make similar efforts, and these do not necessarily lead to competitive advantage unique to Chugai 1 3 2 Directly accelerate Chugai's unique R&D and contribute to the improvement of competitive superiority Gen AI at present Technically feasible Need to handle advanced knowledge and build Gen AI and peripheral systems  Organize the needs of each unit from the perspective of the technical area that leads to competitive advantage and time to produce results  (1) “Identification of insights and support for decision-making,” (2) “Mining and utilization of latent knowledge within the company,” (3) Improvement of operational efficiency Direction of our Action to Extract Insights and Support Decisionmaking in R&D 37 Research TR Clinical Development CMC Ensuring reusability of past findings Utilization for targeted molecule discovery ・・・ Present structured evidence, regardless of whether it is unstructured RWD or simple charts. Structuring unstructured data ・・・ Proposal of clinical study design and development plan ・・・ Generation of synthetic processes ・・・ Study design assistance Creation of clinical study documentation and FAQ Creation of request form, report and review support  We encourage utilization in R&D from the following perspectives: Changes in the Web 39  Web 3.0 is a new concept of the Internet since 2020. The key point is that data is distributed and managed by each system/individual, and that individuals directly connect with each other without relying on gigantic IT companies Key Elements of Web 3.0 - Blockchain-related elements (FT, NFT/DID/DAO) 40  The main elements of Web 3.0, such as “DAO,” “DID,” and “NFT/FT,” are based on blockchain technolog What is Web 3.0? (Chugai’s Interpretation) 41  Web 3.0 will change ideas about the ideal states of “personal data sovereignty,” “organization/community,” and “‘space’ for value creation,” and realize a world in which “individuals” play the leading role Healthcare Worldview Created by Web 3.0  Through Web 3.0, a world in which each stakeholder in the healthcare field will receive benefits is realized, and the values to be provided in that world can be broadly classified into 4 categories (A to D). Web 3.0 × Healthcare Worldview (realization image)  Web 3.0 × Healthcare worldview will be realized through new use cases created by Web 3.0 Example of Realization Image: Research DAO  Accelerate innovation creation through formation of DAO mainly from collaboration/idea emergence and fair distribution toward the NFT-ization of IPs and contributions Rigorous protection and fair operation of intellectual property • Protection of intellectual property through the utilization of digital evidence • Visualization of contribution activities by token Promotion of diversity in co-creation • Promotion of diversity in research participants • Encouragement of patient/company involvement Issues Incentive/rule design Because the organization operates autonomously, it is unclear where Concern over understanding and handling of responsibility lies when a problem occurs intellectual property related regulations (e.g., balance between work and research) Acceleration of innovation creation • Increase researcher motivation through fair incentives • Insight creation through new collaboration Roadmap for Realizing Chugai's vision  First, we will strengthen our platform, expand applicable use cases by making the value chain more efficient, and ultimately promote the use of Web 3.0 toward crucial “revolutionization of the drug discovery process.” Acquisition of capabilities and creation of initial results Social implementation of a new mode of healthcare Expansion of scope and results Phase 1 (up to 2025) Phase 2 (up to 2030) Phase 3 (2030 and beyond) Relationship to Chugai Vision Innovative new drug creation Optimize value chains Strengthening the digital platform Provide innovative services  Create initial case - DAO (e.g., internal DAO) - Token economy - Metaverse - Manufacturing digital twin  Understand requirements - Biological digital twin Democratization of healthcare data ownership Transformation of collaboration Revolutionize the drug discovery process Construction and Operation of Internal DAO 46 Overview of DAO Concept Name of DAO Objectives A “place ” where each and every employee can play the leading role and demonstrate their personality and identity Is free and open communication promoted? • “Anonymity” unfettered by department or position • “Visual design” that supports the display of individuality and identity Is it possible to display autonomous intent and visualize degree of contribution? • “Voting function” that makes it possible to express one’s own will • “Engagement score” to quantify the actions of individuals, such as posting ideas or commenting Verification of “valuation of ideas” through tokens “NFT-ization” that manages promising ideas through BC • Issuing of “Chugai internal coin (FT)” that assigns value to NFT-ized ideas Concept of Deployment of Internal DAO 47  Aim to dynamize new value creation by making ideas spawned by DAO into projects Overview of LABORN Activities and Feel of Use 48  While enhancing the feel of use, posting ideas on specific themes and discussing them Examples of topics discussed • Because of anonymity, participation without conjecture is possible, and the hurdles for posting are lowered • I felt free to speak up, and it was easy to make a comment at the “just an idea” level • I felt good that I could make proposals beyond the scope of my usual work, and I could indicate that I agreed with other people's proposals. • It was interesting because I was able to post content that I wouldn’t have been able to talk about in my usual work. Visualization of contribution  Calculate individual contribution by engagement score and display ranking on site Next Step 49 Internal DAO (LABORN) External DAO (Development of new services, drug discovery, etc.) Action in FY2023  Company-wide deployment  Continuation of LABORN as new communication space Planning and execution of the first project FT-related policy design (Linking benefits/distribution to individuals, etc.) Formulation of the DAO operation policy (Legal positioning of DAOs and tokens, handling of IP and privacy data, etc.) Organization and structure planning Work requirements definition and design System requirement definition, system design/configuration Seek involvement of community members  Start the first phase of external DAO operation  Design and plan the second phase operation Planning and execution of the second project UI improvements Action in FY2024 and beyond Seek availability of new external DAO  For the internal DAO, define and promote projects initiated by DAO  For the external DAO, proceed with the work/system requirement design and configuration in preparation for the first phase of operation Insight Business From the new special website on growth strategy “TOP I 2030” • Business aiming to provide sustainable medical solutions to improve the value of providing drugs, etc. to patients and other stakeholders Concept Solutions •App •Device, etc. Data •Drug discovery •Non-clinical •Clinical studies •RWD (1) Build an insight creation cycle (2) Commercialization by providing solutions. Meaningful quantitative data Advanced analysis Insights Solutions based on insights Patients Pharma Companies Medical Institutions The core business of Chugai is to provide innovative pharmaceuticals, and that remains unchanged. However, as a response to an era in which we are being called upon to provide value that goes beyond the “creation of innovative drugs,” we are working on improving the value proposition by providing solutions that utilize insights obtained from the analysis of various data. W Develop insights obtained from in various businesses such as pharmaceutical R&D activities into solutions to reaprocesseslize a value cycle that leads to (1) maximization of pharmaceutical value, (2) simultaneous generating MDAS* through continuous data acquisition, and (3) creation of new insights I Scope of Efforts in the Insight Business 55 Basic principles Direction of approach • Focus on patients and pain in medical institutions/systems, considering Chugai's competitive advantage and synergy with the pharmaceutical business • Deployment of solutions from Roche in Japan • Development of solutions related to products created by Chugai, utilization in clinical development, and post-launch deployment • Decision support • Clinical (healthcare providers, patients): CDS • R&D (for companies, including Chugai): RDS • Remote Patient Monitoring (RPM) • Clinical (healthcare providers, patients) • R&D (for companies, including Chugai) Efforts toward Integrated Solutions at Roche 56 What is an integrated solution? Examples of efforts • Generic term for a non-product-dependent solution that leads to improvement of the value delivered to patients through a patient journey • Early + accurate detection • Timely diagnosis • Remote disease monitoring • Individually tailored care/interventions • Remote monitoring of patients with multiple sclerosis (MS) via smartphone app • Remote monitoring of patients with ophthalmologic diseases via smartphone app • Remote monitoring of cancer patients via smartphone app • Support for analysis of radiological images and pathological images of cancer patients Efforts in the Insight Business at Chugai 57 TOP I 2030: One of the growth foundation reforms Examples of efforts • With the goal of establishing a business system for the insight business by 2030, we will work on verifying technologies and effects through individual use cases, reexamine our internal system, and establish a model for collaboration with external parties. Decision-making support Remote monitoring Endometriosis • Diagnostic imaging •Pain Cancer •Foundation medicine business •Prognosis prediction •Early diagnosis •Adverse events/ prognosis Hemophilia •Asymptomatic hemorrhage • Motor function Eye diseases •Eye function Activity goals for the insight business Contribution to pharmaceutical business Exploration phase Up to 2023 Technology verification through individual use cases Verification of effects and expansion of scale by application to multiple projects Verification phase 2024 to 2026 Establishment of a business structure that will enable continuous creation and sustained delivery of insights Commercialization phase 2027 to 2030 ▶ Maximization of insight utilization ▶ Acceleration of drug R&D with data/insights gained through insight business-related activities Example of Efforts in the Insight Business: Endometriosis What is endometriosis? Anti-IL-8 recycling antibody • 10% of adult women are affected • A disorder in which tissue similar to the "endometrium" covering the inside of the uterus forms outside the uterine cavity • One of the causes of infertility. Frequency of occurrence is low, but can be one of the risk factors for ovarian cancer • Major symptoms include pain due to adhesion and inflammation and worsening of menstrual pain. Major treatment methods include hormone therapy and other pharmacotherapy, as well as surgery (nonradical) • Issues: • Non-invasive diagnostic imaging • Quantitative evaluation of pain • Currently under development for endometriosis Predisease Diagnosis Treatment Prognosis #1 Diagnostic imaging #3 Virtual Care Platform #2 Pain measurement 58 Mode of action of anti-IL-8 recycling antibody Anti-IL-8 recycling antibody Neutralizes the effects of IL-8 fibrosis Fibrosis in endometriosis Transformation into myofibroblasts • high contractility • Production of extracellular matrix • α-SMA expression Neutrophil migration induced by IL-8 MCP1 production by IL-8 ↑ Macrophage migration by MCP1 Bleeding/ inflammation IL-8 production↑ Macrophage accumulation in fibroblasts, TGFβ1 production↑ Neutrophil Macrophage IL-8 MCP1 TGFβ1 Bleeding and inflammation in endometriotic tissue → IL-8 production Drawing from Sci. Transl. Med. 15(684):eabq5858 Figure S5 Inf Efforts in the Insight Business, Case #1 59 Issues in imaging diagnosis of endometriosis Efforts - Current diagnosis: Definitive diagnosis by endoscopy, which is highly invasive, is required. Diagnostic imaging is difficult to adjudicate, and evaluation differs greatly among evaluators. - Variability in efficacy assessment in clinical studies - Efforts to develop models for detecting pelvic organ and nodal lesions by artificial intelligence (collaboration with Preferred Networks) Efforts in the Insight Business, Case #2 60 Issues in pain measurement Efforts - Large variation in subjective evaluation - Variability in efficacy assessment in clinical studies - Development of continuous pain measurement technology using digital devices (collaboration with Biofourmis) - Mechanism of pain in endometriosis: autonomic nerves - Vitals related to autonomic nerve pain: heart rate fluctuation, skin conductance (1) Visual analog scale (VAS) No pain Use 10 cm scale The worst pain imaginable 100 No pain The worst pain imaginable (2) Numeric rating scale (NRS) (3) Face rating scale (FRS) Japan Chronic Pain Information Center https://itami-net.or.jp/download (Access: September 26, 2023) Created by Chugai Pharmaceutical with reference to and citation of pain education content data for medical education. 0 1 2 3 4 5 6 7 8 9 10 A B C D E F Biofourmisʼs Biovitals™ Platform Pain Algorithm Biovitals® Pain Index Mobile app Femme Rhythm™ App Objective Pain Measurement Using a Wearable Biosensor and a Mobile Platform in Patients With Endometriosis The E4 wristband https://www.empatica.com/research/e4/ （Access: September 27, 2023） Empaticaʼs E4 Wearable Biosensor Efforts in the Insight Business, Case #3 61 Issues in the delivery of solutions Efforts - Need to build access infrastructure to deliver solutions to patients and medical institutions - Efforts to deliver solutions through the virtual care platform that Biofourmis provides to medical institutions - Development of a virtual care platform that enables real-world pain measurement - Small-scale provision of virtual care for patients with endometriosis in the U.S. - Find insights from data gathered from platform to support Chugai R&D on endometriosis and post-launch activities for the endometriosis drug candidate under development Future Issues - Regulatory compliance (medical device regulations, personal information protection, etc.) - Acquire capabilities to appropriately protect personal information and comply with various regulations depending on where the solution is deployed (Japan, overseas) - Involvement of relevant stakeholders (patients, healthcare professionals/medical institutions, etc.) - From the solution development stage, we consider the benefits to various stakeholders and build solutions that can provide services sustainably. - Knowledge accumulation - Improve efficiency by accumulating knowledge through creating solution development examples.  Aiming to commercialize an insight business that realizes the provision of sustainable solutions based on insights to further improve the value provided by pharmaceuticals.  By developing solutions from insights obtained from pharmaceutical R&D activities and other business processes, we aim to realize a value cycle that leads to (1) maximization of pharmaceutical value, (2) simultaneous construction of MDAS* through continuous data acquisition, and (3) creation of new insights S

4.  Daiichi Sankyo’s Challenge to Realize: 2030 Version 

Russell Group principles on the use of generative AI tools in education Our universities are committed to the ethical and responsible use of generative AI and to preparing our staff and students to be leaders in an increasingly AI-enabled world. The rise of generative artificial intelligence (AI) has the potential for a profound impact on the ways in which we teach, learn, assess, and access education. Our universities wish to ensure that generative AI tools can be used for the benefit of students and staff – enhancing teaching practices and student learning experiences, ensuring students develop skills for the future within an ethical framework, and enabling educators to benefit from efficiencies to develop innovative methods of teaching. Valuable work undertaken by organisations such as the Quality Assurance Agency for Higher Education (QAA) and Jisc has helped develop the sector’s understanding of the opportunities and considerations of generative AI 12 , and the Department for Education (DfE) has set out its position on the use of generative AI in the pre-university education sector 3 . Russell Group universities have contributed sector-wide insight and have been proactively working with experts to revise and develop policies that provide guidance to students and staff. Collaboration, coordination, and consistency on this issue across the education and professional sectors – including professional bodies, schools, FE colleges and employers – will be crucial. In recognition of this, Russell Group universities have collectively developed the following principles that will guide the approach to generative AI tools across our universities and, we hope, beyond: 1. Universities will support students and staff to become AI-literate. 2. Staff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience. 3. Universities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access. 4. Universities will ensure academic rigour and integrity is upheld. 5. Universities will work collaboratively to share best practice as the technology and its application in education evolves. 1. Universities will support students and staff to become AI-literate. 1.1 Generative AI tools are capable of processing vast amounts of information to generate responses but they have significant limitations. It is important that all students and staff understand the opportunities, limitations and ethical issues associated with the use of these tools and can apply what they have learned as the capabilities of generative AI develop. These include: (a) Privacy and data considerations: whether a generative AI tool is designed to learn directly from its users’ inputs or not, there are risks to privacy and intellectual property associated with the information that students and staff may enter. (b) Potential for bias: generative AI tools produce answers based on information generated by humans which may contain societal biases and stereotypes which, in-turn, may be replicated in the generative AI tool’s response. (c) Inaccuracy and misinterpretation of information: data and information contained within generative AI tools is garnered from a wide range of sources, including those that are poorly referenced or incorrect. Similarly, unclear commands or information may be misinterpreted by generative AI tools and produce incorrect, irrelevant or out-of-date information. This means that accountability for the accuracy of information generated by these tools when transferred to another context lies with the user. (d) Ethics codes: users of generative AI tools should be aware that while ethics codes exist, they may not be embedded within all generative AI tools and that their incorporation, or otherwise, may not be something that users can easily verify. (e) Plagiarism: generative AI tools re-present information developed by others and so there is the risk of plagiarised content and/or copyright infringement being submitted by a user as their own, and artwork used by image generators may have been included without the creator’s consent or licence. (f) Exploitation: the process by which generative AI tools are built can present ethical issues. For example, some developers have outsourced data labelling to low-wage workers in poor conditions4 . 1.2 Our universities will provide guidance and training to help students and staff understand how generative AI tools work, where they can add value and personalise learning, as well as their limitations. By increasing AI-literacy, our universities will equip students with the skills needed to use these tools appropriately throughout their studies and future careers, and ensure staff have the necessary skills and knowledge to deploy these tools to support student learning and adapt teaching pedagogies. 2. Staff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience. 2.1 Our universities will develop resources and training opportunities, so that staff are able to provide students with clear guidance on how to use generative AI to support their learning, assignments, and research. 2.2 The appropriate uses of generative AI tools are likely to differ between academic disciplines and will be informed by policies and guidance from subject associations, therefore universities will encourage academic departments to apply institution-wide policies within their own context. Universities will also be encouraged to consider how these tools might be applied appropriately for different student groups or those with specific learning needs. 2.3 Engagement and dialogue between academic staff and students will be important to establish a shared understanding of the appropriate use of generative AI tools. Ensuring this dialogue is regular and ongoing will be vital given the pace at which generative AI is evolving. 3. Universities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access. 3.1 Universities continually update and enhance their pedagogies and assessment methods in response to drivers including new research, technological developments and workforce needs – adapting to the use of generative AI technology is no different. Incorporating the use of generative AI tools into teaching methods and assessments has the potential to enhance the student learning experience, improve critical reasoning skills and prepare students for the real-world applications of the generative AI technologies they will encounter beyond university. 3.2 Appropriate adaptations to teaching and assessment methods will vary by university and discipline, and protecting this autonomy is vital. All staff who support student learning should be empowered to design teaching sessions, materials and assessments that incorporate the creative use of generative AI tools where appropriate. Professional bodies will also have an important role in supporting universities to adapt their practices, particularly in relation to accreditation. 3.3 As the technologies develop and new generative tools become available, elements of generative AI used within universities may reside behind paywalls or be restricted to paying subscribers. Universities will need to consider how best to respond to a potential proliferation of such subscription tools and attempt to ensure fairness of access so that students and staff can access the generative AI tools and computing resources they need in support of their teaching and learning practices. 4. Universities will ensure academic rigour and integrity is upheld. 4.1 All 24 Russell Group universities have reviewed their academic conduct policies and guidance to reflect the emergence of generative AI. These policies make it clear to students and staff where the use generative AI is inappropriate, and are intended to support them in making informed decisions and to empower them to use these tools appropriately and acknowledge their use where necessary. 4.2 Such clear and transparent policies are critical to maintaining consistent and high standards of learning, teaching and assessment across Russell Group universities. 4.3 Ensuring academic integrity and the ethical use of generative AI can also be achieved by cultivating an environment where students can ask questions about specific cases of their use and discuss the associated challenges openly and without fear of penalisation. 5. Universities will work collaboratively to share best practice as the technology and its application in education evolves. 5.1 Navigating this ever-changing landscape will require collaboration between universities, students, schools, FE colleges, employers, sector and professional bodies, with the ongoing review and evaluation of policies, principles and their practical implementation. 5.2 Our universities will regularly evaluate policies and guidance for staff and students relating to generative AI tools and their impact on teaching, learning, and assessment practices. This will include monitoring the effectiveness, fairness, and ethical implications of the integration of generative AI tools into academic life, and adapting policies and procedures to ensure they remain valid as generative AI technologies evolve. 5.3 Fostering relationships between higher education institutions, schools, employers, professional bodies who accredit degrees, AI experts, leading academics and researchers, as well as ensuring an inter-disciplinary approach to addressing emerging challenges and promoting the ethical use of generative AI, will be crucial. Russell Group universities recognise the challenges that lie ahead and will continue to value the input of others, along with contributing expertise to the national and international discussions around generative AI and its applications within teaching, learning, assessment and support.

5. How to get ChatGPT regulatory approved as a medical device  

 The advent of ChatGPT and similar large language models (LLMs) has created unprecedented excitement for their application in medicine. Advocates of the technology are imagining a wide range of clinical utility, from clinical note-taking to diagnostic tools and beyond. During the initial splurge of hype it can be easy to get carried away with futuristic thinking, while pragmatists and sceptics will only be more validated as the limitations and risks of such models become more widely acknowledged and addressed. However, there will eventually be a path that emerges that brings us closer to successfully applying this ground-breaking technology to medicine, and that’s what we’re going to explore in this blog.


Is it even possible?

First, let’s begin with the bad news. ChatGPT (and other models like it) can not be used safely in medical practice in their current form. They are prone to hallucination, bias, and can produce extremely plausible sounding misinformation. As such large language models are far better suited to generating creative rather than factual output. Additionally, they aren’t necessarily compliant with data protection laws such as GDPR and HIPAA, a number of cybersecurity risks present themselves, and there is little to no information publicly available on how they were built, trained and validated so no effective quality assurance can be conducted. To rely on them in medicine would likely be illegal in most jurisdictions, violating professional standards, clinician codes-of-conduct, medical device regulations and patient data protection laws. Even allowing them to be used for general medical search and queries could land their developers in trouble, as according to Haupt et al they could be liable for medical misinformation, as well as setting up litigation issues for clinical end users who may rely on plausible sounding, non-standard misinformation to make clinical decisions. 

You might be thinking, what about Google search? Yes, doctors use the internet of course, but general systems such as those are protected under laws such as Section 230 of the 1996 Communications Decency Act, and as such have no regulatory burden when it comes to processing medical queries, since the content provided is from third parties (each with their own liability). ChatGPT and the like are not protected since they do not disclose the sources of third party information ingested into their training, and are self-contained systems which act as far more than “a passive transmitter of information provided by others”.

Where to start

So, how could you go about demonstrating compliance for a medical large language model? It’s certainly not going to be easy, and currently most likely impossible, but we can at least explore what this might look like in the future by learning from current regulatory frameworks and ongoing active research into AI safety.

Define the problem

Before even starting to think about approving an LLM for medical use, take a step back and define the problem you are trying to solve. Blindly adopting the newest technology for the sake of it does not equate to creating an economically valuable solution. Taking radiology AI as an example from the 2010s, there is still uncertainty of how the use of these computer-vision tools translates into economic value for healthcare providers and systems. 

Defining your unmet clinical need and subsequent business case is more important than ever in the turbulent economic climate of 2023. With funding continuing to slow and investors prioritising near-term profitability over the promise of longer term potential, it’s crucial to demonstrate how generative LLMs will solve the problem in an economically viable way. Once you've accomplished this, you’re ready to start the product development and regulatory journey.

Intended Use

The starting point for any new technology in medicine is to define its intended use in order to a) decide if it is a medical device or not, in which case it requires regulatory approval in the form of FDA approval, a CE or UKCA mark as Software as a Medical Device (SaMD) or more specifically AIaMD, and b) what risk class and special controls are required. The intended use statement informs performance and safety requirements, as well as defining end users, clinical indications, clinical and operational context, and importantly, reasonably foreseeable misuse. Let’s pick an example that has been widely discussed - providing a differential diagnosis to a doctor based on a patient consultation. 

As part of the overall intended use for such a device, indications for use may be written as: 

“MedGPT is intended for use by qualified medical practitioners in the context of outpatient clinical consultations with patients aged over 18 for any clinical condition that is not immediately life threatening or critical. MedGPT provides a top three differential diagnosis based on clinician-derived prompt inputs, basing its outputs on data from a curated general medical knowledge database, real-time structured data from the patient record combined with consultation transcriptions. MedGPT is not intended to be used in emergent care, paediatrics, obstetrics, or psychiatry and its outputs should assist with clinical decision making only, not drive management or be relied upon for formal diagnosis.” 

This statement clearly sets limits on who can use the device, in what context, and for what clinical conditions in a limited adult population. The full intended use document would need to be much more detailed, but this is a good starting point for our theoretical example. You can already see that we have had to significantly limit the potential of the language model to a predefined target use case of clinical decision support for a given clinical population with limited scope and severity of conditions. This example is indeed a medical device, and will require regulatory approval.

Risk Classification


Next, we would have to determine the risk classification of this device. Based on the MDCG risk classification guidance this device would likely be Class IIa in the UK and EU, since it is intended to inform clinical decision making. The UK regulator, the MHRA is clear on this point - 

“A device is considered to ‘allow direct diagnosis’ when it provides the diagnosis of the disease or condition by itself, it provides decisive information for making a diagnosis, or claims are made that it can perform as, or support the function of, a clinician in performing diagnostic tasks.” 

Indeed, predicate devices such as Ada Assess are CE marked as Class IIa. 

Treating or diagnosing

EU Class III

Critical risk for patient

EU Class IIb

Serious risk for patient

EU Class IIa

Non-serious risk for patient

Driving clinical
management

EU Class IIb

EU Class IIa

EU Class IIa

Informing 
clinical management

EU Class IIa

EU Class IIa

EU Class IIa

Note that if further claims are made such as treating or diagnosing, then the risk classification can easily fall into class IIb or III, which require a higher burden of regulatory oversight. Currently CADx (diagnostic) systems in the United States of America are likely to be Class III, so it is important that we temper our intended use to clarify it is intended for clinical decision support only. Additionally, since this is novel technology, the FDA may require a De Novo submission, which has a longer timeline and greater regulatory scrutiny than a standard 510(k) submission that is used to leverage already existing substantially equivalent devices that are FDA approved.

Defining our requirements

Next, we’ll need to define the requirements for our medical large language model. What are the parameters in which we want it to operate, associated claimed benefits, performance benchmarks and safety requirements? In medical device regulation, there are essential requirements, and then product specific requirements. The essential requirements (also known as general safety and performance requirements) include the elimination of risk to the maximum extent possible for the duration of the lifetime of the device, appropriate design of electronic programmable systems and general requirements regarding labelling and product information (think instruction manuals and user information). Performance requirements depend on the claims being made - so we would need to demonstrate a valid clinical association that a medical LLM system can provide factually correct clinical diagnostic differential (which is currently hard to prove). We also need to decide on appropriate clinically measurable metrics, often based on the results and outputs of a systematic literature review specifically conducted to explore current state of the art, as well as technological benchmarks. Our Literature Review should be reported to PRISMA standards. All of this feeds into what is known as a Benefit Risk Analysis, which again feeds back into your requirements stack, alongside a Product Requirements and Risk Register managed within an ISO 13485 certified Quality Management System. We’ll also need to consider operating environment requirements, systems connectivity, architecture and data security. There’s a lot to do!

A theoretical system

Let’s look at architecture next, since we can’t test something until we actually have something to test. Research in this space is moving fast, with many groups working on improving the reliability and safety of large language model outputs. Taking example from Microsoft’s work into building a fact checking system incorporating an LLM, external knowledge and automated feedback, a sensible starting point may look something like this:

View fullsize



Here we have a third-party LLM connected by API which receives prompts guided by a custom prompt engine linked to a data retriever which can access a curated medical knowledge base. Some form of rules-based module controls the system to prompt, receive feedback, store or read memory or retrieve data. The system is designed to automate feedback so that LLM responses can be fact checked against the medical knowledge base, and sent back to be refined if not deemed factual when compared to knowledge extracted from the medical knowledge base. The memory module ensures all data in the flow is stored and informs the system so it improves rather than deteriorates in response quality. The rules-based module, with a series of pre-programmed IF>THEN logic could disallow queries relating to our exclusions (e.g. obstetrics, critical care), as well as function as a prompt guide rail to ensure LLM responses are returned in a specific format.

Let’s assume this set up works in a test environment (a feasibility study), and provides more verifiably factually correct outputs than current unprocessed LLM outputs. The entire system architecture will have to be designed and documented within an appropriate Software Development LifeCycle to IEC 62304 and 82304-1 standards, and verified as cybersecure to at least ISO 27001, while also being GDPR and HIPAA compliant. That’s a lot of technical documentation to go into your Medical Device File.

3 major hurdles

Our challenge will now be to verify and validate each component of our system, and demonstrate that it works in the real world. Software verification and validation may be relatively simple under current regulatory frameworks for most of our system modules except for the medical knowledge base and of course, the LLM itself. 

Validating a curated medical knowledge base 

The knowledge base would by necessity need to be curated and validated, and this is where the first major hurdle lies. Even if we could ingest the entirety of medical literature, not all medical information is up-to-date, accurate or relevant to all locations. Papers can be biassed, results can be outdated, and guidelines broken in a myriad of ways. Disease prevalence, population demographics and best practice guidance all differ across the world, and as such curating a database that is fit for purpose within our intended use will be extremely challenging. Additionally, access to all this information will be expensive if we aim to be comprehensive, and not all of it is machine-readable. However, let’s assume it’s somehow possible to curate, vet and validate such a large database, and move on to consider the third-party LLM. 

Software of Unknown Provenance

Currently, access to LLMs such as ChatGPT are by API only. The developers have not made public any documentation as to how it was built, trained or maintained as these details remain a trade secret. This is our second major hurdle, known as SOUP, or Software of Unknown Provenance. This means that if we cannot verify or validate a piece of third party software according to IEC 62304, we cannot claim to have mitigated all risks, since it could for instance be changed without warning (the current status quo will not stay still for long!), or be withdrawn from market making our system unable to function. The UK regulators are clear on this position, but that doesn’t mean that all hope is lost. As LLM technology becomes more accessible, developers will start building their own versions, and it may be that in due course someone will produce one with the required documentation (perhaps like Med-PaLM). Until then, we won’t see a regulatory approved system that uses an LLM at its core.

Analytical and Clinical Validation

Probably the largest hurdle will be to demonstrate the clinical evidence required to prove that the system is safe and effective for all cases within our intended use. Assuming we can set a benchmark of non-inferiority to clinician diagnostic differential performance, we would need to run a clinical investigation to ISO 14155 standard, with ethical approval, appropriately powered to achieve statistically relevant performance metrics, with enough room left over to analyse the almost infinite sub-stratifications of cases, all done to the STARD-AI criteria. We’re talking about the mother of clinical investigations. 

Ground-truthing the sheer number of potential cases for the clinical investigation will be a challenge in itself, likely requiring a form of independent panel-approved vetting of clinical input scenarios and data, expert group opinion on acceptable outputs, and a robust system for fact-checking, ranking or rating final system outputs. Checks would need to be in place for ‘red flag’ cases, and to make it even more difficult, our system ideally should not be changed or tweaked for the duration of the investigation. 

If we wanted to claim that our system actually helps clinicians reduce time spent making decisions, we would also need to run an investigation to prove it, ethically approved and appropriately registered, comparing current clinical practice without AI to a new pathway with AI, and measure the differences. We should use best practice guidance such as SPIRIT-AI and  TRIPOD-AI depending on our claims and intentions. This would not necessarily need to be in a randomised-control fashion, but could require investigation of two matched groups of clinicians with matched cases across two powered cohorts (again, no mean feat!). 

Technically, none of this is impossible, but it will require a significant amount of time and expertise to pull off. Ultimately, all of our clinical evidence, from literature review, feasibility studies, clinical evaluation plan (CEP), clinical investigation plans (CIPs) and reports (CIRs) will need to be compiled into a regulatory compliant Clinical Evaluation Report (CER). Of course, regulatory approval will entirely depend on our investigation actually showing positive results, so fingers crossed it actually works as planned.

Putting it all together

Let’s assume that we have managed to overcome all of the above hurdles, and have ended up with a fully documented Software Development LifeCycle, Clinical Evaluation Report and have compiled a Medical Device File. The process simplified to its core components looks like this:

View fullsize



Ongoing monitoring

You’ll note we haven’t covered everything in the above diagram for the sake of brevity, but one essential component will be post market follow up. It’s important to acknowledge that once our device is ‘on market’ (i.e. being made available for use as per its intended use), there is a legal requirement for us to monitor its performance and safety for the duration it remains on market. This is done both proactively and reactively in two components known as Post Market Surveillance and Post Market Clinical Follow Up. At the most simplistic level we will need to predefine our ongoing surveillance, including all complaints and feedback, as well as declaring our methodology for ongoing clinical assurance, which could be further clinical investigations and powered studies (to demonstrate performance across subgroups where we haven’t fully been able to demonstrate safety pre-market) or ongoing sampled audits. Do not underestimate the magnitude of this challenge - imagine having to audit potentially billions of input/output pairs forever and act accordingly for all errors and adverse events. The results of the post market surveillance must be incorporated back into our Clinical Evaluation Report and reported annually to the regulators. We’ll also need to update our Literature Review annually to check for any studies done on our device, and to assess performance and safety issue issues of other similar devices. If we don’t maintain these processes, then our device could be removed from market.

Getting regulatory approval

Now the fun begins. Hopefully, way back at the start we engaged with a regulatory body who is going to audit our work and certify us. This could be the FDA directly, or a UK Approved Body or EU Notified Body, or any number of country-specific competent authorities, depending on which geographies we want to deploy our system into clinical care. Each audit will come with fees (think tens of thousands) plus a nice long time delay (think months to years) before an audit can actually be performed. Assuming we pass audit, we receive our market authorisation, and we are almost there. We just need to appoint an appropriately qualified Person Responsible for Regulatory Compliance, register our device, produce appropriate labeling and instructions for use, and then start selling it! If we want our system to be used in multiple countries, we may also consider upgrading our Quality Management System to pass MDSAP standards, giving us entry into multiple markets. Although, caution here - we will need to prove it works in multiple languages too, and will additionally need to re-run our clinical investigations in each of our target countries to demonstrate it works on different populations with different disease prevalences, clinical guidelines and benchmark performances.

Updating our system

One tiny detail we haven’t yet mentioned is that current regulatory frameworks do not allow for continuous updating of software and AI-based medical devices. That’s going to be a problem… not only will our LLM be changing regularly, but our system self-feeds back to improve factual accuracy. None of that is straightforward under current regulatory frameworks - but hope is on the horizon. The FDA, Health Canada and the UK MHRA are all working on Predetermined Change Control Plans (PCCPs) to allow for safe, quality-assured ongoing updates of software devices as long as they remain within the confines of their regulatory-cleared intended use. We don’t yet know when these frameworks will be finalised, but at the time of writing there is a public FDA consultation on the subject. In essence, developers will be able to submit plans to update software over time, but they must stick to those plans and not deviate from them, or change their intended use, otherwise a new regulatory submission may be required.

Other considerations


We haven’t covered many other important aspects of the regulatory journey in great detail here, as our hope is simply to inform the interested reader of the general procedures and frameworks when it comes to AI as a Medical Device. We should however make note of the following topics, some of which are still in flux and subject to change:

Cybersecurity for AI driven devices

Health informatics standards

Explainability of AI devices

Formative usability testing

Human centred design

Bias detection and mitigation

Algorithmic change protocols

AI framework standards

Functional safety

The EU AI Act

Conclusion

So there we have it - a roadmap for how to get a medical large language model-based system regulatory cleared to produce a differential diagnosis. It won’t be easy or for the faint-hearted, and it will take millions in capital and several years to get it built, tested and validated appropriately, but it is certainly not outside the realms of future possibility. 

To put it all in context, we can vividly remember when deep learning first exploded around 2012 and it took approximately five years before the first regulatory approved AI-driven device came on market (that was one of Dr Harvey’s, a CE marked Class IIa decision support system for breast mammography). Now there are over 500 of AI-enabled devices with regulatory approval! 

There is one big BUT in all this that we feel compelled to mention. Given the lengthy time to build, test, validate and gain regulatory approval, it is entirely possible that LLM technology will have moved on significantly by then, if the current pace of innovation is anything to go by, and this ultimately begs the question - is it even worth it if we are at risk of developing a redundant technology? Indeed, is providing a differential diagnosis to a clinician who will already have a good idea (and has available to them multiple other free resources) even a good business case? 

In reality, these risks are simply a fact for all medical devices, as innovation always moves forward, and risks need to be weighed against the potential benefits of improving patient care in the near term. We are excited to see where this goes, and of course, our team at Hardian are ready, willing and able to help anyone who dares go on this adventure into the unknown. Consider yourself warned. You know where to find us.


Hardian Health is a clinical digital consultancy focused on leveraging technology into healthcare markets through clinical evidence, market strategy, scientific validation, regulation, health economics and intellectual property.



6.  AstraZeneca data and AI ethics

Ethics at the core of our data and artificial intelligence ambitions

Rapid advancements in the field of artificial intelligence (AI) are revolutionising the pharmaceutical industry. At AstraZeneca, these advances have already begun to change the way we work in several business areas. AI systems can help us improve our supply chain when delivering medicines, design smarter trials and better understand diseases, and match patients with the right clinical trials by creating biological insight Knowledge Graphs. AI tools, including generative AI technology, have the potential to do this and much more, creating opportunities for us to push the boundaries of science to deliver life-changing medicines. Our foundational ethical principles are supporting our ambitions for the future of AI at AstraZeneca.

Our principles for ethical data and AI

Rapid developments in AI technology have brought us into uncharted territory, and companies and regulators must work together to meet the new challenges posed. Our principles empower us and our partners to navigate this new environment safely and effectively. By encouraging innovation and evolution while maintaining our values, they provide a long-term ethical foundation for our AI work to benefit patients and employees and enable us to make a positive contribution to society.

Explainable & transparent

We are open about the use, strengths and limitations of our data and AI systems.

We explain to people if they are interacting with an AI system and whether interactions are recorded.

We are able to explain when and how AI is used to aid a decision that impacts humans.

We will ensure appropriate levels of explainability and transparency in line with our legal obligations.

We will ensure our assumptions are clear, we will ensure algorithms are appropriately documented, decisions are explainable as needed, and processes are in place to deal with unanticipated impacts.

We can demonstrate the legitimacy of our data sources, and how models are trained and maintained.

We have the ability to explain processes, data and algorithms when required to do so while protecting our intellectual property.

We are transparent about the use of AI to build trust and credibility in all our endeavours.

*Explainable refers to the ability of humans to understand the results of a solution generated by Artificial Intelligence

Fair

We endeavour to use robust, inclusive datasets in our Data and AI systems.

We seek to ensure our use of AI is sensitive to social, socio-geographic and socio- economic issues, and protect against discrimination or negative bias to the best of our ability.

We will continually adapt and improve our AI systems and training methods to drive inclusiveness.

We treat people and communities fairly and equitably in the design, process, and outcome distribution of our AI systems.

We are aware of the limits of our AI systems. We strive to apply their outputs in the right context and in a non-discriminatory fashion.

We monitor our AI systems to maintain fairness throughout their lifecycle.

We acknowledge all data sources and human effort in our Data and AI Systems, while protecting our intellectual property. We aim to use ethically sourced AI tools and partners.

Accountable

We apply governance proportional to the impact and risk of our Data and AI Systems.

We diligently assess risk against opportunities to act consistently with our company values.

We take accountability of our use of Data and AI Systems throughout their life cycle, so their use is appropriate and monitored over time.

We anticipate and mitigate the impact of potential unfavourable consequences of AI through testing, governance, and procedures.

We are accountable for our findings and the recommendations from AI systems. We govern AI-supported decisions appropriately.

We recognise and address unforeseen consequences resulting from our AI usage appropriately, and ensure that lessons are learned.

Human-centric and socially beneficial

Where Data and AI is involved, humans oversee the system and are accountable for driving clear, expected benefits to people and society.

We apply AI to contribute to a sustainable workforce, business, and planet, to help make AstraZeneca a Great Place to Work, and accelerate our contribution to society.

We involve people at appropriate times to responsibly deploy AI where decisions carry a material impact. We harness the capabilities of AI to accelerate the development and delivery of the right life-changing medicines to the right patients with the right commercial potential.

We employ human-led governance over our AI systems. We respect human dignity and autonomy and strive to reflect this in our AI systems.

We drive prudent and sustainable energy consumption when using Data and AI systems.

We recognise that protecting the environment is an integral part of ensuring AI systems are socially beneficial. We aim to reduce the energy consumption of our AI systems in line with our Ambition Zero Carbon.

Private and secure

We respect privacy and control, and act in a manner compatible with intended data use.

We respect privacy and the rights of all stakeholders, and will act in accordance with relevant laws and regulations.

We assign appropriate protective measures to keep all information held or generated by AstraZeneca’s AI systems secure.

We employ Data and AI systems that are designed to be secure.

We strive to protect our AI systems against information breaches and unintended applications, with mitigation processes in place. We manage our AI systems through their life cycle, including information used and generated.

We review third party AI providers’ data protection standards to seek alignment, and comply with applicable law.



7. Procaps participate in panels on digitalization and artificial intelligence in pharma manufacturing

Procaps Group to Participate at CPHI Barcelona 2023

Rhea-AI Impact

(Low)

Rhea-AI Sentiment

(Neutral)

Tags

conferences

Rhea-AI Summary

Procaps Group announces participation at CPHI Barcelona 2023

Positive

Procaps Group will participate at CPHI Barcelona 2023, showcasing their specialized technologies and capabilities.

Procaps is one of the five sustainable sponsors at the event, highlighting their commitment to sustainability.

Gonzalo Nieto, head of Innovation and ESG, will present a panel on sustainability.

PhD. Joe Villa, head of Corporate R&D, will participate in panels on digitalization and artificial intelligence in pharma manufacturing.

Negative

None.

  

  

  

AD

10/16/2023 - 08:31 AM

MIAMI & BARRANQUILLA, Colombia--(BUSINESS WIRE)-- Procaps Group (NASDAQ: PROC) (“Procaps”), a leading integrated LatAm healthcare and pharmaceutical conglomerate, announced today that it will participate at the CPHI Barcelona 2023, that will take place from October 24th to October 26th, continuing with our two-decade presence in the conference.

We welcome you to learn more on our specialized technologies and capabilities such as our novel oral delivery system formulations, gummies, development of new chemical entities, expertise in hormonal, high potency compounds, as well as successful product tech-transfers, and much more.

We are pleased to announce our participation as one of the five sustainable sponsors at CPHI Barcelona this year. Our sustainable approach is seen in different areas of our business, including in our own technologies, such as Unigel. This patented format allows the development of fixed-dose combinations in one single softgel, reducing environmental waste by decreasing the number of packaging material used when manufacturing two products or more in one single format, rather than individually. This technology creates a large positive environmental impact and an even larger footprint on health by simplifying treatments with Unigel.

One of Procaps´s executive – Gonzalo Nieto, head of Innovation and ESG, will present the panel "Getting Started with Sustainability – Learn from the Experts" at the CPHI, in Barcelona, on October 25.

In addition to that, with our 8 manufacturing plants across the Americas and our extensive manufacturing capabilities, exclusive delivery systems and product development expertise, we will participate on another panel. PhD. Joe Villa, head of Corporate R&D of Procaps, will participate in the speaking session of “How digitalization is driving efficiencies, cost-savings, and output for drug manufacturing”, on October 25 and also will present the panel “Loading Potential: Artificial Intelligence for Pharma Manufacturing”, on the same day.

We look forward to welcoming you to our booth #3M40. For more information about CPHI, please visit: https://europe.cphi.com/europe/en/home.html

About Procaps Group

Procaps Group, S.A. (“Procaps”) (NASDAQ: PROC) is a leading developer of pharmaceutical and nutraceutical solutions, medicines, and hospital supplies that reach more than 50 countries in all five continents. Procaps has a direct presence in 13 countries in the Americas and more than 5,500 employees working under a sustainable model. Procaps develops, manufactures, and markets over-the-counter (OTC) pharmaceutical products and prescription pharmaceutical drugs (Rx), nutritional supplements and high-potency clinical solutions.





8. 1DOC3: ACCESSIBLE HEALTHCARE TO MILLIONS

Background

1Doc3 (pronounced “uno doc tres”) is an online health platform that allows millions of Spanish-speaking users to ask health-related questions and receive professional medical guidance in real-time using artificial intelligence (AI). In addition, it provides data to health insurers and pharmaceutical companies to let them reach customers more efficiently. Furthermore, 1Doc3 is free and allows users to remain anonymous. It receives around $2 million in funding from investors like Wayra, TheVentureCity, Mountain Partners and Mountain Nazca.

The platform, which can be downloaded on computers and mobile devices, keeps a database of over 400 licensed doctors who are recruited, trained and monitored, ensuring that patients receive answers from the most qualified professionals. These doctors build their reputation online by providing personalized answers to users for free. This type of access is convenient, free and anonymous and allows users to make more informed choices regarding their health and wellbeing.

Helping Its Users

1Doc3 has served over 490 million Spanish-speaking users in 120 countries, 53 percent of whom are below 34 years old. Over 60 percent of the questions asked by these younger users are related to sex. While these types of questions may normally be too embarrassing to ask in person, the anonymity of 1Doc3 allows young patients to receive the right medical guidance and even provides coupons for products like condoms.

The platform uses AI to help these users navigate towards relevant information. For example, if a user were to ask a question related to their back pain, AI would ask where the pain is “above or below,” and if it is a “stabbing pain.” The personalized and innovative service is highly sought after and has even earned itself a partnership with Internet.org, a system that brings connectivity to users in places where internet access is spotty at best.

Helping Insurance Companies and Pharmacies

There is also a commercial aspect to 1Doc3. The platform’s AI serves as a data collecting module. Over 70 million questions are asked each year and this makes the database extremely informative. With this information, health insurers can provide cheaper treatment to patients by eliminating the necessity to physically go to a physician’s office – in fact, users save an average of 11 percent on treatment when they use 1Doc3.

The platform also helps medical insurance and pharmaceutical companies identify patients for rare diseases. For example, 1Doc3 helped a client pharmaceutical company find patients who were suffering from ankylosing spondylitis, which is a relatively rare and difficult to diagnose. 1Doc3 identified back-pain along with the presence of three or four other symptoms to seek out those suffering from ankylosing spondylitis and provide resources for treatment. In this case, it connected patients to pharmaceutical companies who could provide the right medication and professional care.

The Future of the Health Industry

1Doc3 is described by Javier Cardona as a pocket-size doctor who is available to users at any time and provides integrated solutions to health issues. Although the bulk of its users are in Colombia, Mexico, Argentina and Peru, the administrative team is planning to expand globally and provide these capabilities to users all over the world.

While other healthcare platforms may also provide medical information to users, it is not always personalized. 1Doc3 is a revolutionary free service that changes the face of healthcare by connecting patients to doctors in a timely manner and pointing users in the right direction. It removes barriers like time, cost and inaccessibility and puts the health back in the hands of the user.

– Julian Mok



9. Artificial Intelligence at CSL

When CSL assembled last year at its Data Science Summit in Bern, Switzerland, the company brought together participants from every aspect of its operations, from Finance to Pharmacometrics.

With data science and artificial intelligence playing an ever-increasing role, Global Head of Digital Transformation and Execution Systems Karen Etchberger posed a question relevant to all: “How do we move from where we are today to a future where we can bring this ambition to life?”

John Thompson, CSL’s Global Head of Advanced Analytics and Artificial Intelligence, is helping to lead that effort. Thompson has been on the forefront of artificial intelligence and its use in business for more than 30 years. He has helped build analytic systems for giants of global industry like Coca-Cola, Anheuser-Busch and Dell. Now, he’s doing the same for CSL.

“This company has grown tremendously and has been making smart moves along the way,” Thompson said. “Now it’s really starting to see the value of data and analytics.”

Developing a Data Science Framework

CSL is taking a two-pronged approach that includes a Center of Excellence and a Community of Practices on advanced analytics and AI, Thompson said. Data scientists are part of both groups and working on numerous projects throughout CSL. The setup ensures each project is the right one to address individual team needs while benefitting CSL as a whole.

“There’s a lot do,” Thompson said. “My days go from dawn to dusk every day and I feel as energized as I did when I started in the morning. It’s an exciting time to be here.”

Artificial intelligence and machine learning can be used to comb through vast sets of data to find outliers or similarities that can illuminate understanding of any number of scenarios. Like other global industries, Thompson said the company will be using artificial intelligence to improve supply chain efficiency and to comply with regulatory and legal requirements.

Making AI Work for Patients

But CSL also wants to use those powerful engines to solve the burdens faced by patients with rare and serious diseases. A major goal in CSL Behring’s AI push is to shorten the amount of time between the onset of symptoms for a patient and an accurate diagnosis. In Pharmacovigilance – the front lines of patient safety - robotics process automation can speed the flow of information and improve operational efficiency, said Richard Wolf, Executive Director, Global Clinical Safety and Pharmacovigilance.

Wolf says his team is also working with others across industry to find areas where AI and natural language processing can be utilized to ensure crucial information is readily surfaced, One day, he believes it could even serve to predict risks associated with certain medications.

“We do think there is a place for artificial intelligence in our work,” Wolf said. “And we’re taking a careful and thoughtful approach toward implementing it.”

Analytics can also help CSL uncover important medical insights from vast amounts of “real-world data,” such as physician notes in a patient’s chart. Real-world data are obtained outside of randomized controlled trials and generated during routine clinical practice. Prior to advanced analytics and AI, this information existed but it was difficult to gather and analyze.

Haley Kaplowitz, Executive Director & Global Head of Safety Sciences, is leading an organization-wide effort to employ analytics as a key tool for gaining real-world evidence to be used in decision-making across the product life cycle.

Both real-world data and real-world evidence are playing an increasing role in healthcare decisions, particularly from regulatory authorities and payers. They may also help predict patient groups at increased risk of adverse events and demonstrate product effectiveness and differentiation in the marketplace, Kaplowitz said.

“The industry is under increasing pressure to provide evidence and demonstrate the value of products to multiple stakeholders, particularly in actual clinical practice,” she added. “Real-world evidence is inherent to reach this goal and increasingly crucial to ensure patient access and commercial success.”



10. Artificial Intelligence in Africa’s Healthcare: Ethical Considerations

AI IN THE HEALTHCARE SECTOR Globally, the most critical issue in healthcare is providing overarching and effective treatment options that improve standards of living. The World Health Organization (WHO) has developed a five-year strategic plan for reaching public-health targets, as outlined in the Sustainable Development Goals (SDGs). In 2019, the WHO introduced the “triple billion” targets for global health, i.e. universal healthcare, health emergency protections, and overall better health outcomes for one billion 2 people across the world. AI-centric solutions can help achieve these goals by increasing access, improving quality and reducing costs. Developments in AI will drastically improve health services, diagnostics and personalised medicine. Various initiatives are already employing basic technology applications to provide essential healthcare services, for example, to expectant and nursing mothers. These are particularly relevant in the context of African healthcare, where the technology currently being used can easily incorporate AIbased solutions. For example, Safermom is a Nigerian start-up that empowers pregnant women and new mothers to make informed decisions by using low-cost mobile technologies (two-way SMS, voice calling, and mobile apps) 3 to transmit vital health information. In addition to improving direct patient care, AI can maximise supply- chain efficiencies, reduce administrative tasks, and streamline and improve life-saving compliance measures. It can also generate new capabilities for safeguarding against public-health epidemics that plague the most vulnerable populations, e.g. the containment of dengue fever or the prediction of birth asphyxia using 4 a mobile phone. Moreover, real-time access to maternal newborn healthcare data can be used to swiftly identify and respond to childhood diseases, malnutrition or related challenges. Despite countless benefits, however, the application of AI is vulnerable to pitfalls. The current AI-powered health systems suffer from an absence of accurate datasets and the uneven management of sensitive health data. To be sure, the most significant ethical violations are not rooted in malicious intent, but in a lack of awareness of appropriate AI practices and safeguards. Stakeholders, including government and international organisations, are attempting to incorporate and implement safeguarding measures, with ethics as a central tenet of the AI framework. Using data from the United States (US) as a reference, it is known that cancelled appointments can be costly to doctors. A 2013 cross-university study estimated that the cost of no-show appointments per doctor in the US 5 was US$725.42 per day. This is calculated based on an average daily patient count of 24 patients per doctor, with an 18 percent baseline no-show rate. Thus, individual practices suffer an annual loss of over US$182,000. A direct relationship between this data and Africa is perhaps most appropriate with respect to cities in the continent. In addition, however, it can be applicable to Africa’s rural areas. These regions suffer from physician shortages, with patients frequently unable to s e e hi g hl y in- d emand d o c to r s . If transportation is expensive or difficult, for inINCREASING ACCESS Artificial Intelligence in Africa’s Healthcare: Ethical Considerations ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 3 demand rural doctors, the issues will not only be financial but loss of human life. AI solutions are making a headway in addressing these challenges. For example, the Nigerian company DokiLink helps patients book doctor’s appointments by creating personal calendars for doctors and their aides. The platform also provides means for doctors to collaborate and exchange information concerning medical questions. The founder, Dr. Niyi Osamiluyi, says that AI will “help expand the capacity and capabilities of healthcare providers, especially in the areas of 6 radiology and pathology.” Medical appointments can often be timeconsuming, inconvenient and physically demanding for patients. Geography and economic restraints can limit access for both rural and urban residents, e.g. patients living in remote mountainous regions or those living in urban areas with little access to transportation. In cases where specialised medical professionals and equipment are required, AI-based telemedicine technology can bridge borders, overcome language barriers and address economic constraints. AI has the potential to offer patients unfettered access to specialists around the globe and allow for unprecedented coordination between professionals. With an increase in mobilephone-based applications, emerging markets can also benefit, notwithstanding the network and infrastructure challenges. For example, Novartis has partnered with Vodacom South Africa to connect community health workers 7 to doctors through mobile technology. AI is becoming increasingly instrumental for the early detection of diseases, which allows IMPROVING QUALITY for more accurate diagnoses, reducing instances of misdiagnosis and the resulting health and cost burden to patients. Datasharing amongst health professionals provides doctors with myriad case studies to inform diagnoses and allow for in-depth analyses of previous studies. This can give physicians a foundational understanding of many illnesses, even without significant prior exposure. In addition to enhanced diagnostic procedures, AI-enabled technologies can provide superior treatment options. Improving the quality of healthcare systems is beneficial for not only patients but also physicians, nurses and ancillary 8 professionals. According to Athenahealth, physicians spend an average of “40 percent of their time processing thousands of administrative documents and forms and chasing down hundreds of missing lab and 9 imaging orders.” Automation of processes such as cataloging charts, filling prescriptions and transcription services can ease the burden placed on medical professionals and yield positive externalities for patients. In the context of unstructured medical data, however, it is necessary to have sophisticated natural language processing algorithms. Since much of the technology is currently being developed in Western or Asian contexts, transferring them to the African markets may prove challenging. The technology must be adaptable to the local language, to allow for modifications based on different languages, language structures and even speech accents. Moreover, the medical situations themselves may be vastly different, e.g. different types of diseases and healthmanagement systems. Artificial Intelligence in Africa’s Healthcare: Ethical Considerations 4 ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 While the uptake of any new system is a challenge that requires various incentive s truc tures , Afri can countries have demonstrable interest. Dr. Osamiluyi says that the use of AI in medical cases in Nigeria will “help to alleviate the lack of human resources for health, poverty and epidemiological transition of disease burden. It will help in primary care by making patient diagnosis 10 faster and more accurate.” According to Deloitte’s 2019 Global Healthcare Outlook, many public health systems across the globe are still financially unable to address “accessibility (imbalanced distribution, including a rural-urban divide), affordability (especially for patients with low economic status), awareness (of lifestyle diseases, risk factors, vaccinations), absent or inadequate infrastructure and skilled human 11 resources.” Global spending is slated to grow at an annual rate of 5.4 percent until 2022, a notable increase from the 2.9 percent in the 12 last five years. Similarly, government healthcare spending is increasing at an average annual rate of 6.7 percent in West Africa and 4.5 percent in Southern Africa. Despite these efforts, the 2017 report by the World Bank and WHO indicates that half of the world’s population do not have access to essential health services, with health expenses causing 13 100 million people to live in extreme poverty. The integration of AI into the healthcare space can help check the rising medical diagnosis costs, making treatments more affordable. Nigeria has developed a system called Apmis, which allows healthcare data to be shared and exchanged easily by hospital REDUCING COSTS owners, healthcare professionals, caregivers, patients and other stakeholders. It allows for easy, transparent, secure and low-cost data14 sharing. Another successful case is Kenya Medical Supplies Agency and IBM’s Watson, which are engaged in a pilot project to transform healthcare supply chains. Users can interact with AI through various platforms, including SMS, computer and voiceover data, to improve healthcare logistics such as communication, sending medical records, and 15 appointment updates. AI-assisted technologies are expected to save the global healthcare industr y 16 approximately US$150 billion a year by 2026. Accenture estimates that the top costs savers will come from “robot assisted surgery (US$40 billion), virtual nursing assistants (US$20 billion), administrative workflow assistance (US$18 billion), fraud detection (US$17 billion), dosage error reduction (US$16 billion), connected machines (US$14 billion)” 17 and similar tools. By 2021, the market for AI in healthcare is expected to reach US$6.6 billion, with an annual growth rate of 40 18 percent. The telemedicine market for virtual appointments is expected to become a US$1.49-billion industry by 2025, an annual 19 growth of nearly 20 percent. Thus, the role of AI is crucial in reducing healthcare costs, fostering innovation and creating positive economic output. Access to Data For AI to function properly, it needs massive amounts of data. If the data is flawed or biased, to begin with, the result will also be CHALLENGES Artificial Intelligence in Africa’s Healthcare: Ethical Considerations ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 5 flawed. Thus, the collection and monitoring of the training datasets that go into an AI algorithm are major challenges in AI use in healthcare. For the integration of ethics in AI, it is crucial to ensure the collection of unbiased, accurate data. In April 2019, the AI Now Institute published a study on gender, race and power in AI, calling out the lack of diversity in AI 20 workplaces and the bias in technologies. Public and private organisations alike are now taking notice of this problem. For example, facial recognition has become a hugely controversial technology because it tends to provide lower standards of recognition for African faces compared to Caucasian ones. This is likely the result of systems using training datasets that are primarily composed of Caucasian faces. Medical research can also produce false results if it fails to capture the nature of the whole of a population being treated. For example, symptoms of heart attacks in women present differently than in men. Current medical research datasets tend to focus on men. As a result, existing datasets on heart attacks, for instance, may not be as accurate for women. Similarly, health issues may change significantly across nations and ethnicities. Such variance has been observed in genetic disorders or genetic predispositions (e.g. diabetes is more prevalent in African American communities than in the US), disease prevention (e.g. a European disease prevention programme may not prioritise water-borne illnesses), and medical infrastructure (e.g. the AI system anticipates a state-of-the-art operating room rather than a basic medical facility in rural Lesotho.) Throughout history, key thought leaders have commented on the development of new forms of technological systems. Harvard Professor Jonathan Zittrain articulated how the generative internet and such systems are facilitating new kinds of control. Timnit Gebru, cofounder of “Black in AI,” has similarly discussed the diversity crisis in AI systems. Harvard Professor Cass Sunstein has written about how social technologies impact governance and society as well as how AI algorithms can be used to overcome the pitfalls of cognitive biases. For AI to work in the African healthcare sector, native researchers must be involved in the development of new technology, with African datasets informing such development. Any import of foreign AI technology must be done with awareness of its development process and limitations. Policymakers should have transparency into the algorithms and some understanding of the data supporting them. African datasets should be made available to researchers and companies working with imported AI tech, to ensure locally applicable outcomes. Since AI can only deliver what it has learnt, human engagement is necessary to ensure that the learning is unbiased and holistic. For policymakers, this means striking a balance between data access and personal privacy, and ensuring that African data is incorporated in AI development. Protecting Sensitive Data Personal health data—genetic information, biometric indicators such as fingerprints, a person’s HIV status—is often assigned the highest level of regulatory privacy protections. Data privacy and security are key to the Artificial Intelligence in Africa’s Healthcare: Ethical Considerations 6 ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 implementation of AI-based medical technologies, both for compliance purposes and public trust in these solutions. The breach of sensitive data can pose a serious threat to public safety, and the efficient and accurate treatment of patients. Any company leveraging AI techniques in healthcare must be particularly attuned to the data-regulation norms and the management of sensitive patient data, to avoid legal and ethical impropriety. Current data regulation standards for sensitive healthcare vary widely across regions. The European Union (EU), for instance, passed the comprehensive dataprotection law, General Data Protection Regulation (GDPR), in 2018; the US’ Health Insurance Portability and Accountability Act (HIPAA) handles treatment specifically for medical information. Laws protecting personal data are being developed across Africa, including in Kenya, Morocco, Nigeria and South Africa. While there is still a long way to go, some best practices can minimise the vulnerability of sensitive data, e.g. the anonymisation of all datasets used in algorithms, distributed ledgers, multifaceted cyber-security systems, encryption during storage and transmission, proper destruction of identifying information, data facility security, and targeted investment in IT infrastructure. Accountability Accountability mechanisms when managing healthcare information can promote integrity and durability in AI systems. By approaching AI systems with measured diffidence, a company can implement checks on the AI algorithm to reduce biases and promote holi s ti c ana l y s e s . A s tudy by the biopharmaceuticals company Syneos Health found one of the primary public concerns facing AI systems to be the “lack of human oversight and the potential for machine errors 21 leading to mismanagement of their health.” The following are some examples of current ethical checks. These can highlight best practices for accountability: 1 Cross-Sector Research Efforts: In 2018, the EU-backed EU High-Level Expert Group on Artificial Intelligence, which included industry professionals, non-governmental agencies and scholars, released a guidance note concerning AI Ethics. The AI-HLEG is a European Commission-backed working group comprising representatives from industry, 22 academia and NGOs. 2. I n d u str y - L e d E th ic a l P ri n ci p l e s: Entrepreneurs and major technology companies, such as Elon Musk, Peter Thiel, Sam Altman, Infosys, Microsoft and Amazon, have created a joint non-profit AI-research 23 company called Open AI. 3. Multilateral Cooperation: The International Telecommunication Union (ITU) and the WHO have partnered to create a Focus Group on AI, aiming to establish standards and guidelines for AI-based methods in the healthcare sector. On 4 May 2020, the ITU will hold their fourth annual workshop on AI for Good Global Summit, connecting innovators with problem owners for sustainable development. The ITU is an Africa-friendly forum, which may present opportunities for African AI researchers and innovators, as well 24 as related healthcare experts. Artificial Intelligence in Africa’s Healthcare: Ethical Considerations ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 7 In recent years, there has been increased global endeavour to establish basic principles of the ethical use of AI and accountability. However, current regulatory approaches to this field of technology remain mostly in the philosophical realm. Since this aspect of technolog y development is without precedent, there is little basis for formulating regulations. Moreover, the need to strike a careful balance between allowing t e c h n o l o g y g r ow t h a n d e n s u ri n g accountability, renders inadequate most of the current proposals. The GDPR brought many issues regarding data regulation to the forefront: how to ensure data privacy for individuals; the role of government in the regulation of technology; and the best ways to effectively and ethically leverage big data. AI systems have been subject to sectorspecific laws or subject-specific guidelines on a haphazard and piecemeal basis—such as dataprotection acts, cyber-security laws, antidiscrimination regulations—creating large regulatory gaps. However, fuelled by concerns regarding the ethical implications of AI usage, countries are now beginning to explore AIspecific guidelines and regulations. With the increased use of AI to perform tasks, analyse data and create new systems, regulations have begun to emerge. The public sector is catching up to these discussions of control, monitoring and bias. Nations are participating in the rapid development of AI and its best practices. The most significant geographies, in this regard, include the EU, the US, Singapore and Dubai; there are also some key international organisations. CRAFTING A POLICY EU The EU released the “Ethics Guidelines for Trustworthy AI” in April 2019, through a highlevel expert group on AI. The guidelines state key principles that AI systems should abide by, including respect for human autonomy, prevention of harm, fairness and explicability. For holding up these principles, it recommends seven key requirements, such as human agency, oversight, privacy, data governance, diversity, non-discrimination and fairness. The guidelines appropriately link the ethical discussion to the broader discussion surrounding data protection and privacy. US Under President Donald Trump’s Executive Order on AI, the National Institute of Standards and Technology has been tasked with creating a plan regarding federal standards for deploying AI technologies, produced on 10 August 2019. NIST solicited public comments to formulate the plan. The guide is focused on minimising vulnerability to attacks from bad actors, encouraging innovation, and promoting public confidence in AI. Singapore Singapore has released a framework on how AI can be ethically and responsibly used. It is intended to be a living document, evolving new perspectives and challenges emerge. Certain articles in the framework suggest a nuanced understanding of the challenges with aggregating data and preserving human autonomy. Article 3.6, for instance, acknowledges that individuals live in their unique societal contexts and recommends Artificial Intelligence in Africa’s Healthcare: Ethical Considerations 8 ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 that organisations operating in multiple countries consider differences in societal norms and values. Article 3.7 states that some risks to individuals may only manifest at the group level. However, there should be caution against adopting the stance that ethics and norms are overly subjective. The document allows corporations to decide their own ethics, in turn creating scope for the kind of subjectivity that will fail to establish ethical norms in AI. Dubai The Smart Dubai Office created the new Ethical AI Toolkit, which provides counsel to individuals and organisations offering AI services. Notably, the toolkit recommends making AI systems explainable, attempting to eliminate the “black box” issue surrounding them. It also suggests carefully examining whether the decision-making processes introduce bias. While the document offers some good high-level guidelines, it falls short on describing how such values should be implemented. Multilateral Organisations International organisations are eager to include AI-related topics in their agendas, work-plans and research topics. Around mid2020, the Organisation of Economic Cooperation and Development (OECD) is preparing to release AI guidelines, at a ministerial meeting and the International Telecommunications Union (ITU) is set to hold its third high-level “AI for Global Good Summit.” Other organisations active in this space include the World Intellectual Property Organisation, which recently issued a major report on AI; and the WHO, which is not only issuing its own report but also collaborating with the ITU to form a “focus group” on the topic; and the International Labour Organisation, which has a workstream on the “future of work.” While these discussions can seem detached from the national regulatory processes, they promise to be powerful platforms that shape norms and set acceptable parameters for national policymakers looking for guidance. Technology continues to evolve at a rapid pace, and AI-related progression will only accelerate that change. To successfully integrate AI into the healthcare industry, governments must aim to create regulations that promote ethics by design, i.e. including checks and balances into the systems that utilise AI. It is also important to define what these systems should contain and how the development process is structured. Moreover, issues of human agency and bias must be considered while creating the algorithms. As with all cost-benefit analysis, however, regulators must weigh the impact of regulation against the stifling of innovation. This brief recommends keeping in mind three key concepts: 1. Not all AI is the same. There are very different kinds of applications and uses for AI in healthcare. Regulations must take these differences into account and not impose a blanket prohibition on AI use. Further, issues of ethics, trust and fairness must be addressed in conjunction CONCLUSION Artificial Intelligence in Africa’s Healthcare: Ethical Considerations ORF ISSUE BRIEF No. 312 l SEPTEMBER 2019 9 with existing protections, such as consumer protection, consumer rights and data protection. 2. Policies must be well informed and align with the needs and values of the cultures represented in each African nation, as well as provide a holistic vision for a better future. Thus, AI regulation overlaps with areas such as data privacy, big tech and data regulation, consumer rights, ethics, social justice and law. The government must work with technology companies, researchers and academia, and civil society groups, putting aside differences in perceptions of ethical standards and social justice. This is the only way to ensure that they arrive at the most effective ways to regulate this sector. 3. AI allows for the creation of solutions in a new way that can mask the underlying logic. Underrepresented and historically marginalised communities are already victimised by existing systems that reinforce their positions in society. Therefore, all stages and elements of AI applications—training data, algorithms, effective performance—must be carefully examined to ensure fairness towards such groups. Governments have struggled to develop cyber-security and data-privacy norms that foster both security and growth. Today they face the additional challenge of developing norms that also address the ethical use of AI. However, governments cannot formulate regulations regarding the emerging uses of AI based on vague ethical ideas, since that will not only be ineffective but could also be detrimental to the innovative process. The imperative, therefore, is not policymaking but educating companies about their obligations and the society at large about the potential utility and risks of AI in healthcare. The governments of Africa should focus on “ethics by design” and be forwardlooking in technology implementation and use. These measures can help create effective long-term regulations while also allowing for innovative AI solutions for healthcare in African countries.