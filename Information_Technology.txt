11. Artificial Intelligence (AI) usage policy for Microsoft

Using Artificial Intelligence tools on Microsoft Q&A

AI tools garner much attention as they're more accessible to individuals with the quickly growing popularity of ChatGPT.

Many Microsoft Q&A contributors use these tools to help with the questions that are posted.

In May 2023, we launched Q&A Assist for Microsoft Q&A. Q&A Assist uses AI to help you find similar questions, ask better questions, and get answers from Microsoft Learn documentation.

These tools are focused on assisting question authors, and there isn't an option to use these tools to help answering questions.

This document explains the policy for using AI tools to answer questions on Microsoft Q&A. It also describes how the platform moderators handle violations of these guidelines.

AI usage policy for Microsoft Q&A

Follow these steps when using AI to generate an answer on Q&A:

Mention the AI service name that generated the answer, fully or partly.

Check the AI output accuracy and relevance and adjust it as needed. Indicate you validated or updated the AI output in your answer.

Include any sources AI generated or you used as you validated and updated the AI output.

Using AI to help with answers on Microsoft Q&A requires following these three steps. Otherwise, moderators might delete the content and suspend your account.

Moderator actions for AI-generated content on Microsoft Q&A

If a moderator finds AI-generated content on the platform that violates the requirements, they may take the following steps:

Reply to the content with a comment including information the platform's AI policy.

Delete the content that violates the policy, if the user doesn't respond to the notice or fails to make the necessary changes.

Suspend the user's account, if the user repetitively fails to make the necessary changes.

Use of alternative accounts to circumvent a suspension, would be considered in violation of our Code of Conduct for Microsoft Q&A.

Appeals process for deletions or suspensions due to AI usage on Microsoft Q&A

You can appeal the removal of your content from Q&A or the suspension of your account at this email address.





12. Apple Restricts Use of ChatGPT

Apple has restricted use of ChatGPT and other external artificial intelligence tools as it develops its own similar technology, according to a document reviewed by The Wall Street Journal and people familiar with the matter.

Apple is concerned workers who use these types of programs could release confidential data, according to the document. Apple also told its employees not to use Microsoft -owned GitHub’s Copilot, which automates the writing of software code, the document said.

ChatGPT, created by Microsoft-backed OpenAI, is a chatbot derived from a so-called large language model that is able to answer questions, write essays and perform other tasks in humanlike ways.

When people use these models, data is sent back to the developer to enable continued improvements, presenting the potential for an organization to unintentionally share proprietary or confidential information. OpenAI disclosed in March that it took ChatGPT temporarily offline because a bug allowed some users to see the titles from a user’s chat history.

An OpenAI spokeswoman pointed to an announcement last month where the company introduced the ability for users to turn off their chat history, which the company said would block the ability to train the AI model on that data.

Apple is known for its rigorous security measures to guard information about future products and consumer data. A number of organizations have also grown wary of the technology as its workers have begun using it for everything from writing emails and marketing material to coding software.

JPMorgan Chase and Verizon have barred use. David Banks, the chancellor of New York City’s schools, said in an op-ed published on Thursday that it rescinded its ChatGPT ban.

Amazon.com has urged its engineers who want to use ChatGPT for coding assistance to use its own internal AI tool, a spokeswoman recently told the Journal. Apple is also working on its own large language models, people familiar with the matter said.

Apple’s AI efforts are being led by John Giannandrea, whom Apple hired from Google in 2018. Under Mr. Giannandrea, a senior vice president at Apple reporting to Chief Executive Tim Cook,

Apple has acquired a number of artificial intelligence startups.

In Apple’s most recent quarterly earnings call with analysts, Apple Chief Executive Tim Cook expressed some concerns about advancements in this area also known as generative artificial intelligence.

“I do think it’s very important to be deliberate and thoughtful in how you approach these things,” Mr. Cook said. “And there’s a number of issues that need to be sorted as is being talked about in a number of different places, but the potential is certainly very interesting.”

Apple has also recently paid close attention to new software coming onto its iPhone App Store that takes advantage of generative artificial intelligence. When app developer Blix tried to update its email app BlueMail with a ChatGPT feature, Apple temporarily blocked the update on grounds that it could potentially show inappropriate content to children.

Apple’s review team asked the developer to either move up the app’s age restriction to 17 and older—it was set at 4 and older—or include content filtering. Once Blix assured Apple that it already had implemented content filtering on the ChatGPT feature, the app was approved.

On Thursday, OpenAI announced a ChatGPT app for the iPhone and iPad.

Apple was an early entrant into the consumer application of artificial intelligence when it launched the Siri voice assistant in 2011. But the company fell behind the likes of Amazon’s Alexa in subsequent years.



13. Google AI principles

Our Principles

While we are optimistic about the potential of AI, we recognize that advanced technologies can raise important challenges that must be addressed clearly, thoughtfully, and affirmatively.  These AI Principles describe our commitment to developing technology responsibly and work to establish specific application areas we will not pursue.

Objectives for AI applications

1. Be socially beneficial.

The expanded reach of new technologies increasingly touches society as a whole. Advances in AI will have transformative impacts in a wide range of fields, including healthcare, security, energy, transportation, manufacturing, and entertainment. As we consider potential development and uses of AI technologies, we will take into account a broad range of social and economic factors, and will proceed where we believe that the overall likely benefits substantially exceed the foreseeable risks and downsides.

AI also enhances our ability to understand the meaning of content at scale. We will strive to make high-quality and accurate information readily available using AI, while continuing to respect cultural, social, and legal norms in the countries where we operate. And we will continue to thoughtfully evaluate when to make our technologies available on a non-commercial basis.

2. Avoid creating or reinforcing unfair bias.

AI algorithms and datasets can reflect, reinforce, or reduce unfair biases. We recognize that distinguishing fair from unfair biases is not always simple, and differs across cultures and societies. We will seek to avoid unjust impacts on people, particularly those related to sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief.

3. Be built and tested for safety.

We will continue to develop and apply strong safety and security practices to avoid unintended results that create risks of harm. We will design our AI systems to be appropriately cautious, and seek to develop them in accordance with best practices in AI safety research. In appropriate cases, we will test AI technologies in constrained environments and monitor their operation after deployment.

4. Be accountable to people.

We will design AI systems that provide appropriate opportunities for feedback, relevant explanations, and appeal. Our AI technologies will be subject to appropriate human direction and control.

5. Incorporate privacy design principles.

We will incorporate our privacy principles in the development and use of our AI technologies. We will give opportunity for notice and consent, encourage architectures with privacy safeguards, and provide appropriate transparency and control over the use of data.

6. Uphold high standards of scientific excellence.

Technological innovation is rooted in the scientific method and a commitment to open inquiry, intellectual rigor, integrity, and collaboration. AI tools have the potential to unlock new realms of scientific research and knowledge in critical domains like biology, chemistry, medicine, and environmental sciences. We aspire to high standards of scientific excellence as we work to progress AI development.

We will work with a range of stakeholders to promote thoughtful leadership in this area, drawing on scientifically rigorous and multidisciplinary approaches. And we will responsibly share AI knowledge by publishing educational materials, best practices, and research that enable more people to develop useful AI applications.

7. Be made available for uses that accord with these principles.

Many technologies have multiple uses. We will work to limit potentially harmful or abusive applications. As we develop and deploy AI technologies, we will evaluate likely uses in light of the following factors:

Primary purpose and use: the primary purpose and likely use of a technology and application, including how closely the solution is related to or adaptable to a harmful use

Nature and uniqueness: whether we are making available technology that is unique or more generally available

Scale: whether the use of this technology will have significant impact

Nature of Google’s involvement: whether we are providing general-purpose tools, integrating tools for customers, or developing custom solutions

AI applications we will not pursue

In addition to the above objectives, we will not design or deploy AI in the following application areas:

1. Technologies that cause or are likely to cause overall harm. Where there is a material risk of harm, we will proceed only where we believe that the benefits substantially outweigh the risks, and will incorporate appropriate safety constraints.

2. Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.

3. Technologies that gather or use information for surveillance violating internationally accepted norms.

4. Technologies whose purpose contravenes widely accepted principles of international law and human rights.

As our experience in this space deepens, this list may evolve.



14. Meta Guideline to Responsible AI

Our commitment to Responsible AI



Our Responsible AI efforts are propelled by our mission to help ensure that AI at Meta benefits people and society. Through regular collaboration with subject matter experts, policy stakeholders and people with lived experiences, we’re continuously building and testing approaches to help ensure our machine learning (ML) systems are designed and used responsibly.

Meta’s five pillars of responsible AI that inform our work

We ground our work to ensure that AI is designed and used responsibly around a set of core values.

Privacy and security

Protecting the privacy and security of people’s data is the responsibility of everyone at Meta.

Fairness and inclusion

Everyone should be treated fairly when using our products and they should work equally well for all people.

Robustness and safety

AI systems should meet high performance standards, and should be tested to ensure they behave safely and as intended.

Transparency and control

People who use our products should have more transparency and control around how data about them is collected and used.

Accountability and governance

We build reliable processes to ensure accountability for our AI systems and the decisions they make.

Our progress and learnings in AI fairness and transparency

Datasets

Building diverse datasets and tools for more inclusive AI products

One way we are addressing AI fairness through research is the creation and distribution of more diverse datasets. Datasets that are used to train AI models can reflect biases, which are then passed on to the system. But biases might also be due to what isn’t in the training data. A lack of diverse data — or data that represents a wide range of people and experiences — can lead to AI-powered outcomes that reflect problematic stereotypes or fail to work equally well for everyone.

Privacy

Protecting privacy while addressing fairness concerns

Improving fairness will often require measuring the impact of AI systems on different demographic populations and mitigating unfair differences. Yet the data necessary to do so is not always available — and even when it is, collecting it and storing it can raise privacy concerns. After engaging with civil rights advocates and human rights groups that further confirmed the fairness challenges, we identified new approaches to help us access data with the potential to meaningfully measure the fairness of the AI models on our platforms across races.

Ad delivery

Innovating to improve fairness in ad delivery

A critical aspect of fairness is ensuring that people of all backgrounds have equitable access to information about important life opportunities, like jobs, credit, and housing. Our policies already prohibit advertisers from using our ad products to discriminate against individuals or groups of people. However, even with neutral targeting options and model features, factors such as people’s interests, their activity on the platform, or competition across all ad auctions for different audiences could affect how ads are distributed to different demographic groups. That’s why we’ve developed a novel use of machine learning technology to help distribute ads in a more equitable way on our apps.

Associations

Generating responsible associations

In 2022, we assembled a cross-disciplinary team, including people from our Civil Rights, Engineering, AI Research, Policy, and Product teams, to better understand problematic content associations in several of our end-to-end systems and to implement technical mitigations to reduce the chance of them occurring on our platforms that use AI models.

As part of this collaborative effort, we carefully constructed and systematically reviewed the knowledge base of interest topics for usage in advanced mitigations that more precisely target the problematic associations. As more research is done in this area and shared with the greater community, we expect to build on this progress and to continue to improve our systems.

AI-driven feeds and recommendations

Giving more control over AI-driven feeds and recommendations

AI-driven feeds and recommendations are a powerful tool for helping people find the people and content they are most interested in, but we want to make sure that people can manage their experience in ways that don’t necessarily rely on AI-based ranking.

System cards

Developing new methods for explaining our AI systems

Because AI systems are complex, it is important that we develop documentation that explains how systems work in a way that experts and nonexperts alike can understand.

Policy approaches

Testing new policy approaches to AI transparency, explainability, and governance

The rapid advance of emerging technologies makes it difficult to fully understand and anticipate how they might eventually impact communities around the world.



15. AWS Responsible AI Policy

Last Updated: September 28, 2023

This AWS Responsible AI Policy (“Policy”) applies to your use of artificial intelligence and machine learning Services, features, and functionality (including third-party models) that we provide (collectively, “AI/ML Services”). This Policy supplements the AWS Acceptable Use Policy and AWS Service Terms.

Prohibitions. You may not use, or facilitate or allow others to use, the AI/ML Services:

for intentional disinformation or deception; 

to violate the privacy rights of others, including unlawful tracking, monitoring, and identification; 

to depict a person’s voice or likeness without their consent or other appropriate rights, including unauthorized impersonation and non-consensual sexual imagery; 

for harm or abuse of a minor, including grooming and child sexual exploitation;

to harass, harm, or encourage the harm of individuals or specific groups; 

to intentionally circumvent safety filters and functionality or prompt models to act in a manner that violates our Policies;

to perform a lethal function in a weapon without human authorization or control.

Responsible AI Requirements. If you use the AI/ML Services to make consequential decisions, you must evaluate the potential risks of your use case and implement appropriate human oversight, testing, and other use case-specific safeguards to mitigate such risks. Consequential decisions include those impacting a person’s fundamental rights, health, or safety (e.g., medical diagnosis, judicial proceedings, access to critical benefits like housing or government benefits, opportunities like education, decisions to hire or terminate employees, or access to lending/credit, and providing legal, financial, or medical advice). You agree to provide information about your intended uses of the AI/ML Services and compliance with this Policy upon request.

You and your end users are responsible for all decisions made, advice given, actions taken, and failures to take action based on your use of AI/ML Services. AI/ML Services use machine learning models that generate predictions based on patterns in data. Output generated by a machine learning model is probabilistic, and generative AI may produce inaccurate or inappropriate content. Outputs should be evaluated for accuracy and appropriateness for your use case.

AWS may investigate and enforce violations of this Policy as noted in the AWS Acceptable Use Policy. AWS is committed to developing safe, fair, and accurate AI and ML services and providing you with tools and guidance to assist you in building and using AI and ML applications responsibly, see our Responsible Use of AI and ML page for additional tools and resources.

Building AI responsibly at AWS

The rapid growth of generative AI brings promising new innovation, and at the same time raises new challenges. At AWS we are committed to developing AI responsibly, taking a people-centric approach that prioritizes education, science, and our customers, to integrate responsible AI across the end-to-end AI lifecycle.

Core dimensions of responsible AI

Fairness

Considering impacts on different groups of stakeholders

Explainability

Understanding and evaluating system outputs

Privacy and security

Appropriately obtaining, using, and protecting data and models

Safety

Preventing harmful system output and misuse

Controllability

Having mechanisms to monitor and steer AI system behavior

Veracity and robustness

Achieving correct system outputs, even with unexpected or adversarial inputs

Governance

Enabling stakeholders to make informed choices about their engagement with an AI system

Transparency

Incorporating best practices into the AI supply chain, including providers and deployers



16. Oracle Generative AI strategy

Artificial intelligence (AI) is fundamentally changing the way that we interact with the world. This change presents both opportunities and challenges for organizations that want to take advantage of new AI technologies. Nowhere is this idea truer than with generative AI. Generative AI models combine the ability to assimilate knowledge from many sources and use it to automate tasks and enhance human creativity and productivity. Using this technology, organizations can summarize documents, build tables, create new and meaningful text, generate code, and synthesize ideas.

Many vendors are trying to move quickly in this space, but no one is addressing the specific, end-to-end needs of enterprise customers with generative AI.

Oracle’s strategy aims at the enterprise

Oracle’s strategy is built around the reality that enterprises work with AI through three different modalities: Infrastructure, models and services, and within applications.

First, we provide a robust infrastructure for training and serving models at scale. Through our partnership with NVIDIA, we can give customers superclusters, which are powered by the latest GPUs in the market connected together with an ultra-low-latency RDMA over converged ethernet (RoCE) network. This solution provides a highly performant, cost-effective method for training generative AI models at scale. Many AI startups like Adept and MosaicML are building their products directly on OCI.

Second, we provide easy-to-use cloud services for developers and scientists to utilize in fully managed implementations. We’re enabling new generative AI services and business functions through our partnership with Cohere, a leading generative AI company for enterprise-grade large language models (LLMs). Through our partnership with Cohere, we’re building a new generative AI service. This upcoming AI service, OCI Generative AI, enables OCI customers to add generative AI capabilities to their own applications and workflows through simple APIs.

Third, we embed generative models into the applications and workflows that business users use every day. Oracle plans to embed generative AI from Cohere into its Fusion, NetSuite, and our vertical software-as-a-service (SaaS) portfolio to create solutions that provide organizations with the full power of generative AI immediately. Across industries, Oracle can provide native generative AI-based features to help organizations automate key business functions, improve decision-making, and enhance customer experiences. For example, in healthcare, Oracle Cerner manages billions of electronic health records (EHR). Using anonymized data, Oracle can create generative models adapted to the healthcare domain, such as automatically generating a patient discharge summary or a letter of authorization for medical insurance.

Oracle’s generative AI offerings span applications to infrastructure and provide the highest levels of security, performance, efficiency, and value.

The Oracle difference for generative AI

Many major cloud providers are coming out with generative AI offerings right now. But what makes Oracle best suited for generative AI? Some of the differences are clear, such as an established history of storing the world’s most business-critical, valuable data, a modern data platform, and low-cost, high-performance AI infrastructure.

However, more factors demonstrate why Oracle can declare that its generative AI offering is truly built for enterprises with an approach that spans considerations, such as cloud-to-on-premises data, deployment to business apps, security, data privacy, and now the best LLMs for enterprise success.

Powerful and high-performing models

Oracle works to create the best models for your organization using its unique data and industry knowledge. From acquisitions, such as Cerner, and experience through our suite of business apps, Oracle trains specialized models that are unique to verticals and our industry-leading SaaS solutions. But Oracle’s approach further enables organizations to refine these prebuilt models using their own data, so the models understand an organization’s business like no other.

Unrivaled data security, privacy, and governance

As customers refine and train prebuilt generative AI models with their own data, they can also trust that Oracle continues to fully protect data. Unlike other generative AI offerings, Oracle’s generative AI doesn’t mix customer data. The models trained by customers are unique to them, and tools for accessing data provenance and lineage are available. Oracle protect data privacy and sovereignty.

Embedded generative AI services

Oracle is integrating and embedding AI across its portfolio of cloud applications, including CRM, ERP, HCM, CX and EMR applications. In addition, Oracle is making generative AI capabilities available in its database portfolio in the same way that it introduced machine learning (ML) features in Oracle Database service and MySQL HeatWave.

Generative AI available wherever customers need it

Customers can use Oracle’s upcoming OCI Generative AI service on Oracle Cloud Infrastructure (OCI) and reap all the benefits of public cloud: Pay for what you use, scale on demand, customize models, and create private model endpoints.

Oracle can also deliver the complete AI stack to organizations’ data centers with cloud deployment options, such as OCI Dedicated Region, to enable organizations to combine generative AI capabilities together with their on-premises data and applications.

Join the AI revolution

As Oracle Chairman and CTO Larry Ellison highlighted in the Q4 FY23 earnings call, a technology revolution is dawning, one in which AI plays a central role. Oracle envisions a world where AI models can help humans more quickly extract insights from enterprise data, augment human creativity and empower human-centric interfaces to solve real-world problems. Join the revolution with Oracle!



17. Adobe Generative AI Guideliens

Adobe Generative AI User Guidelines

 

Last Updated: May 10, 2024

 

These Generative AI User Guidelines (“Guidelines”) govern your use of Adobe’s generative AI features. In addition, if your agreement with Adobe is governed by the General Terms of Use located at adobe.com/go/terms, then your use of these generative AI features is also governed by the Adobe Generative AI Additional Terms located at adobe.com/go/adobe-gen-ai-addl-terms, which are incorporated by reference into these Guidelines.


These Guidelines have two goals: to maintain the high quality of content generated using Adobe’s suite of products and services, and to keep our products and services accessible to our users in an engaging and trustworthy way that fosters creativity and productivity.

 

1. No AI/ML Training

 

When using our generative AI features, you agree you will use them only for your creative and productivity work product and not to train artificial intelligence or machine learning models.


This means you must not, and must not allow third parties to, use any content, data, output or other information received or derived from any generative AI features, including any Firefly outputs, to directly or indirectly create, train, test, or otherwise improve any machine learning algorithms or artificial intelligence systems, including any architectures, models, or weights.


2. Be Respectful and Safe

 

Do not use Adobe’s generative AI features to attempt to create, upload, or share abusive, or illegal, or content that violates the rights of others. This includes, but is not limited to, the following:

Pornographic material or explicit nudity

Hateful or highly offensive content that attacks or dehumanizes a group based on race, ethnicity, national origin, religion, serious disease or disability, gender, age, or sexual orientation

Graphic violence or gore

The promotion, glorification, or threats of violence

Illegal activities or goods

Self-harm or the promotion of self-harm

Depictions of nude minors or minors in a sexual manner

Promotion of terrorism or violent extremism

Dissemination of misleading, fraudulent, or deceptive content that could lead to real-world harm

Private information of others

 

Your prompts and the results generated by generative AI features in Creative Cloud products may be reviewed through both automated (e.g., machine learning) and manual methods for abuse prevention and content filtering purposes.

 

Please note that we may report any material exploiting minors to the National Center of Missing & Exploited Children (NCMEC).

 

If at any time you believe someone has violated these Guidelines, please report it by contacting us at abuse@adobe.com.

 

3. Be Authentic

 

We disable accounts that engage in behavior that is deceptive or harmful, including:

Using fake, misleading, or inaccurate information in your profile

Impersonating other people or entities

Using unauthorized automated or scripting processes (such as bulk or automated uploading of content through a script)

Engaging in schemes or third-party services to boost account engagement (artificially increasing the number of appreciations, views, or other metrics)

 

4. Be Respectful of Third-Party Rights

 

Using Adobe’s generative AI features to create, upload, or share content that violates third-party copyright, trademark, privacy, publicity, or other rights is prohibited. This may include, but is not limited to, entering text prompts designed to generate copyrighted, trademarked, or otherwise infringing content, uploading an input or reference image that includes a third party’s copyrighted content, generating text that plagiarizes third-party content, or using a third party’s personal information in violation of their privacy or data protection rights. If you’re not sure whether your content violates the rights of a third party, you may want to reach out to an attorney or consult publicly available reference materials at the following:

U.S. Copyright Office

U.S. Patent & Trademark Office

Lumen

 

If you want to report the misuse of your own creative work or your own intellectual property by one of our users, you can do that here: Intellectual Property Removal Policy located at https://www.adobe.com/legal/dmca.html. If you have a contract or other dispute with an Adobe user regarding content they have uploaded to our products and services, please resolve the issue directly with the user. We can’t moderate contract, employment, or other disputes between our users and the public.

 

5. Use Your Judgment

 

Generated outputs sometimes may be inaccurate or misleading, or otherwise reflect content that does not represent Adobe’s views. As a result, please use your judgment to review and validate generated outputs.

 

6. No Professional Advice

 

Generative AI features are not intended for professional advice. Do not use generative AI features to seek or provide legal, medical, financial, or other kinds of professional advice or any opinions, judgments, or recommendations without conducting your own independent consultation or research. Generative AI features cannot replace advice provided by a qualified professional and do not form any such relationship (e.g., attorney-client relationship).

 

7. Content Credentials

 

Adobe may attach or publish Content Credentials for content created with generative AI features to let people know it was generated with AI.  Learn more about Content Credentials.

 

8. Commercial Use

 

In general, you may use outputs from generative AI features commercially. However, if Adobe designates in the product or elsewhere that a beta version of a generative AI feature cannot be used commercially, then the generated outputs from that beta feature are for personal use only and cannot be used commercially.

 

9.  Additional Content Sources

 

The output from our generative AI features may include non-generative content (such as Adobe Stock assets) that is subject to the terms of your agreement with Adobe. This non-generative content will be noted in the user interface or related documentation.


10. More Information

 

We may take action on your Adobe account if we discover content or behavior that violates these Guidelines. For more information about what you can and can’t do while using Adobe’s generative AI features, please refer to your agreement with Adobe.



18. Qualcomm AI strategy

AI is transforming everything. We are making AI ubiquitous.

Today, more intelligence is moving to end devices, and mobile is becoming the pervasive AI platform. Building on the smartphone foundation and the scale of mobile, Qualcomm envisions making AI ubiquitous—expanding beyond mobile and powering other end devices, machines, vehicles, and things. We are inventing, developing, and commercializing power-efficient on-device AI, edge cloud AI, and 5G to make this a reality.

Creating new intuitive experiences and interactions.

AI enables devices and things to perceive, reason, and act intuitively. Drawing inspiration from the human brain, AI will expand our human abilities by serving as a natural extension of our senses. It will also personalize our experiences through seamless interactions in our everyday life.

Reshaping industries and creating new possibilities.

Goldman Sachs predicts that generative AI could raise global GDP by almost $7 trillion over a 10-year period. Hybrid AI, which distributes processing between the device and cloud, is a key part of achieving this growth across the global economy. On-device AI processing offers many benefits, including user privacy, security, immediacy, enhanced reliability, and cost. The pace, scale, and high integration that drive the mobile industry is allowing AI to scale to billions of devices. The power-efficient AI technology that we research and develop is fundamental across industries and products, spanning from smartphones, PCs, and XR to IoT and automotive.

The future of AI is hybrid.

As generative AI adoption grows at record-setting speeds and drives higher demand for compute, AI processing must be distributed between the cloud and devices for AI to scale and reach its full potential. Beyond cost, a hybrid AI architecture offers the additional benefits of performance, personalization, privacy, and security. The cloud and edge devices work together to deliver more powerful, efficient, and highly optimized AI. The hybrid AI approach is applicable to virtually all generative AI applications and device segments – including phones, laptops, XR headsets, cars, and IoT. 

We are making hybrid AI a reality.

Qualcomm is enabling intelligent computing everywhere. As the on-device AI leader, Qualcomm Technologies is uniquely positioned to scale hybrid AI with industry-leading hardware and software solutions across billions of edge devices. Our hardware offers industry-leading performance per watt, and our perpetual flywheel of innovation keeps us at the forefront of on-device AI solutions. With our technology leadership, global scale, and ecosystem enablement, Qualcomm Technologies is making hybrid AI a reality.

We are the R&D engine fueling the AI industry.

With over a decade of AI research and development, we are creating the essential components to scale AI across industries. To make AI ubiquitous, we are focused on efficient hardware, algorithmic advancements, and software tools. Our research is diverse, and we are keenly focused on power efficiency and personalization to make AI seamless across our everyday experiences.

AI and 5G will power the era of distributed intelligence.

Our technology inventions are not only driving power efficient AI, but are also leading the world to 5G—the foundation for an intelligently connected future. The combination of these two technology platforms, led by Qualcomm, will scale intelligence to trillions of connected things.

AI is here today.

With our industry-leading technology in 5G and low-power heterogeneous computing, we are well positioned to lead the AI revolution. We’ve introduced a broad portfolio of products across a variety of industries to make AI ubiquitous. From efficient AI hardware and software to comprehensive software development kits, we are collaborating with the industry to bring exciting new experiences to the world.

Qualcomm Vision Intelligence Platform

The Qualcomm® Vision Intelligence Platform brings powerful visual computing and edge computing for machine learning to a variety of IoT devices.



AI Development Tools

Discover our AI software development kits, documentation, forum, and other resources to kickstart your projects from the Qualcomm Developer Network.



Learn more about our AI resources





AI Open Source Projects

Qualcomm Innovation Center open sourced the AI Model Efficiency Toolkit (AIMET), which includes state-of-the-art quantization and compression techniques.

Power efficiency

Power efficiency is essential for AI. We research deep learning techniques for power-efficient implementations across hardware, algorithms, and software. 



Download presentation





Personalization

Personalization is essential for superior user experiences. We research continuous learning techniques that allow adaption to user behavior and preferences. 

AI is here today.

With our industry-leading technology in 5G and low-power heterogeneous computing, we are well positioned to lead the AI revolution. We’ve introduced a broad portfolio of products across a variety of industries to make AI ubiquitous. From efficient AI hardware and software to comprehensive software development kits, we are collaborating with the industry to bring exciting new experiences to the world.

Qualcomm Vision Intelligence Platform

The Qualcomm® Vision Intelligence Platform brings powerful visual computing and edge computing for machine learning to a variety of IoT devices.



AI Development Tools

Discover our AI software development kits, documentation, forum, and other resources to kickstart your projects from the Qualcomm Developer Network.



Learn more about our AI resources





AI Open Source Projects

Qualcomm Innovation Center open sourced the AI Model Efficiency Toolkit (AIMET), which includes state-of-the-art quantization and compression techniques.



Learn more about open sourcing AIMET





19. Generative AI: 5 Guidelines for Responsible Development

Editor’s Note: AI Cloud, Einstein GPT, and other cloud GPT products are now Einstein. For the latest on Salesforce Einstein, go here.

Generative artificial intelligence (AI) has the power to transform the way we live and work in profound ways and will challenge even the most innovative companies for years to come.

But generative AI is not without risks. It gets a lot of things right but many things wrong. As businesses race to bring this technology to market, it’s critical that we do so inclusively and intentionally. It’s not enough to deliver the technological capabilities of generative AI, we must prioritize responsible innovation to help guide how this transformative technology can and should be used — and ensure that our employees, partners, and customers have the tools they need to develop and use these technologies safely, accurately, and ethically.

Generative AI at Salesforce

The potential for generative AI at Salesforce — and enterprise technology more broadly — is vast.

AI is already an integral part of Einstein 1, and our Einstein AI technologies deliver nearly 200 billion predictions every day across Salesforce’s business applications, including:

Sales, which uses AI insights to identify the best next steps and close deals faster.

Service, which uses AI to have human-like conversations and provide answers to repetitive questions and tasks, freeing up agents to handle more complicated requests.

Marketing, which leverages AI to understand customer behavior and personalize the timing, targeting, and content of marketing activities.

Commerce, which uses AI to power highly personalized shopping experiences and smarter ecommerce.

Now, generative AI has the potential to help our customers connect with their audiences in new, more personalized ways across many sales, customer service, marketing, commerce, and IT interactions. We’re even exploring the use of AI-generated code to help our customers – even those without certified Salesforce developers on staff – write high-quality code faster, using fewer lines of code and therefore, requiring less CPU.

Building Generative AI We Can Trust

INSIGHTS FROM SALESFORCE'S CHIEF SCIENTIST



Guidelines for Trusted Generative AI

Like all of our innovations, we are embedding ethical guardrails and guidance across our products to help customers innovate responsibly — and catch potential problems before they happen.

Given the tremendous opportunities and challenges emerging in this space, we’re building on our Trusted AI Principles with a new set of guidelines focused on the responsible development and implementation of generative AI.

We are still in the early days of this transformative technology, and these guidelines are very much a work in progress — but we’re committed to learning and iterating in partnership with others to find solutions.

Below are five guidelines we’re using to guide the development of trusted generative AI, here at Salesforce and beyond.

Accuracy: We need to deliver verifiable results that balance accuracy, precision, and recall in the models by enabling customers to train models on their own data. We should communicate when there is uncertainty about the veracity of the AI’s response and enable users to validate these responses. This can be done by citing sources, explainability of why the AI gave the responses it did (e.g., chain-of-thought prompts), highlighting areas to double-check (e.g., statistics, recommendations, dates), and creating guardrails that prevent some tasks from being fully automated (e.g., launch code into a production environment without a human review).

Safety: As with all of our AI models, we should make every effort to mitigate bias, toxicity, and harmful output by conducting bias, explainability, and robustness assessments, and red teaming. We must also protect the privacy of any personally identifying information (PII) present in the data used for training and create guardrails to prevent additional harm (e.g., force publishing code to a sandbox rather than automatically pushing to production).

Honesty: When collecting data to train and evaluate our models, we need to respect data provenance and ensure that we have consent to use data (e.g., open-source, user-provided). We must also be transparent that an AI has created content when it is autonomously delivered (e.g., chatbot response to a consumer, use of watermarks).

Empowerment: There are some cases where it is best to fully automate processes but there are other cases where AI should play a supporting role to the human — or where human judgment is required. We need to identify the appropriate balance to “supercharge” human capabilities and make these solutions accessible to all (e.g., generate ALT text to accompany images).

Sustainability: As we strive to create more accurate models, we should develop right-sized models where possible to reduce our carbon footprint. When it comes to AI models, larger doesn’t always mean better: In some instances, smaller, better-trained models outperform larger, more sparsely trained models.



20. CGI optimizing generative AI potential

The rush to bring the benefits of AI and generative AI to federal technology applications has dominated the mainstream media. Solution and services providers to the federal government are racing to position and expand with new AI capabilities, which brings new challenges.

Unlike many other commercial solutions, CGI’s Momentum ® is specifically designed around the nuances of federal regulations and a leading choice for today’s federal agencies for budgeting, financial management, acquisitions, and asset management (as provided by CGI’s Sunflower solution).  

At CGI, our approach with generative AI for our Momentum and Sunflower solutions focuses on three principles of choice, support, and security. For this first blog in a series, I will discuss the aspects of what “choice” means with our Momentum and Sunflower solutions.

An agnostic approach enables agencies to choose with freedom from lock-in

Every agency is unique with different missions, environment, technology. While entering into the AI realm for efficiencies is exciting, agencies need to proceed with caution as new capabilities bring new risks and vulnerabilities. The White House continues to provide guidance on this topic with its latest executive order.

At CGI, we understand that balancing business and mission priorities is a delicate process and federal agencies need a solution that is flexible to keep up changes in mandates, policies, and the technology landscape. Our approach to generative AI brings all the benefits to federal agencies, but with the highest levels of flexibility and choice. We’ve embedded application programming interfaces (APIs) and accelerator packages into our Momentum® and Sunflower™ solutions, enabling customers to leverage generative AI capabilities with their own data, using their own data models or models we’ve provided in the systems. In other words, the customer controls the data set and AI interface.

Our AI vision and Momentum ® and Sunflower ™ solutions roadmaps are aligned with the principles of remaining agnostic as well as bring-your-own-consumption model (not CGI-owned). As a systems integrator, our approach to AI is much broader with the ability to connect to any model, cloud, or tool and our federal clients have ultimate flexibility to seamlessly add new AI capabilities into their existing applications. This approach allows agencies to gain new capabilities and leverage competitive pricing models, without being locked into a specific approach, engine, or tool.

Choose an AI journey unique to your agency

The power of generative AI comes from learning using large sets of data; as a system ingests and analyzes vast amounts of data and information, it gains a wide array of choices it will later use when generating its own text. As our generative AI and accelerator packages are cloud and tool agnostic, using the right APIs enables the AI functions in any IT environment.

However, it is important to train the AI on data that is directly relevant to the system’s intended use. This is where Momentum and Sunflower clients can experiment and refine their approach, allowing the system to provide specific agency data and therefore training the system directly on the agency’s information parameters for expected outcomes and processes.  This reduces the likelihood of “hallucinations” that can occur when generative AI models presents incorrect responses based on the highest-probability data or bias in large data sets. Our accelerator packages, which are prepackaged solutions that work against Momentum and Sunflower data, reduce the hurdles of adoption, whereby agencies can access generative AI benefits against the Momentum datasets with proven AI solutions. 

Intentional use of algorithms to yield expected outcomes

Before deploying any application, agencies need to show how the algorithms produce specific output. That proof point could also become a point of improper disclosure.

Therefore, we urge agencies to be cautious about claims that combining multiple agencies’ data into a single training set is the way to go. To the contrary, agencies want to train algorithms only with the data sufficient to get your required outcomes.

Agencies must, in their AI programs, build in compliance with statutes and executive orders connected to Health Insurance Portability and Accountability Act (HIPAA) information and any type of PII and controlled but unclassified information (CUI). They should add any agency-specific prohibitions on information they are allowed to share or aggregate.

To be sure, thanks to our work at multiple agencies, CGI has developed big data sets. For instance, in our Momentum application, we handle data covering a significant percentage of federal spending. But we also pay strict attention to how users can easily query the data they need and retrieve only the appropriate data for the result they want—and are authorized to access.

We understand that each agency has its own PII and otherwise sensitive data. That is why we use controlled, measured approaches to each client’s application. Our aim is to enable agencies to create AI applications that make them more productive and efficient, without introducing undue cybersecurity or disclosure risk. We recognize that an agency’s data sets belong to the agency and are not to be shared where such sharing is prohibited.

AI has become more powerful and versatile than it once was, and federal agencies appear to be ready to experiment and try AI applications. Agencies that are careful, thoughtful and judicious in how they apply AI stand to reap the rewards.

Finding the right use cases

As much as any other technology, AI excels in some use cases, and is not helpful at all in others. Agencies should analyze possible implementations to determine where AI would produce quick, easy and significant results.  Specific implementations and use cases allow low-consequence experiments. CGI’s federal experts are well-versed in helping clients define and implement use cases to understand benefits and optimize outcomes.  

Some use cases that agencies can explore include:

Predictive analytics—Many agencies respond to challenges reactively. With AI, you get ahead of events. The system sends warning signs, so you act proactively and minimize problems.

Automate and streamline contract closeouts or other processes: With contract closeouts, the system recognizes when a device reaches its end of life or when the expenditure has hit its ceiling and automatically closes out the contracts rather than have an employee manually take those steps.

Trend analysis: AI systems collect, consolidate, correlate, and deliver new insights. Agencies identify how their internal organizations have been operating and can see how to make them better and more effective.

Natural language processing (NLP):  Incorporating natural language processing into customer interactions provides self-service, reduces processing time and lowers errors, all of which improve the citizen experience.

A vision for the future

CGI is your partner to understand, develop, and implement generative AI within your agency to meet your goals. Our experts can provide guidance and insight into proven strategies from technology to implementation. Whether you’re seeking to optimize Momentum or Sunflower or create unique use cases to inform your agency vision, CGI is your proven partner.

For more information about CGI’s AI capabilities in the federal marketplace, visit our website. 

For more information on Momentum and Sunflower, visit www.cgi.com/Momentum and www.cgi.com/Sunflower. 





21. LG Presents ‘AI Ethics Principles’ for Trustworthy AI Research

LG Presents ‘AI Ethics Principles’ for Trustworthy AI Research

LG presented its “AI Ethics Principles” that serve as the basis for right action and value judgment of all LG employees who research and use AI. It is a significant step forward in the development of ESG management.

The principles were carefully selected based on LG’s management principles of “Creating Values for Customers” and “People-oriented Management” as well as core values for developing trustworthy AI. LG plans to establish a concrete implementation plan so that its AI Ethics Principles do not violate or regulate the autonomy of research and is further employed to develop ethical AI which adds more value to customers’ life beyond technology.

LG’s AI Ethics Principles consist of 5 core values: Humanity, Fairness, Safety, Accountability, and Transparency. Each of them is further described below:

 

■ Humanity
• LG AI provides benefits to humanity and society.
• LG AI does not violate human rights.

■ Fairness
• LG AI treats all people fairly while respecting diversity.
• LG AI avoids unfair discrimination based on individual characteristics.

■ Safety
• LG AI operates in a safe and robust manner.
• LG AI predicts and responds to potential risks.

■ Accountability
• LG AI clearly defines roles and responsibilities for those involved in development and use.
• LG AI strives to fulfil the responsibility to operate appropriately in line with the intention of the human.

■ Transparency
• LG AI is transparent with our customers to help them better understand and trust how AI works.
• LG AI follows principles and standards to ensure transparency in our algorithms and data.

 

As the hub of AI research for the entirety of LG Group, LG AI Research newly established “AI Ethical Inspection TF (Task Force)” in order to properly publicize the importance of the AI Ethics Principles and to review and implement measures to practically apply them in various tasks. The AI Ethical Inspection TF aims to train all the members of LG on the AI Ethics Principles, thereby allowing them to contemplate on AI technology and its social responsibility. The TF is also in charge of examining ethical problems that may occur in the course of AI research and development.

With regard to the presentation of the principles, the Chief of LG AI Research Kyunghoon Bae commented, “The valuation of AI will vary depending on how people regard and use AI. Through ‘LG AI Ethics Principles’ LG will continue our research on the coexistence of human and AI and create genuine customer values.”

By the end of this year, LG AI Research plans to create an “AI Ethics Working Group” in collaboration with 10 LG subsidiaries to discuss major AI ethics issues. AI Ethics Working Group will be comprised of members from LG Electronics, LG Display, LG Innotek, LG Chem, LG Energy Solution, LG Household & Health Care, LG Uplus, LG HelloVision, and LG CNS. The data and case studies accumulated by AI Ethical Inspection TF and AI Ethics Working Group will be utilized to create ethical guidelines for each and every field of AI that LG is currently involved by the year 2023.

Since its foundation at the end of 2020, LG AI Research has spared no efforts in researching trustworthy and ethical AI technologies for its customers. One of the most notable technologies is the “hate speech detection” technology designed to help customers avoid hateful and discriminatory statements when using customer service chatbots or other AI services. Started as the idea to resolve the issue of malicious comments which are the root cause of various social conflicts, it is currently under development to not only analyze the words but also the context of each sentence to filter out aggressive and biased information.

“Explainable AI (XAI)” is another ethical AI technology developed by LG AI Research. XAI is an AI technology that can help humans to understand how and why an AI arrived at a specific decision and what, if any, caused an error in the process. For example, during a vision inspection in an automated manufacturing process, the AI not only analyzes an image and determines whether a product is defective or not, but also explains to the operator why the product is determined to be defective. Therefore, it is a technology that is essential in developing expert AI that helps humans make decisions in fields of expertise that require trustworthiness and transparency, such as medicine, law and finance.

In order to develop an ethical AI that is trustworthy, transparent and accountable, LG AI Research is advancing XAI technologies and at the same time, collaborating with multiple LG affiliates to explore ways to utilize them in our daily lives. Utilizing XAI technology, LG AI Research is building a demand forecasting model in collaboration with LG Electronics and LG Household & Health Care, and developing a predictive model for customized immune chemotherapy treatment in collaboration with LG Chem. Furthermore, LG AI Research is working closely with Seoul National University Graduate School of AI and the University of Michigan on technologies associated with AI ethics to develop safer and more ethical AI technologies. LG AI Research will continue to exert our utmost efforts to present high-performance, trustworthy, and safe AI for customers.



22. Fujitsu Generative AI use guidelines

Fujitsu Generative AI Guidelines January, 2024 ver1.1 © 2024 Fujitsu Limited Introduction To stakeholders who read this document This guideline for the use of Generative AI has been prepared by the Fujitsu Group for our employees. It explains the general risks of Generative AI and examples of countermeasures especially from an ethical and legal perspective. Our company employees take appropriate risk mitigation measures in accordance with this guideline and provide safe and secure Generative AI services to society. The Fujitsu Group has decided to make this guideline available not only to our company employees but also to stakeholders to help society in understanding how to use Generative AI. We hope it will be useful for you. Generative AI Overview © 2024 Fujitsu Limited "Generative AI" refers to a type of AI that generates human-like natural outputs which hardly seem to be auto-generated, in response to simple prompts such as words and sentences. Its generating capacity has already reached a indistinguishable level from that of human intellectual activity. Anyone can generate content quickly and easily without the experience, expertise, or technology Generative AI Use Cases Generative AI is capable of generating a variety of content such as text, program code, image and video, music. For example, it can be used in a wide range of tasks that require intellectual creation, such as: Text Program code Image and video Music Rule-based simple task Grammar check and proofreading Debugging Noise Removal, High Resolution Trimming, Loop Creation Improvement in quality Summaries, Translations Program structure improvement Avoid Errors/Bugs Monochrome → Color Image generation Illustration - Photo/Movie generation Arrangement of an existing song Assistance for content creation Writing idea proposal Module code generation Photo and video generation from text Melody and rhythm generation Content generation Thesis and story generation App Design Document generation Proprietary app generation Photobook generation Comic book generation Creating original music styles and genres Reference: How does ChatGPT work? As of fiscal year 2023, ChatGPT is the leading text generation AI tool. Its overwhelmingly huge amount of knowledge (more than several hundreds of GB of training data and hundreds of billions of parameters) offers a significantly broader range of expression. ChatGPT predicts the word most likely to follow the previous word, based on the past training data and other sources. It generates text by repeating this prediction process. It is important note that ChatGPT just predicts and generates the most probable combination of words. It does not verify the factual accuracy of its output. Risks Brought by Generative AI Generative AI is a very useful tool, but it involves legal and ethical risks such as copyright infringement and information leaks. However, these risks are not unique that suddenly arose with the introduction of Generative AI; they have existed in conventional AI as well. Risks and countermeasures are summarized mainly from the viewpoint of Generative AI users, assumed use within the scope of Japanese law. 1. Accuracy The training data for Generative AI often includes a massive amount of web content that has not yet determined to be correct or false. In some cases, Generative AI are trained only on outdated data. Also, as mentioned on page 7, Text Generation AI such as ChatGPT generates text by repeating "predicting probabilities," during the generation process, it does not verify the factual accuracy of its output. As a result, errors due to lack of "accuracy" can appear in the product. If you rely too much on AI to automatically generate text and code, you might be under the illusion that you‘ll always only get accurate outputs by AI. Offering its outputs without human’s judgment of accuracy or adequate checks can lead to lose the trust of customers and society. In some cases, you may be held responsible for damage. 2. Fairness Generative AI can produce unfair results, mainly due to biases and prejudices which inherent in the data and algorithms it learns. 2 Points to note When using outputs of Generative AI, make sure there are no biases or prejudices involved. Use equity-conscious AI tools and services. As you can see above, you may face racial and gender biases. Some Generative AI is trying to improve biases, but we still need to be careful about outputs that eliminate diversity. 3. Copyright infringement Generative AI generates new contents by learning other people’s copyrighted materials on the Internet, such as text and images. As a result, its outputs may infringe the copyright of a third party. On considering the relationship between Generative AI and copyrights, it is desirable to separate the "development and learning stages" and "generation and usage stages" because the point to discuss differs in these stages. This document deals with the risks in the "generation and usage stages." In the stages of using Generative AI, outputs may be considered to violate a copyright under Japan’s Copyright Act if it meets the following two requirements. Similarity Reliance The output of is "identical" or "similar" to an existing copyrighted work. The output is generated based on an existing copyrighted work. There is not yet a definite social consensus on whether the results produced by Generative AI can be thought to "rely" on its training data. Given this lack of consensus on the "reliance," it is important to take every precaution to avoid the risk of "similarity.“ * Even if these two requirements are met, the output may be considered not to infringe a copyright in some cases (when the user has obtained permission from the copyright owner, when the term of copyright protection has expired, etc.). Avoid using prompts that involve copyrighted works whose reproduction or modification is prohibited. Make sure that the generated output is not similar to any existing copyrighted works. 4. Information management Some Generative AI systems use the information inputted by users as AI training data or reusing in their outputs. Therefore, if you input confidential information or personal information to Generative AI, that information may be leaked to the provider of the AI system or other users. Do not input confidential information. Internal information Do not input confidential information into a Generative AI system unless you have a non-disclosure agreement with the provider or some other appropriate measures managing secrecy in place regarding prompts and output results. Review the terms of the contract with the customer who has disclosed the information. If the input of the information is not permitted, get consent from the customer. The rule for the input of personal information is also the same as the rule for the input of customer information. In addition, you need to comply with the laws regarding personal information in the country or region concerned. If you have any questions, contact the Legal Division or Information Management Division for advice. Customer Information Personal Information Opt out of being learned these information if possible. Reference: Leakage of confidential information Some Generative AI systems offer an opt-out option that allows users to block input from being used in training data.However, opt-out does not necessarily mean that your input information will not be used for training and be leaked. In addition to opting out, Fujitsu mandates the use of Generative AI services from companies with confidentiality obligations, in accordance with our security policy. When the provider is only required not to use input information for training to used for learning When the provider is required not to use input information for training as well as to keep information secret (not to leak information) 5. Misuse The use of Generative AI may make it easier than before for people with malicious intent to create malware and other tools for stealing personal information, as well as to commit fraud and other criminal acts. Check whether the information is reliable or figure out the intention of the person who has provided that information. nform customers fully about the uses and risks of Generative AI tools. Summary: Using Generative AI in Fujitsu © 2024 Fujitsu Limited The risks of AI (including Generative AI) presented in the overview and individual cases in this course are typical ones. The course does not cover all the potential ethical and legal risks. Responses to risks may change depending on factors such as the purpose of use, the laws and regulations of each country, region, or industry where AI is used, and social conditions. Please make sure to check the contract conditions and terms of use when you use Generative AI. Fujitsu actively promotes the use of Generative AI while taking appropriate risk mitigation measures such as elearning and an ethical review process. Let’s collaborate to realize a society where everyone can enjoy the value brought by AI. For people considering business use of Generative AI ●Fujitsu Kozuchi ●Fujitsu Research Portal Fujitsu AI Ethics anc Governance Generative AI Consulting （with Ridgelinez Ltd.） For a secure and trustworthy AI society Giving users opportunities to quickly test Fujitsu's advanced technologies by providing APIs and web applications of tech components for free. Fujitsu Kozuchi is a set of secure, reliable, cloud-based AI services that enhance the productivity and creativity of your business. https://www.fujitsu.com/jp/about/research/technology/ai/aiethics/index.html https://www.ridgelinez.com/service/generative-ai.php (Japanese) https://www.fujitsu.com/global/services/kozuchi/ https://en-portal.research.global.fujitsu.com/ ●Cloud base chat application that anyone can easily use ●Easily incorporated into existing or developing systems that can improve your business efficiencies drastically ●Equipped with “hallucination(*)” detection technology: (*) produce wrong outcomes while using generative AI ●Continuous enhancement 【Feature】 https://www.fujitsu.com/global/services/kozuchi/ Reference Considerations for AI Code Generator AI code generator is a powerful tool which improves developers’ work efficiency and should be used appropriately. However, there are some points to be considered when using AI code generators due to the characteristics of AI. This chapter summarizes points to keep in mind when using AI Code Generator. Considerations for AI Code Generator Verify the accuracy and functionality of generated codes by human review. Even if the code seems to work well without any error, the generated codes may not be suitable for the intended purpose or may include unexpected and unexplainable logic. Even in the case where the purpose is achieved, the code may not be appropriate from a security point of view. Accuracy Reference Compare with the original specifications. © 2024 Fujitsu Limited Conduct appropriate reviews and tests. Considerations for AI Code Generator When you use AI code generator, the generated code may be similar to the existing third parties’ codes because the AI model which has learned the existing source codes generates the output. In particular, the AI model may insert code that it has been trained on into the Copyright generated code it produces, resulting in a license violation. infringement Reference © 2024 Fujitsu Limited Confirm the contamination of existing codes including OSS. Consider using analysis tools that allow you to identify code contamination in existing OSS. Considerations for AI Code Generator Confirm the configuration or contracts to prevent AI from learning user’s inputs. Information management Reference © 2024 Fujitsu Limited Our inputs, including codes, are typically sent to the AI code generator's server to suggest new code. Please confirm that the codes we input are kept confidential. In particular if personal data or authentication data is included in the code we input, there is significant risk if it will not be kept confidential. Please take an appropriate measure to avoid inputting raw data. In general, it is better not to include such raw data in source codes. Avoid inputting confidential information or replace it with dummy data. Comments containing sensitive information or a key to use an API functionality

23. Panasonic Responsible AI use

RESPONSIBLE AI

The Panasonic Group's Efforts to Ensure Responsible AI-Utilizing Operations

Panasonic Group, one of many corporations operating in this changing world,
has a primary purpose: to be a customer-friendly corporation and a business that always makes customers wellbeing.

The Panasonic Group has a grand vision of its AI technologies making customers wellbeing
and contributing to sustainable social development.
To make this vision a reality, the Group has formulated AI Ethics Principles
to serve as an ethical guideline for the use and application of AI technologies.
We hereby make these principles open to the public.

Panasonic Group AI Ethics Principles

Panasonic Holdings Corporation announced the establishment of its Panasonic Group AI Ethics Principles which must be observed during the development, implementation, and utilization of AI.

The Group aims to realize customers' well-being and social sustainability utilizing AI. In order to provide innovative products and services as solutions in various business fields, including consumer electronics, housing, automobiles, and B2B solutions, the Group has been focusing considerable efforts on the studying, development, and implementation of AI.

In recent years, ethical issues relating to AI have become social issues due to the inappropriate use of AI, which has caused the promotion of discrimination, infringement of privacy, safety concerns, and more. To deal with these problems, the Organization for Economic Corporation and Development (OECD) has adopted international policy guidelines. In Japan, the Cabinet Office has issued Social Principles for Human-Centric AI, and the Ministry of Economy, Trade and Industry has issued Governance Guidelines for the Implementation of AI Principles.

In light of these social trends, the Panasonic Group has established AI Ethical Principles that must be observed throughout the entire Group to facilitate the responsible use of AI in order to offer customers reliable AI products and services.

To implement these AI Ethics Principles throughout the Panasonic Group, an AI Ethics Committee has established to conduct AI ethics risk checks at development sites and promote AI ethics training for all Group employees.

Panasonic Group AI Ethics Principles

The Panasonic Group's main goal is to realize an ideal society with affluence both in mind and matter. Incorporating AI into the Group's major businesses—such as consumer electronics, housing, automobiles, and B2B solutions—allows us to offer customers innovative products and services as solutions to their life issues. This is what we are doing in our daily operations in order to create a better way of life and society. AI, when properly applied, can greatly benefit individuals, society, and the environment. Nevertheless, improper application of AI could have grave consequences and disadvantages. To be a customer-friendly corporation and a business that always puts our customers' well-being first, we have established the following AI Ethics Principles.



1. Creating a better life and society

With an awareness of the effects that AI products and services have on our customers, society, and the environment, we deliver AI products and services to customers only when convinced that they stand to offer them a better life while also improving society and the environment. Even after delivering our AI products and services, we will continue to evaluate the way that they affect our customers' lives, society, and the environment, and put to good use what we discover in our products and services.

2. Design, development, and verification for safety

Our top priority is to provide safe AI products and services. To ensure this, we are developing new technologies and building a store of technical knowledge, which we actively employ in our designs. Even after providing customers with our products and services, we take the necessary actions to ensure their safety.

3. Respecting human rights and fairness

When providing our AI products and services, we respect people's diversity and exercise caution to make sure that no unfairness or discrimination arises from their use. To ensure reliable implementation, we provide all of our Group employees with the necessary training and education.

4. Transparency and accountability

Keeping our customers and clients fully informed about our AI operations is important to us. Our AI development and design processes are carried out based on this principle of transparency. We also respond to all customer inquiries about the effects that AI products and services might have, and are ready to provide the information that customers need.

5. Protecting customers' privacy

We respect customers' privacy by managing customers' information properly and safely in accordance with related laws and Panasonic's internal regulations.

The AI Ethics Principles described herein will be revised in the future to keep pace with the development of AI and social changes.

First version issued on August 29, 2022

Related article : Panasonic Newsoom Global Press Release
Panasonic Group Releases Ethics Principles for AI (August 29, 2022)
https://news.panasonic.com/global/press/en220829-2

Systems and Actions to Promote Responsible AI-Utilizing Operations

Panasonic Group AI Ethics Committee

The Panasonic Group AI Ethics Committee has been set up as the organization responsible for supervising/supporting Operating Companies and making sure that they develop and run AI products (AI equipment and services) and use AI technologies in compliance with the AI Ethics Principles. Discussions on establishing the Committee started in 2019, and it was inaugurated in April 2022, the month in which Panasonic Corporation was reborn as Panasonic Holdings Corporation.

The Panasonic Group AI Ethics Committee (hereinafter “AI Ethics Committee”) has been set up as an organization within Panasonic Holdings Corporation, and is made up of personnel in charge of AI ethics and staff who have expertise in Legal Affairs, Intellectual Property, Quality Control, etc. brought in from each Operating Company.



Roles of the AI Ethics Committee

- Set, implement, and promote AI Ethics Principles.

- Promote assessment of risks related to AI ethics, and provide support tools and systems for assessments.

- Carry out training of Group employees in AI ethics.

Examining risks related to AI ethics at AI development sites

The Panasonic Group has a wide range of business fields in which AI technologies are utilized. To make efficient assessments of the risks related to AI ethics that arise in these business fields, risk checks are made by staff at AI product development sites and by the AI Ethics Committee at various stages of the process. These shared responsibilities and roles accelerate innovative AI-utilized business operations.。

The AI Ethics Committee has developed an AI ethics check system for efficiently checking risks related to AI ethics and provides AI development sites with the system. Currently, our AI ethics check system is under trial as a pilot project within the Group. It is scheduled to be progressively introduced to every AI development project from FY2022.



AI ethics education for all Group employees

All Group employees are receiving education in AI ethics. Ordinary employees are given basic education every year, while employees working at AI development sites or engaging in AI ethics programs are given technical education when needed. AI ethics seminars are also held regularly, at which invited lecturers discuss AI ethics issues.



24. Sony Responsible AI Usage

Sony Group's Initiatives for Responsible AI

Sony, with the aim of utilizing AI technology to enrich people's life styles and contribute to the development of society, will pursue accountability and transparency while actively engaging in dialogue with stakeholders. We will continue to promote responsible AI in order to maintain the trust of products and services by stakeholders.

Sony Group AI Ethics Guidelines

Sony established the Sony Group AI Ethics Guidelines in September 2018.

Through the utilization of artificial intelligence (AI), Sony aims to contribute to the development of a peaceful and sustainable society while delivering kando - a sense of excitement, wonder or emotion - to the world. Starting from the electronics business, Sony has continued to expand its business area and has become a diverse global company that offers entertainment such as music and movies, as well as financial services. To operate these business areas based on Sony's Purpose to "Fill the world with emotion, through the power of creativity and technology.", Sony Group AI Ethics Guidelines are hereby set forth below to ensure and promote a dialogue with various stakeholders and the proper utilization and research and development ("R&D") of AI within Sony Group.

1. Supporting Creative Life Styles and Building a Better Society

Through advancing its AI-related R&D and promoting the utilization of AI in a manner harmonized with society, Sony aims to support the exploration of the potential for each individual to empower their lives, and to contribute to enrichment of our culture and push our civilization forward by providing novel and creative types of kando. Sony will engage in sustainable social development and endeavor to utilize the power of AI for contributing to global problem-solving and for the development of a peaceful and sustainable society.

2. Stakeholder Engagement

In order to solve the challenges arising from use of AI while striving for better AI utilization, Sony will seriously consider the interests and concerns of various stakeholders including its customers and creators, and proactively advance a dialogue with related industries, organizations, academic communities and more. For this purpose, Sony will construct the appropriate channels for ensuring that the content and results of these discussions are provided to officers and employees, including researchers and developers, who are involved in the corresponding businesses, as well as for ensuring further engagement with its various stakeholders.

3. Provision of Trusted Products and Services

Sony understands the need for safety when dealing with products and services utilizing AI and will continue to respond to security risks such as unauthorized access. AI systems may utilize statistical or probabilistic methods to achieve results. In the interest of Sony's customers and to maintain their trust, Sony will design whole systems with an awareness of the responsibility associated with the characteristics of such methods.

4. Privacy Protection

Sony, in compliance with laws and regulations as well as applicable internal rules and policies, seeks to enhance the security and protection of customers' personal data acquired via products and services utilizing AI, and build an environment where said personal data is processed in ways that respect the intention and trust of customers.

5. Respect for Fairness

In its utilization of AI, Sony will respect diversity and human rights of its customers and other stakeholders without any discrimination while striving to contribute to the resolution of social problems through its activities in its own and related industries.

6. Pursuit of Transparency

During the planning and design stages for its products and services that utilize AI, Sony will strive to introduce methods of capturing the reasoning behind the decisions made by AI utilized in said products and services. Additionally, it will endeavor to provide intelligible explanations and information to customers about the possible impact of using these products and services.

7. The Evolution of AI and Ongoing Education

People's lives have continuously changed with the advance in technology across history. Sony will be cognizant of the effects and impact of products and services that utilize AI on society and will proactively work to contribute to developing AI to create a better society and foster human talent capable of shaping our collective bright future through R&D and/or utilization of AI.

AI Ethics: Promotion & Projects

Framework for AI Ethics Initiatives

The Sony Group AI Ethics Committee, established in 2019, examines AI use cases to ensure compliance with the Sony Group AI Ethics Guidelines.
In 2021, the AI Ethics Office was established to provide subject matter expertise on AI ethics to all Sony business units. In addition, Sony has established a notification system for AI utilization in products, services, and internal operations in Sony Group's business units, to share information on AI ethics risks.
In March 2021, in accordance with the Sony Group AI Ethics Guidelines, Sony established an internal document stipulating requirements to be complied with in the commercialization process of electronic products and services, and in July 2021 started conducting AI ethics assessments in the product development life cycle. Sony uses e-learning tools to promote an understanding of AI ethics among its employees and invites speakers from outside the company to discuss this issue at lectures and symposia.

Stakeholder Dialogue and External Collaboration

Sony actively pursues dialogue with relevant companies, organizations, and the academic community on ethical issues surrounding AI utilization, while considering the interests of diverse stakeholders, including customers and creators.
In May 2017, Sony became the first Japanese company to join the Partnership on AI to Benefit People and Society (PAI), a non-profit organization created to contribute to solutions for some of humanity's challenging problems, including advancing the understanding of AI and addressing ethics surrounding AI technology. One of the most common issues in AI ethics is that of fairness, transparency, and accountability, abbreviated as "FTA." Sony utilizes knowledge it has gained from its AI and robotics related research, development, and business ventures and contributes to a number of working groups addressing this issue. Sony chaired the Social and Societal Influences of AI Working Group, which focuses on the social impacts of AI, and currently serves as an expert advisor for PAI's strategic planning. Sony also serves on the steering committee for ABOUT ML,* an initiative to improve the transparency of machine learning. Sony also serves as an expert advisor to the Explainability Research Project and Diversity and Inclusion Research Project.

* ABOUT ML stands for "Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles."

Sony's Global Head of AI Ethics is also one of the General Chairs of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT), the premier conference on sociotechnical algorithmic systems. Sony is also involved with Japanese initiatives to establish principles and guidelines that promote the utilization of AI for social good. These initiatives include the AI Utilization Strategy published by Keidanren (Japan Business Federation) in February 2019 and the Social Principles of Human-centric AI published by Japan's Cabinet Office in March 2019. Sony is currently a member of the Conference toward AI Network Society, a group within the Ministry of Internal Affairs and Communications whose goal is the comprehensive study of the social, economic, ethical, and legal factors involved in the promotion of AI networks throughout society as a whole. Additionally, Sony is a participant in the Global Partnership on AI, an initiative launched in June 2020 to promote the development and utilization of AI based on human-centric principles, and serves as a member of the AI and Pandemic Response Subgroup, a working group that aids the development of responsible AI solutions for epidemics of infectious disease such as COVID-19. Trusted R&D for AI

Sony pursues R&D for AI that is trusted and backed by solid technologies, and is engaged in technical initiatives related to AI ethics. As a solution for securing FTA, Sony equipped its AI development tool Neural Network Console with eXplainable AI (XAI) to make it easy to use. XAI is a technology that enables people to understand the logic behind AI decision-making, an area often called the "black box" since it is not always immediately apparent. Sony has also released its machine learning fairness library and Responsible AI XAI source code as open source software.

Additionally, Sony provided its Prediction One predictive analysis tool with the ability to visualize the predictive reasoning. In 2021, Sony also launched the AI ethics flagship within Sony AI with projects to conduct cutting-edge research on the challenges faced in the development of AI products and services, including ethical data collection and algorithmic fairness. Taking advantage of its position as a company that extends across a wide range of industries, Sony will put fair and transparent AI into practice, leveraging its global and diverse perspective. Related Sony Group Initiatives

Integrity and Sincerity are the key Values to achieving Sony's Purpose. The Sony Group Code of Conduct sets forth a standard of daily behavior based on these Values. The Code of Conduct supports Sony's value creation and serves as the basis for earning the trust for the Sony brand through the ethical and responsible conduct of every Sony Group employee.

Sony Group Code of Conduct

Sony's policy on respect for human rights is stipulated in the Sony Group Code of Conduct. All Group companies are required to respect human rights and conduct sincere business operations in accordance with this policy and any related laws and regulations. In regards to ethical issues related to the use of AI, we are responding sincerely from the perspective of fairness and transparency while engaging in dialogue with stakeholders.

Sony's Approach to Respect for Human Rights

Sony, with regard to its philosophy and basic policy on product quality and customer service, aims to improve quality and customer service from the customer's point of view in a way that can provide customers with a sense of "satisfaction", "trust", and "safety".

Commitment to Quality and Customer Service

In order to protect the privacy and personal information of customers, employees and other stakeholders, Sony carries out continuous activities under a system of governance over the entire Group in such a way that allows us to effectively manage potential risks and incorporate measures to protect privacy into our systems and products, and we are working hard on the utilization of AI as part of these activities.

Protection of Privacy and Personal Information



25. ASMPT 2023 Interim Report

CHAIRMAN’S STATEMENT RESULTS SUMMARY ASMPT Limited and its subsidiaries (the “Group” or “ASMPT”) delivered revenue of HK$7.82 billion (US$997.5 million) for the six months ended 30 June 2023, a decline of 25.3% YoY and 12.1% HoH. The Group’s consolidated profit after taxation for the first half of 2023 was HK$623.1 million, a decline of 64.1% YoY and 29.5% HoH. Basic earnings per share for the six months ended 30 June 2023 was HK$1.52, a decline of 63.9% YoY and 29.3% HoH. DIVIDEND The Board of Directors of ASMPT Limited (the “Company”) has declared an interim dividend of HK$0.61 (2022: HK$1.30) per share, payable to shareholders whose names appear on the Register of Members of the Company on 18 August 2023. MANAGEMENT DISCUSSION AND ANALYSIS The performance review for the first six months of 2023 will begin with notable business highlights, followed by a financial review of the Group and its Segments: the Semiconductor Solutions Segment (“SEMI”) and SMT Solutions Segment (“SMT”). 1H 2023 Group Business Highlights For the first half of 2023, the Group continued to be impacted by weak industry conditions, marked by conservative consumer spending and capex investment, and on-going inventory digestion. SMT continued its resilient performance and its revenue exceeded that of SEMI for the fourth consecutive quarter. Recovery for SEMI is taking longer than anticipated as factory utilisation of its customer base, while showing gradual improvement, has not yet reached optimum levels. Amidst a prolonged semiconductor downturn, the Group’s unique and broad portfolio, coupled with its role as a key partner in the technology roadmaps of major customers, provides strong foundations for future growth. In particular, the Group stands to benefit from Advanced Packaging (“AP”), driven by strong growth in generative AI and High Performance Computing (“HPC”), as well as Automotive. AP: Positioned Well for Generative AI and HPC Growth The Group’s AP solutions delivered 1H 2023 revenue of about US$195 million, or 19% of Group revenue, with its Thermo Compression Bonding (“TCB”) solutions contributing the most to this AP revenue. MANAGEMENT DISCUSSION AND ANALYSIS (Continued) 1H 2023 Group Business Highlights (Continued) AP: Positioned Well for Generative AI and HPC Growth (Continued) The Group is in deep collaborations with key customers to enable the strong growth in generative AI, which requires high precision bonding solutions and optimal total cost of ownership: TCB: The Group is in a commanding position to address crucial logic and memory packaging bottlenecks in generative AI. For logic, the Group is enabling major customers in Chip-to-Wafer and Chip-to-Substrate processes that are critical for the heterogeneous integration and assembly of AI computing packages in increasingly sophisticated configurations. For high-bandwidth memory (“HBM”), the Group’s solutions fulfil demanding packaging requirements for next-generation HBM. As generative AI proliferates, customers will migrate to these advanced HBM packages to meet increased storage and processing demands. The Group’s TCB solutions are capable of handling a variety of interconnect types. It is uniquely positioned to benefit from the accelerated adoption of TCB due to the increasing number of interconnects required by complex AI packages. Order flow for logic is promising from foundry and OSAT customers. In particular, demand from its foundry customer base is growing due to expansion for AP capacity. For memory, the Group won repeat orders for HBM and it continues to engage deeply with multiple memory players. The Group is therefore confident of more TCB order flow for both logic and memory in 2H 2023. Other AP: The Group’s Mass Reflow High Precision Die-bonding solutions are benefitting from generative AI, with continuous order flow from top tier global customers. In Hybrid Bonding, the Group continues its engagement with key customers for qualification in various end-market applications, including memory. Generative AI’s increasing demand also benefits other tools in the Group’s portfolio: Silicon Photonics (“SiPh”): The high placement accuracy of the Group’s market leading solutions are relevant for SiPh and Co-Packaged Optic devices such as optical transceivers and photonic engines. The Group received repeat orders for its tools to support a key customer’s transceiver expansion plans to meet generative AI’s high bandwidth transfer requirements and it expects more such orders in 2H 2023. SMT Placement Tools: There is traction in the server business driven by AI applications for the Group’s SMT placement tools that provide flexibility for customers in terms of handling larger server board weights and sizes. The complex chip architectures required by generative AI and HPC packages are also benefitting the Group’s Laser Singulation solutions, which have seen preliminary engagements with global IDMs, as well as its market-leading Panel Electrochemical Deposition Tools that serve HPC applications requiring more advanced substrates with larger form factors and finer line/space. Automotive Contributing Strongly, but Normalising For 1H 2023, the Group’s Automotive end market applications contributed the highest proportion of Group revenue at approximately US$230 million or 23% of 1H 2023 Group revenue. This contribution spanned across the Group’s mainstream solutions, particularly SMT placement tools, molding tools and die bonders. While the Group’s Automotive solutions have contributed strongly to its overall performance in the last two years, this sector has begun to normalise. Nevertheless, demand from EV players and for Silicon Carbide applications remain robust. The Group’s Automotive solutions continue to bag design wins that will eventually translate into High Volume Manufacturing demand. 1H 2023 Group Financial Review Group Revenue declined YoY and HoH, due mostly to SEMI, while SMT revenue held steady. Group revenue was influenced by the following: (i) The Automotive market had the highest contribution to Group revenue at approximately 23%, mostly from mainstream solutions across both SMT and SEMI; (ii) The Industrial market remained robust and contributed about 18% to Group revenue, mostly from SMT; and (iii) The Consumer, Communication and Computers markets combined continued to be weak due to market sentiment. Geographically, contribution from China (including Hong Kong) declined YoY and its share of Group revenue was reduced from 44% to 30%, while revenue from Europe and the Americas grew from 15% to 30% and 9% to 19%, respectively. Customer concentration risk continued to remain low for the Group with its top five customers accounting for approximately 20% of 1H 2023 revenue. Group bookings decreased slightly by 2.7% HoH due to SMT, while SEMI grew from a low base. Bookings declined 43.9% YoY, mainly due to a high base effect. Contributions from AP, Automotive and Industrial end markets accounted for approximately 57% of Group bookings in 1H 2023. The Group ended 1H 2023 with a backlog of HK$7.78 billion (US$992.6 million) and a book-to-bill ratio of 0.84. Group gross margin declined slightly, partly due to segment mix as SMT contributed about 59% to Group revenue in 1H 2023. The Group’s operating margin and net profit YoY and HoH were adversely impacted by lower sales volume. As of 30 June 2023, the Group maintained a healthy liquidity position with gross cash and bank deposits totalling HK$3.77 billion while bank borrowings were at HK$2 billion. MANAGEMENT DISCUSSION AND ANALYSIS (Continued) Q2 2023 Group Financial Review Group Q2 revenue was flat QoQ, while bookings declined due to the ongoing industry downcycle. Both revenue and bookings declined YoY mainly due to a high base effect. Group gross margin declined, mainly due to SEMI, but was partially offset by margin improvement from SMT. The Group’s overall profitability declined marginally QoQ, while YoY declines were due to lower sales volume. SEMI contributed about 42% to Group Q2 2023 revenue. Revenue increased QoQ from a low base. Revenue for the segment was mainly impacted by the following developments in its Business Units (“BU”): (i) The IC/Discrete BU had stable QoQ revenue, with the highest revenue contribution from TCB. There was also some uptick in the contribution from the BU’s mainstream tools. (ii) The Optoelectronics BU recorded higher revenue QoQ. Revenue growth was mainly driven by wire bonders for Conventional Displays. The BU’s high-end Silicon Photonics applications also registered growth. (iii) The CIS BU’s revenue remained relatively low due to continued weakness in the global smartphone market. SEMI bookings declined QoQ and YoY due to the ongoing semiconductor downcycle, while the declines in its gross margin QoQ and YoY were due to a higher mix of wire bonders and volume effects, respectively. SMT’s revenue performance remained relatively stable, contributing about 58% of Group Q2 2023 revenue. Industrial and Automotive applications combined made up almost half of SMT revenue with demand mostly from Europe. SMT has enjoyed a high level of bookings for more than two years and entered a normalisation phase. SMT bookings were driven mostly by Industrial and Automotive end-markets. Segment gross margin increased QoQ and YoY mainly due to a favourable product mix. OUTLOOK The Group’s growth prospects remain strong due to long term structural trends of automotive electrification, smart factories, green infrastructure, 5G, IoT, and HPC fuelled by generative AI growth. The Group has a key competitive advantage as a major supplier and technology partner across many applications and solution areas. Near term visibility continues to be limited due to uncertainty in the macroeconomic environment, marked by persistent inflation, tepid consumer sentiment and ongoing inventory digestion. With these considerations in mind, the Group expects revenue for Q3 2023 to be between US$410 million to US$480 million (–23.4% YoY and –10.5% QoQ at midpoint), mainly due to SMT normalising. RESEARCH AND DEVELOPMENT The Group’s continued commitment to investing in research and development (“R&D”) across the industry cycles is critical to its objective of remaining at the forefront of technology development. This emphasis on R&D positions the Group well to capitalize on technological breakthroughs required for tapping secular growth opportunities. A good example is the Group’s investment in progressively evolving TCB solutions, putting it in a commanding position to address current industry bottlenecks and capture growth from generative AI and HPC demand. With a global workforce of more than 2,500, the Group operates several R&D centres worldwide: the Americas (Boston), Asia (Chengdu, Hong Kong, Huizhou, Sanhe, Shenzhen, Suzhou, Singapore and Taoyuan) and Europe (Beuningen, Munich, Porto, Regensburg and Weymouth). For 1H 2023, the Group invested around HK$1 billion in R&D. To date, the Group has delivered about 2,000 patents and patent applications. CHAIRMAN’S STATEMENT (CONTINUED) LIQUIDITY AND FINANCIAL RESOURCES Cash and bank deposits as of 30 June 2023 were HK$3.77 billion (31 December 2022: HK$4.42 billion). Capital additions during the period amounted to HK$227.8 million (1H 2022: HK$212.9 million), which was fully funded by the period’s depreciation and amortization of HK$244.6 million (1H 2022: HK$249.1 million), excluding the depreciation of right-of-use assets of HK$110.0 million (1H 2022: HK$114.9 million) due to HKFRS 16 application. As of 30 June 2023, the debt-to-equity ratio was 0.127 (31 December 2022: 0.142). The Group had available banking facilities of HK$3.42 billion (US$435.8 million) (31 December 2022: HK$3.40 billion (US$435.6 million)) in the form of bank loans and overdraft facilities, of which HK$1.87 billion (US$238.3 million) (31 December 2022: HK$1.86 billion (US$238.5 million)) were committed borrowing facilities. Bank borrowings, which are mainly to support day-to-day operations and capital expenditure, are denominated in Hong Kong dollars. The Group had unsecured bank borrowings of HK$2.00 billion as of 30 June 2023 (31 December 2022: HK$2.25 billion), consisting of variable-rate syndicated loan. These bank borrowings are repayable by instalments. The syndicated loan is repayable by instalments from March 2022 to March 2024. The Group uses interest rate swaps to mitigate its exposure of the cash flow changes of the variable-rate syndicated loan by swapping HK$1.75 billion (31 December 2022: HK$1.75 billion) of the syndicated loan from variable rates to fixed rates. The Group’s equity attributable to owners of the Company was HK$15.60 billion as of 30 June 2023 (31 December 2022: HK$15.74 billion). As of 30 June 2023, cash holdings of the Group were mainly in US dollars, Euros and Chinese RMB. The Group entered into HK dollar and Euro hedging contracts to mitigate foreign currency exposure of the inter-company loans denominated in Euro. SMT entered into US dollar and Euro hedging contracts to mitigate foreign currency risks, as a significant portion of the production of SMT equipment and its suppliers are located in Europe, while a substantial part of the Group’s revenue for SMT equipment is denominated in US dollars. In terms of currency exposure, the majority of the Group’s sales and disbursements in respect of operating expenses and purchases were mainly in US dollars, Euros and Chinese RMB. SIGNIFICANT INVESTMENT As at 30 June 2023, Advanced Assembly Materials International Limited (“AAMI”) was regarded as a significant investment of the Group as the value of the Group’s investment in AAMI comprised 5% or more of the Group’s total assets. Save as disclosed in this announcement, the information pursuant to paragraph 32(4A) of Appendix 16 to the Rules Governing the Listing of Securities on The Stock Exchange of Hong Kong Limited (“Listing Rules”) in relation to the Group’s investment in AAMI has not changed materially from the information disclosed in the most recent published annual report. CHAIRMAN’S STATEMENT (CONTINUED) HUMAN RESOURCES The Group deeply appreciates the contributions of its employees worldwide for their unwavering commitment during an industry downturn. Several cost optimisation measures were put in place to mitigate the effects of the downturn and they were well supported by the Group’s employees, exhibiting a true team spirit. At the same time, the Group’s HR efforts continue to focus on introducing and enhancing initiatives that create an inclusive and positive work culture and employee experience. As part of these measures, the Group recently successfully launched the ‘Global People System’, a global HR system aimed at improving the entire employee digital experience, which encompasses transactions, engagement, growth and development. This HR system has already been implemented for Asia and the Americas and will soon be rolled out for Europe. Other recent HR initiatives include the kick-off for the establishment of global job grades and career structures and a Leadership Development Centre, which will collectively strengthen the Group’s leadership pipeline and succession efforts. As of 30 June 2023, total headcount for the Group was approximately 11,300, which excludes some 700 flexi workers and outsourced workers. Of this 11,300, approximately 1,000 are based in Hong Kong, 5,100 in mainland China, 1,100 in Singapore, 1,100 in Germany, 900 in Malaysia, 400 in Portugal, 400 in the United Kingdom, 400 in the United States, and the rest in other parts of the world. Total manpower costs for the Group for 1H 2023 was HK$2.49 billion versus HK$2.68 billion over the same period in 2022. The Group continues its commitment to fairly remunerate its employees while manoeuvring through the prolonged semiconductor downcycle with a prudent and measured approach towards managing overall manpower costs. REPORT ON REVIEW OF CONDENSED CONSOLIDATED FINANCIAL STATEMENTS TO THE BOARD OF DIRECTORS OF ASMPT LIMITED (incorporated in the Cayman Islands with limited liability) INTRODUCTION We have reviewed the condensed consolidated financial statements of ASMPT Limited (the “Company”) and its subsidiaries (collectively referred to as the “Group”) set out on pages 13 to 42, which comprise the condensed consolidated statement of financial position as of 30 June 2023 and the related condensed consolidated statement of profit or loss, statement of profit or loss and other comprehensive income, statement of changes in equity and statement of cash flows for the six months then ended, and certain explanatory notes. The Rules Governing the Listing of Securities on The Stock Exchange of Hong Kong Limited require the preparation of a report on interim financial information to be in compliance with the relevant provisions thereof and the Hong Kong Accounting Standard 34 “Interim Financial Reporting” (“HKAS 34”) issued by the Hong Kong Institute of Certified Public Accountants. The directors of the Company are responsible for the preparation and presentation of these condensed consolidated financial statements in accordance with HKAS 34. Our responsibility is to express a conclusion on these condensed consolidated financial statements based on our review, and to report our conclusion solely to you, as a body, in accordance with our agreed terms of engagement, and for no other purpose. We do not assume responsibility towards or accept liability to any other person for the contents of this report. SCOPE OF REVIEW We conducted our review in accordance with Hong Kong Standard on Review Engagements 2410 “Review of Interim Financial Information Performed by the Independent Auditor of the Entity” issued by the Hong Kong Institute of Certified Public Accountants. A review of these condensed consolidated financial statements consists of making inquiries, primarily of persons responsible for financial and accounting matters, and applying analytical and other review procedures. A review is substantially less in scope than an audit conducted in accordance with Hong Kong Standards on Auditing and consequently does not enable us to obtain assurance that we would become aware of all significant matters that might be identified in an audit. Accordingly, we do not express an audit opinion. CONCLUSION Based on our review, nothing has come to our attention that causes us to believe that the condensed consolidated financial statements are not prepared, in all material respects, in accordance with HKAS 34. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS For the six months ended 30 June 2023 1. BASIS OF PREPARATION The condensed consolidated financial statements have been prepared in accordance with the applicable disclosure requirements of Appendix 16 to the Rules Governing the Listing of Securities on The Stock Exchange of Hong Kong Limited (the “Stock Exchange”) and with the Hong Kong Accounting Standard 34 “Interim Financial Reporting” issued by the Hong Kong Institute of Certified Public Accountants (the “HKICPA”). The condensed consolidated financial statements do not include all the information required for a complete set of Hong Kong Financial Reporting Standards financial statements and should be read in conjunction with the Group’s annual consolidated financial statements as at 31 December 2022. 2. PRINCIPAL ACCOUNTING POLICIES The condensed consolidated financial statements have been prepared on the historical cost basis except for the derivative financial instruments, other investments, other financial assets and certain financial liabilities which are measured at fair value at the end of reporting period. Other than changes in accounting policies resulting from application of amendments to Hong Kong Financial Reporting Standards (“HKFRSs”), the accounting policies and methods of computation used in the condensed consolidated financial statements for the six months ended 30 June 2023 are consistent with those followed in the preparation of the Group’s annual consolidated financial statements for the year ended 31 December 2022. Application of amendments to HKFRSs In the current interim period, the Group has applied the following amendments to HKFRSs issued by the HKICPA, for the first time, which are mandatorily effective for the Group’s annual period beginning on 1 January 2023 for the preparation of the Group’s condensed consolidated financial statements: HKFRS 17 (including the October 2020 and February 2022 Amendments to HKFRS 17) Insurance Contracts Amendments to HKAS 1 and HKFRS Practice Statement 2 Disclosure of Accounting Policies Amendments to HKAS 8 Definition of Accounting Estimates Amendments to HKAS 12 Deferred Tax related to Assets and Liabilities arising from a Single Transaction Except as described below, the application of the amendments to HKFRSs in the current interim period has had no material impact on the Group’s financial positions and performance for the current and prior periods and/or on the disclosures set out in these condensed consolidated financial statements. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) For the six months ended 30 June 2023 2. PRINCIPAL ACCOUNTING POLICIES (Continued) Application of amendments to HKFRSs (Continued) Impacts and changes in accounting policies on application of Amendments to HKAS 12 Deferred Tax related to Assets and Liabilities arising from a Single Transaction Accounting policies Deferred tax is recognised on temporary differences between the carrying amounts of assets and liabilities in the condensed consolidated financial statements and the corresponding tax bases used in the computation of taxable profit. Deferred tax liabilities are generally recognised for all taxable temporary differences. Deferred tax assets are generally recognised for all deductible temporary differences to the extent that it is probable that taxable profits will be available against which those deductible temporary differences can be utilised. Such deferred tax assets and liabilities are not recognised if the temporary difference arises from the initial recognition (other than in a business combination) of assets and liabilities in a transaction that affects neither the taxable profit nor the accounting profit and at the time of the transaction does not give rise to equal taxable and deductible temporary differences. In addition, deferred tax liabilities are not recognised if the temporary difference arises from the initial recognition of goodwill. Transition and summary of effects As disclosed in the Group’s annual financial statements for the year ended 31 December 2022, the Group previously applied the HKAS 12 Income Taxes requirements to assets and liabilities arising from a single transaction as a whole and temporary differences relating to the relevant assets and liabilities were assessed on a net basis. Upon the application of the amendments, the Group assessed the relevant assets and liabilities separately. The Group also recognised a deferred tax asset (to the extent that it is probable that taxable profit will be available against which the deductible temporary difference can be utilised) and a deferred tax liability for all deductible and taxable temporary difference associated with right-of-use assets and lease liabilities. The application of the amendments has had no material impact on the Group’s financial position and performance. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) For the six months ended 30 June 2023 3. SEGMENT INFORMATION The Group has two (2022: two) operating segments: development, production and sales of (1) semiconductor solutions and (2) surface mount technology solutions. They represent two (2022: two) major types of products manufactured by the Group. The operating segments are identified on the basis of internal reports about components of the Group that are regularly reviewed by the Company’s Chief Executive Officer, the chief operating decision maker (“CODM”), for the purpose of allocating resources to segments and assessing their performance. The Group is organised and managed around the two (2022: two) major types of products manufactured by the Group. No operating segments have been aggregated in arriving at reportable segments of the Group. Segment results represent the profit before taxation earned by each segment without allocation of interest income, finance costs, unallocated other income and other gain, unallocated net foreign exchange gain and fair value change of foreign currency forward contracts, unallocated general and administrative expenses, share of result of a joint venture and other expenses. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) For the six months ended 30 June 2023 3. SEGMENT INFORMATION (Continued) Segment revenue and results (Continued) No analysis of the Group’s assets and liabilities by operating segment is disclosed as they are not regularly provided to the CODM for review. All of the segment revenue derived by the segments is from external customers. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) For the six months ended 30 June 2023 7. INCOME TAX EXPENSE (Continued) Current tax: (Continued) (c) ASMPT Singapore Pte. Ltd. (“ATS”) has been granted a Pioneer Certificate (“PC”) to the effect that profits arising from the manufacture of certain semiconductor products are exempted from tax for a period of 10 years effective from 1 January 2022 to 31 December 2031 across specified products, subject to fulfillment of certain criteria during the relevant periods. ATS has also been granted a Development and Expansion Incentive (“DEI”) to the effect that certain income arising from qualifying activities conducted by ATS, are subject to a concessionary tax rate for a period of 10 years from 1 January 2021 to 31 December 2030, subject to fulfillment of certain criteria during the relevant period. Income of ATS arising from activities not covered under the PC or DEI are taxed at the prevailing corporate tax rate in Singapore of 17% (for the six months ended 30 June 2022: 17%). (d) The calculation of current tax of the Group’s subsidiaries in Germany is based on a corporate income tax rate of 15.00% (for the six months ended 30 June 2022: 15.00%) plus 5.50% (for the six months ended 30 June 2022: 5.50%) solidarity surcharge on the corporate income tax for the assessable profit for the period which derives at tax rate of 15.825% (for the six months ended 30 June 2022: 15.825%). In addition to corporate income tax, trade tax is levied on taxable income. The applicable German trade tax (local income tax) rates for the Group’s subsidiaries in Germany vary from 11.187% to 17.150% (for the six months ended 30 June 2022: 12.465% to 17.150%) according to the municipal in which the entity resides. Thus the aggregate tax rates were between 27.012% and 32.975% (for the six months ended 30 June 2022: between 28.290% and 32.975%). (e) Taxation for other jurisdictions is calculated at the rates prevailing in the relevant jurisdictions. The deferred tax credit is mainly related to the tax effect of temporary difference between the tax base of certain assets and liabilities and the carrying value of the assets and liabilities. The balance mainly includes deductible temporary differences arising from retirement benefit obligations, provisions, inventories, trade receivables, right-of-use assets and lease liabilities. 13. PROVISIONS (Continued) Based on the subsidiary’s consultant’s report and the directors’ estimate of the expenditure required to settle the Group’s obligations in relation to the litigation, a provision of approximately HK$39,239,000 (31 December 2022: HK$38,985,000) was made. The remaining is mainly provision for restoration of right-of-use assets. 14. BANK BORROWINGS At 30 June 2023, the bank borrowings bear interest at HIBOR plus a margin per annum (for the six months ended 30 June 2022: fixed-rate interest or interest at LIBOR or HIBOR plus a margin per annum), at an effective interest rate 2.84% (31 December 2022: 3.11%) per annum. During the six months ended 30 June 2023, the Group repaid bank borrowings of HK$250,000,000 (for six months ended 30 June 2022: HK$289,662,000). During the six months ended 30 June 2022, the Group obtained new bank borrowings amounting to HK$639,238,000 (for six months ended 30 June 2023: nil). Note: Included in variable-rate borrowings were bank loans of HK$1,750,000,000 (31 December 2022: HK$1,750,000,000) which were under cash flow hedges. The interest rates for the borrowings are fixed to 2.315% and 2.38% under the interest rate swap contracts with the maturity date on 21 March 2024. NOTES TO THE CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) For the six months ended 30 June 2023 16. SHARE-BASED PAYMENTS At the annual general meeting of the Company held on 7 May 2019, the shareholders approved the adoption of an Employee Share Incentive Scheme (the “Scheme”) commencing on 24 March 2020 (the “Adoption Date”), under which shares of the Company (the “Awarded Shares”) may be allocated or awarded to employees or directors of the Company and certain subsidiaries as determined by the Board (the “Selected Employees”). Unless otherwise cancelled or amended, the Scheme will remain valid and effective for a period of ten years from the Adoption Date. Details of the Scheme were set out in the Company’s circular to shareholders dated 1 April 2019. During the year ended 31 December 2022, the directors resolved to contribute HK$240 million to the Scheme, pursuant to which an independent professional trustee appointed by the Board under the Scheme (“Trustee”) to subscribe or purchase 3,148,600 shares in the Company for the benefits of certain employees and members of the management of the Group who shall remain in employment within the Group upon the expiration of vesting period on 15 December 2022 (the “2022 Vesting Date”). The Trustee (i) purchased a total of 429,700 shares in the Company on the Stock Exchange, and (ii) subscribed 2,633,700 shares in the Company, prior to the 2022 Vesting Date. On the 2022 Vesting Date, the Trustee transferred 438,400 shares purchased on the Stock Exchange (included 8,700 shares purchased during the year ended 31 December 2021) and 2,633,700 subscribed shares to certain Selected Employees who are connected persons and not connected persons of the Company respectively. During the year ended 31 December 2022, 76,500 share entitlements were forfeited and unallocated by the Company. During the period ended 30 June 2023, the directors resolved to contribute HK$181 million to the Scheme, and a total of 2,447,000 shares in the Company are expected to be vested in Selected Employees upon the expiration of the next vesting period on 15 December 2023. The Trustee has purchased a total of 361,500 shares in the Company on the Stock Exchange. The fair values of the shares awarded pursuant to the Scheme in 2022 and 2023 were determined with reference to the market value of the shares at the award date taking into account the exclusion of the expected dividends as the employees were not entitled to receive dividends paid prior to the vesting of the shares. The Group recognised share-based payments for the six months ended 30 June 2023 amounting to HK$65,283,000 (for the six months ended 30 June 2022: HK$85,630,000) in relation to the shares awarded pursuant to the Scheme by the Company, such amount being determined by the fair value of the shares awarded at the award dates. 17. RELATED PARTY TRANSACTIONS Compensation of key management personnel During the period, the emoluments of directors and other members of key management were HK$31,865,000 (for the six months ended 30 June 2022: HK$31,350,000). Certain shares of the Company were awarded to the members of key management under the Scheme (see note 16 for details of the Scheme). The estimated fair value of such shares included in the emoluments above amounted to HK$8,607,000 (for the six months ended 30 June 2022: HK$12,462,000) for the six months ended 30 June 2023. Service income and sales to a joint venture and its affiliates During the period, there are sales of spare parts to a joint venture and its affiliates of HK$1,706,000 (for the six months ended 30 June 2022: HK$9,893,000) and rental services of HK$5,589,000 (for the six months ended 30 June 2022: HK$6,468,000). 18. ACQUISITION OF SUBSIDIARIES On 30 September 2021, the Group entered into a purchase agreement to acquire 100% equity interest in Automation Engineering, Inc. (“AEi”), a company based in Tewksbury, Massachusetts, USA, at a purchase price of US$23,107,000 (equivalent to approximately HK$180,188,000) (“AEi Acquisition”). AEi engages in the automotive camera active alignment market. The AEi Acquisition was completed on 1 February 2022 and has been accounted for using the acquisition method. For details regarding the AEi Acquisition, please refer to 2022 Annual Report. Acquisition of Beijing Borey Advanced Technology Co., Ltd. (“Borey”) On 13 April 2023, the Group entered into a share purchase agreement to acquire 100% equity interest in Borey, a company based in PRC, at a purchase price of RMB27,000,000 (equivalent to approximately HK$30,842,000), subject to certain adjustments as set out in the share purchase agreement (“Borey Acquisition”). Borey engages in the surface mount technology electronic assembly equipment. The Borey Acquisition was completed on 28 April 2023 and has been accounted for using the acquisition method. Acquisition-related costs have been excluded from the cost of acquisition and recognised as an expense in the period when incurred within the “general and administrative expenses” line item in the condensed consolidated statement of profit or loss. Cumulative acquisition-related costs in respect of the Borey Acquisition amounted to HK$2,091,000, of which HK$1,635,000 was charged to profit or loss in the current period with the remaining amount charged to profit and loss in prior year. 18. ACQUISITION OF SUBSIDIARIES (Continued) Acquisition of Beijing Borey Advanced Technology Co., Ltd. (“Borey”) (Continued) The trade receivables acquired in this acquisition had a fair value of HK$954,000, which was the same as the related gross contractual amount. It would be the best estimate from management at acquisition date of the contractual cash flows expected to be collected. Goodwill arose in Borey Acquisition because the cost of the combination includes a control premium. In addition, the consideration paid for the combination effectively included an amount in relation to the benefit of the expected cost synergies and technological integration of surface mount technology. These benefits are not recognised separately from goodwill because they do not meet the recognition criteria for identifiable intangible assets. The initial accounting for goodwill acquired in the above business combination with the fair value HK$22,162,000 has been determined on a provisional basis. The amount of goodwill and intangible assets may be adjusted accordingly upon the completion of initial accounting year which shall not exceed one year from the acquisition date. Acquisition of Soft Rock Technologies Sdn. Bhd. (“SRT”) On 7 February 2023, the Group entered into a share purchase agreement to acquire 100% equity interest in SRT, a company based in Malaysia, at a purchase price of MYR7,033,000 (equivalent to approximately HK$12,902,000), subject to certain adjustments as set out in the share purchase agreement (“SRT Acquisition”). SRT is a software development company with expertise in process and factory automation. The SRT Acquisition was completed on 16 February 2023 and has been accounted for using the acquisition method. Acquisition-related costs have been excluded from the cost of acquisition and recognised as an expense in the period when incurred within the “general and administrative expenses” line item in the condensed consolidated statement of profit or loss. Cumulative acquisition-related costs in respect of the SRT Acquisition amounted to HK$856,000, of which HK$110,000 was charged to profit or loss in the current period with the remaining amount charged to profit and loss in prior year. Assets acquired and liabilities recognised at the date of acquisition are as follows (determined on a provisional basis): SUBSTANTIAL SHAREHOLDERS’ INTERESTS IN SHARES As at 30 June 2023, the following persons (other than the interests disclosed above in respect of directors or chief executives of the Company) had interests or short positions in the share capital of the Company as recorded in the register required to be kept by the Company under Section 336 of the SFO: CORPORATE GOVERNANCE The Company has complied with all the code provisions set out in the Corporate Governance Code (the “CG Code”) contained in Appendix 14 of the Listing Rules throughout the six months ended 30 June 2023. The Company reviews its corporate governance practices regularly to ensure compliance with the CG Code. The Company has adopted the Model Code as set out in Appendix 10 of the Listing Rules. Specific enquiry has been made to all directors of the Company, and all of the directors have confirmed that they have complied with the Model Code throughout the six months ended 30 June 2023. AUDIT COMMITTEE The Audit Committee of the Company (the “Audit Committee”) comprises four Independent Non-Executive Directors and one Non-Executive Director who together have substantial experience in the fields of auditing, legal matters, business, accounting, corporate internal control and regulatory affairs. REVIEW OF FINANCIAL STATEMENTS The Audit Committee has reviewed the Group’s unaudited condensed consolidated financial statements for the six months ended 30 June 2023 in conjunction with the Company’s external auditor. EMPLOYEE SHARE INCENTIVE SCHEME The Company has adopted the Employee Share Incentive Scheme (the “Scheme”) for the benefit of the Group’s employees and members of management. The specific objectives of the Scheme are (i) to recognise the contributions by certain employees and to provide them with incentives in order to retain them for the continual operation and development of the Group; and (ii) to attract suitable personnel for further development of the Group. The Scheme was approved by the shareholders of the Company at the Company’s annual general meeting held on 7 May 2019, and adopted by the Company on 24 March 2020 (the “Adoption Date”). Under the Scheme, the shares of the Company may be allocated or awarded to employees or directors of the Group as selected by the Board. The Scheme will be valid and effective for a period of ten years commencing from the Adoption Date. PURCHASE, SALE OR REDEMPTION OF THE COMPANY’S LISTED SECURITIES During the six months ended 30 June 2023, neither the Company nor any of its subsidiaries purchased, sold or redeemed any of the Company’s listed securities except that an independent professional trustee appointed by the Board under the Scheme, pursuant to the terms of the rules and trust deed of the Scheme, purchased on the Stock Exchange a total of 361,500 shares in the Company at a total consideration of approximately HK$22.8 million (excluding ancillary trading fees, costs and expenses directly attributable to the purchase). CLOSURE OF REGISTER OF MEMBERS For the purpose of determining shareholders’ entitlement to the interim dividend, the Register of Members of the Company will be closed from 16 August 2023 to 18 August 2023, both days inclusive, during which period no share transfers can be registered. In order to qualify for the interim dividend, all transfers accompanied by the relevant share certificates, must be lodged with Company’s Share Registrar in Hong Kong, Tricor Secretaries Limited, at 17/F, Far East Finance Centre, 16 Harcourt Road, Hong Kong, not later than 4:00 p.m. on 15 August 2023. The interim dividend will be paid on or about 31 August 2023.

26. Infosys Responsible AI

Overview

While enterprises around the world harness AI's rapid advancements to unlock business value, they are also getting exposed to their fair share of risks like bias, security threats, privacy violations, copyright infringement, hallucinations, and malicious use, to name a few. Lack of transparency and mechanisms to enforce strong principles of Responsible AI are some of the key hurdles enterprises face.

The regulation and policy landscape is also evolving rapidly, and upcoming legislations like the EU AI Act are putting different obligations on all participants across the AI value chain to adopt specific standards and safeguards. It has now become imperative for enterprises to build technical and policy-driven guardrails to safeguard themselves against any hazards without which they could suffer from loss of reputation, incur hefty penalties from regulators, and face costly litigations from those adversely affected and cause irreparable harm to all stakeholders.

Infosys Responsible AI suite of offerings and services, part of Infosys Topaz, is designed to help enterprises navigate the complex technical, policy, and governance challenges related to embedding strong foundations of Responsible AI across the organization. These offerings have helped Infosys in its journey to become AI-first.

TALK TO OUR EXPERTS

Implementing Responsible AI in enterprises presents a unique set of challenges balancing innovation, ethics, legal compliance, and maximizing return on investments. Ensuring responsible AI practices throughout the supply chain, especially when using multiple AI systems, requires state-of-the-art technical, legal, and domain expertise.

Even with well-established guidelines and governance mechanisms, enterprises can encounter several challenges. Enterprises are caught in the “Responsible AI Gap” - an inability to translate principles and frameworks into tangible actions.

Our Responsible AI Suite is based on the AI3S framework of Scan, Shield, and Steer, built on an end-to-end autonomous platform approach to Scope, Secure, and Spearhead enterprises’ AI solutions and platforms.

The Infosys AI3S Suite of responsible AI offerings - Scan, Shield, and Steer helps our customers adopt AI responsibly.

SCAN

We help our clients identify the overall risk posture, legal obligations, vulnerabilities, and threats arising due to AI adoption using a set of offerings named ‘Scan’ that collate information from multiple external and internal sources and create a single source of truth for tracking the risk and compliance status of all AI projects. Scan includes:

a. Infosys Responsible AI Watchtower for continuous monitoring of external facing events, such as regulation and policy changes, threats, vulnerabilities, risks, industry best practices, and technology advancements in the field of Responsible AI.

b. Infosys Responsible AI Maturity and Risk Assessments for gauging compliance readiness, discovering risks, analyzing gaps, and preparing roadmaps to scale Responsible AI in an enterprise.

c. Infosys Responsible AI Audit to ensure compliance with existing standards and regulations.

d. Infosys Responsible AI Control Center for internal telemetry and monitoring of compliance status of AI systems; sense and predict violations ahead of time, alert the right stakeholders, and stage interventions.

e. Regulation Readiness Consulting to be future-ready for upcoming regulations and third-party risks.

SHIELD

We offer our clients technical and specialized solutions, guardrails, and accelerators for protecting AI models from vulnerabilities. Our suite of offerings bundled under the name ‘Shield’ helps embed ‘Responsible by Design’ across the AI lifecycle. Shield includes:

Infosys Generative AI Guard Rails detects and mitigates anomalies corresponding to Responsible AI tenets and vulnerabilities in both the prompt and the output and acts as a sentient moderation layer to ensure safe and responsible use of Generative AI.

Infosys AI Model Security provides enterprise AI security solutions that detect different adversarial attacks on models like poisoning attacks, evasion attacks, inference attacks, injection, and more, and respond to them in real-time, reducing the threat surface of organizations.

Infosys Responsible AI Toolkit is a collection of specially designed Responsible AI pipelines and API endpoints that can be integrated for enforcing ‘Responsible AI by design’ principles into the AI lifecycle. This toolkit automates the tedious tasks of ensuring compliance, thus enabling data scientists and developers to focus on innovation.

Infosys Responsible AI Gateway brings together our domain expertise to redesign AI-powered mission-critical business workflows to reduce the propagation of AI risks downstream. It involves restructuring the business workflow to safeguard against AI failures by introducing safety aspects such as humans in the loop or generation of alerts.

STEER

We offer our clients advisory and consulting services to enable them to advance their RAI journey and become leaders in the space. We assist them in setting up, governing, and managing a dedicated Responsible AI practice. Our offerings called ‘Steer’ aid clients in formulating their strategy and achieving exceptional results via standardized audits and industry certifications. Steer includes:

Responsible AI Practice Setup develops end-to-end RAI practice and adoption across the organization with a mix of frameworks and advisory services. We assist in developing customized best practices, playbooks, capabilities, and policies to implement strong AI governance across the board.

Responsible AI Strategic Advisory Services address AI governance challenges. Our responsible AI consultants create the mechanisms and escalation paths that provide oversight for an RAI program and formulate an overall strategy.

AI Crisis Management helps in designing crisis management protocols and developing proper disaster recovery mechanisms to engage in swift remediation in the event of an unforeseen AI-related crisis.

VALUE FOR ENTERPRISES

Robust automated model governance process and controls for audits, monitoring, and telemetry

Standardized and automated model life cycle management

Responsible metrics-driven decision-making in model development and post-deployment monitoring

End-to-end service offering by understanding current process maturity, implementing third-party platforms/deploying custom solutions, validating models for risks and standardizing processes

Backed by expert consultants who are well versed with regulation, industry, technology, and product trends and have experience in setting up AI COE

Leverage global partner alliances for bringing industry best practices and solutions as part of the implementation lifecycle

Accelerate responsible innovation by leveraging prebuilt accelerator kits

Fast track time to value for building custom responsible AI solutions using industry and function-specific use cases

Technology-agnostic custom solution development to address different facets of Responsible AI

Practical implementation knowledge based on experience of deploying hundreds of models in production for global clients



Challenges & Solutions

Automating and simplifying complex bureaucratic processes and oversight mechanisms

While enforcing Responsible AI, organizations tend to throttle innovation and progress with several manual checks and balances. Our automated technical and policy guardrails ensure that these checks and balances are embedded and enforced as per guidelines seamlessly across the AI lifecycle without the need for major human interventions. These solutions will manually scan your AI systems for any violations, irregularities, and risks and automatically mitigate the majority of the risks or alert human agents. Our offerings and services help transition management to responsible AI by reducing friction and embracing and internalizing RAI principles.

Technological constraints

Different technical guardrails might not always be available per the customized AI use-cases, models, data types, and RAI principles. Our offerings offer comprehensive protection for all data types like text, structured data, images, speech, and audio and for all types of AI models, and different use-cases and purposes like detecting bias and enforcing fairness, improving transparency, protecting from security and privacy violations.

Customization and scalability

Tailoring RAI guidelines to suit the enterprise’s unique needs and complexity, while maintaining consistency and compliance can be daunting. Our offerings are highly customizable and scalable per different industries and business functions.

Continuous monitoring and enforcement

Continuously monitoring AI applications to ensure compliance with RAI guidelines and enforcing adherence throughout the AI lifecycle can be resource-intensive and complex. AI models and use cases are dynamic and ever-changing. Adapting to evolving regulatory requirements that may change over time necessitates agility and the ability to modify strategies and implementations accordingly. Harmonizing enterprise RAI guidelines with global standards and ensuring consistent adherence across different regions and jurisdictions can be a complex task due to varying regulatory landscapes. Our Infosys offerings continuously monitor the techno-legal landscape for recent threats, vulnerabilities, risks, and policy changes and accordingly adapt the guardrails.



27. Samsung Group Digital Responsibility

AI Ethics Principles

AI is a rapidly developing area and its global social, economic impact is also growing.
While recognizing that technologies promote innovation, the same technologies raise important challenges to be addressed. Samsung SDS acknowledges the use of AI technologies should aspire to human dignity and human rights as well as to a sustainable environmental ecosystem. Therefore, Samsung SDS will establish corporate AI ethics principles based on UNESCO's recommendations for the ethics of artificial intelligence. We believe all use of AI technologies must respect the rule of law, human rights, and values of equity, privacy, and fairness. These principles set out our commitment to develop, deploy, and use technology responsibly.

1. Respect for Human Rights

Based on the Samsung Spirit, our top priority is to benefit humanity and society. We respect and comply with international human rights laws and values in general and also in relation to AI technologies. Further, we will work to limit any potentially harmful or abusive application that can negatively affect human beings and their rights as we develop and deploy AI technologies.

2. Diversity and Inclusion

We believe that everyone should be treated fairly and equitably. We understand that defining fairness is not always simple and differs across cultures and societies. We will seek to avoid biased results and unjust impact on sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief. We also will seek to avoid exposing children to inappropriate content.

3. Data and Privacy Protection

We recognize the importance of protecting the privacy and security of people’s data. To minimize privacy risks, we will continue to monitor data processing processes and develop safe and secure practices.

4. Conservation of Environmental Ecosystem

We will comply with relevant national and international regulations, standards and practices to assure that AI development and services do not adversely affect the sustainability of the environment and ecosystem.

5.Communication

We believe in transparency and explainability. AI will be explainable for users to understand its decisions or recommendations to the extent technologically feasible and that this does not jeopardize corporate competitiveness. Samsung SDS will also devise countermeasures against the risks and negative consequences that AI technology can cause to the users.



28.  Sea Limited – Digital Transformation Strategies

Sea Digital Transformation Strategies Report Overview

Sea Ltd (Sea) is tapping into AI, big data, mobile, cloud and ecommerce among other disruptive technologies to enhance its operational and service capabilities. The annual ICT spending of Sea was estimated at $1.2 billion for 2023. A major share of this spending is earmarked for acquiring software, ICT services, and network and communications from vendors. Sea carries out the development of platforms for online, mobile entertainment, and communication for users. Its Garena platform is an online games developer and publisher with global footprint across more than 130 markets, enables users to access mobile and online PC games, and other entertainment content such as live streaming. The company operates an ecommerce platform called Shopee that connects buyers and sellers. It offers SeaMoney, a digital payments and financial services provider and provides various digital financial services such as ewallet services, payment processing, credit-related digital financial offerings, and other financial products to individuals and businesses. The company operates in Singapore, China, Indonesia, Malaysia, Philippines, Taiwan, and Vietnam.

The Sea Digital Transformation Strategies report will act as a reference point to understand a company/competitor’s digital strategy. It will also help in understanding the digital preparedness of the company against its peers. Information included in these reports are sourced from a mix of our very own internal database and authentic secondary research links such as company’s annual report, presentations, press releases etc. The report covers overview of the company, its digital transformation strategies, technology focus areas, technology initiatives, investments, acquisitions and ICT spending among others.

Digital Transformation Strategies

Sea stores and process large volumes of data related to gameplay, ecommerce, and payment processing. Its proprietary data analysis engine collates and structures data to generate valuable insights on user needs, preferences, and behaviors that help to enhance services, user experience, effectiveness of cross-promotions, and discover opportunities.

To gain more insights on digital transformation strategies and initiatives of Sea

Sea Technology Theme Focus

Sea is tapping into AI, big data, cloud, mobile and ecommerce among other disruptive technologies to enhance its operational and service capabilities.

Sea Technology Initiatives

Sea has been involved in several strategic technology partnerships and collaborations, and technology developments and roll outs over the past few years.

For instance, in 2020, Sea selected Weyland Tech to enter the food delivery market in Jakarta. Under this collaboration Weyland’s AtozGo food and grocery delivery service will be integrated with ShopeePay, allowing users to transact AtozGo orders within ShopeePay mobile app. This collaboration also allows AtozGo mobile app users to pay for their deliveries using ShopeePay’s payment technology.

Buy full report for more insights on other technology initiatives of Sea

Sea Resources ICT Spend by Function

Data center

Communications

Network

Applications

End-user computing

Management

ICT Service desk

Sea ICT Spend by Channel

Internal development and maintenance

Technology vendors (direct)

Local resellers

Telcos

ICT services providers/consulting firms

Specialist outsourcers

Systems integrators

Sea ICT Spend by Segment

Software (including Cloud SaaS)

Hardware (including Cloud IaaS)

ICT services

Consulting

Network and communications

Others

Scope

This report provides:

Insight into Sea’s technology activities.

Insights of its innovation programs.

Overview of technology initiatives covering partnerships and technology introductions.

Insights on each technology initiative including technology theme, objective, and benefits.

Details of estimated ICT budgets.

Reasons to Buy

Gain insights into Sea’s technology operations.

Gain insights into its technology strategies and innovation initiatives.

Gain insights into its technology themes under focus.

Gain insights into its various partnerships.

Sample Report

Sea Limited – Digital Transformation Strategies was curated by the best experts in the industry and we are confident about its unique quality. However, we want you to make the most beneficial decision for your business, so we offer free sample pages to help you:

Assess the relevance of the report

Evaluate the quality of the report

Justify the cost

Download your copy of the sample report and make an informed decision about whether the full report will provide you with the insights and information you need.





29. Grab’s AI Ethics Principles

Since our inception in 2012, Grab’s mission has been to drive Southeast Asia forward by creating economic empowerment for everyone. We believe that the use of AI technologies can help us make rapid progress on our mission and solve Southeast Asia’s most complex problems. Grab uses AI to build a safe and trusted digital platform, solve real-world problems, and empower Southeast Asia’s micro-entrepreneurs to grow and thrive. We also recognise that AI needs to be developed and deployed responsibly to mitigate risk and potential for harm. Our AI ethics principles guide our efforts to this end. We will continue to deliberate on and evolve these principles to keep pace with the rapid developments in AI technologies, their impact, and associated tradeoffs.

1. Promote benefits to society: As we design AI-based solutions for our users, we aim to take into account the social and economic needs of individual users as well as society at large with the goal of ensuring that the final product is beneficial to all. Grab’s values of heart, hunger, honour, and humility are used as guiding principles in the system design process with the goal of ensuring that each stakeholder’s needs are taken into consideration.

2. Fairness and Inclusion: We work hard to ensure that outcomes generated by AI systems are fair and promote inclusion. While different people may understand fairness differently, we strive to ensure that AI-generated outcomes do not disadvantage or harm any stakeholder groups. However, because our ecosystem is composed of various stakeholders such as driver and merchant partners, consumers and the platform, at times we may require balancing individual tradeoffs with the goal to improve the overall efficiency and performance of the ecosystem. Grab will also continue to seek to understand the social, cultural, and other context-specific nuances in Southeast Asian markets in order to create hyper local solutions that limit any unintended discriminatory impact. We will continue to experiment with technical and non-technical solutions to mitigate the risk of bias in AI models and datasets.

3. Transparency: We believe that it is important for our users to understand the process through which the decisions that impact their day to day lives are made. We will strive to provide better clarity on how our algorithms work and the factors that inform them. This transparency will also help us mitigate unfairness concerns in AI outcomes, build customer and partner trust, and continually enhance the design and performance of our AI systems based on feedback from consumers. 

4. Safety and reliability: It is of utmost importance to us that our AI systems function safely, reliably, and adhere to the highest performance standards to minimise any unintended outcomes. We will continue to use AI and enhance its performance to develop more rigorous safety practices for our driver-partners, merchant-partners, and consumers, including use cases like fraud detection. 

5. Human-centricity: We aim to continually understand and accommodate the evolving needs of our consumers, driver-partners, and merchant-partners in the design and deployment of AI systems. The context in which users experience these systems and the feedback they provide ultimately govern how we design and improve AI systems to provide the most optimal user experience. 

6. Accountability: We employ a detailed system of checks and balances to ensure that AI systems are functioning reliably and outcomes are consistent with the intention, design ideas, and values behind them. We will also seek to provide adequate opportunity for human intervention and control where necessary. To enable user access to quick and efficient complaint redressal, we have put appropriate appeals and feedback mechanisms in place and will continue to improve them as needed. 

7. Privacy and Security: We will continue to incorporate appropriate privacy and data security features in the development and use of AI systems. This process will not only involve adherence to how Grab commits to process users’ personal data (see Privacy Notice) and applicable data protection laws, but also include improvements in our security features to ensure that our systems, including those driven by AI, operate in a secure environment to safeguard entrusted data and company assets. 



30. Sea Founder Warns of Turmoil From the Shift to AI

Sea Ltd. founder Forrest Li warned of a difficult transition to AI in coming years, but stopped short of outlining the Southeast Asian e-commerce leader’s plans for developing artificial intelligence tools.

Li talked about the transformative technology, comparing it to the PC and mobile revolution the industry has navigated. Now AI could be a bigger challenge than those, the 45-year-old founder said in a staff memo released on the 15th anniversary of Sea’s founding. But he emphasized that the company was in a better position financially than in the previous decade.

“The AI wave has not quite arrived, but we can see it approaching,” he wrote. “This technology transition may be harder on us than the last one. Back then, being a newcomer to e-commerce and mobile games freed us to fully focus on the new platform from the start. This time, we are the ones who will have to adapt to a new technology.”

Li, who is fond of penning memos to Sea’s 60,000-plus employees, didn’t directly address some of his company’s more immediate challenges.

The owner of the Shopee shopping app faces intensifying competition in a Southeast Asian market of more than 650 million people. ByteDance Ltd.’s TikTok, Alibaba Group Holding Ltd.’s Lazada and newer entrants like Temu and Shein are vying for the shoppers that are moving online in countries such as Indonesia and Thailand.

Read More: Sea, Grab Face Slowest Southeast Asia Online Growth in Years

Yet Sea has had some success improving its profitability, helped by cost cuts. The company said in March it expects to earn its second straight annual profit this year. Its shares have advanced 61% this year and are nearing their highest in 12 months. Yet they remain far below historic levels.

To cope with the rising competition, Li said in August he intends to increase investments into Shopee. Besides AI, he’s stepped up efforts to build out its live-streaming arm, an offensive move that could erode margins and trigger a price war with TikTok and Alibaba.

31.  Atos blueprint for generative AI

AI is a broad topic encompassing many different families of algorithms and techniques. However, we currently live in a narrow AI era where AI is used for very specific tasks. Some AI techniques such as machine learning have proven to be more efficient than humans in areas such as computer vision, automated translation, predictions and anomaly detection.

Despite the progress, a number of technical challenges remain. There is a huge need for diverse and qualitative data and — in some cases — large volumes of historical data. Furthermore, we need more efficient computing, algorithms and DevOps scalability (e.g. parallelization for algorithms and MLOps tooling for DevOps).

Bringing the best out of AI

Atos defines responsible AI with four key dimensions: fair and ethical, robust and secure, industrialized and eco-sustainable. Atos enables you to boost your business by keeping these foundational elements of responsible AI in mind.

The proposed way in which Atos can boost your business are:

Enable AI – design and deliver cost-efficient and secured infrastructure for your AI needs

Augment with AI – leverage AI to optimize existing business processes and operations

Grow the business with AI – leverage AI to create new business models

The four horsemen of the apocalyptic AI

A failure to follow the four dimensions of responsible AI can have a serious business impact. Below are some of the problems and consequences of (non)responsible AI:

(Un)fair and (un)ethical — Compliance fines, reputation damage, reduced talent attraction, negative corporate social responsibility impacts
(Non)robust and (un)secure — Reputation risk, user acceptability, revenue impacts, compliance fines

(Non)industrialized — Impact on gross margin, lower revenue, productization and scalability challenges

(Non)sustainable — Ambiguous decarbonization goals, reputation damage, higher cost for customers, reduced talent attraction, lower revenue, compliance fines

Ethical considerations are important in AI solutions. There has been a shift from algorithms written by humans to algorithms that learn their behaviour from data. This implies that humans need to be in the loop to control outputs of the AI algorithms, which can be biased by input data or potentially compromised. Full delegation of human-controlled tasks to AI solutions implies greater responsibility in the way AI solutions are designed and maintained in a secure and explainable way.

Putting customer’s purposes at the heart

Finally, it’s important to position AI ethics within a customer relationship. Ethics is relative to every purpose and company. Our goal is to support our customers in building responsible AI. We do this by first defining Atos ethics, which explain how we want to drive our AI projects from an ethical point of view. In addition, they must be aligned with the Atos’s enterprise purpose. Then, we define the AI ethics specificity driven by a particular project that requires a custom view on the situation (e.g. a self-driving car that needs to adapt to the principles and cultures of a particular country).

We establish processes and provide rules and tools to make sure that AI solutions take ethical concerns into account up-front, from their creation to their retirement. In a nutshell, we advise our clients to include AI concerns in their ethics.

Only by taking this broad array of factors into account and designing solutions that align with our priorities, client priorities and the specific business, cultural and legal parameters can we develop AI solutions that are a true win-win for all parties.



32. AI code of ethics

Our Code of Ethics for AI AI is a general-purpose technology that can affect entire economies, and which is spreading very visibly beyond the business area to areas of daily life. A challenge facing both business and society today is how to optimize the opportunities offered by AI technology, whilst addressing the risks and fears that AI may generate. Since its foundation, Capgemini places ethics at the center of its activity. As a leader in digital transformation, we are committed to the adoption of AI in a way that delivers clear benefits from AI technologies within a trusted framework, by building a Code of Ethics for AI. Our ethical culture drives our vision of AI, guided notably by 5 of our core Values: Honesty, Trust, Boldness, Freedom, and Modesty. These Values work together to inform our approach. Boldness drives us to act as entrepreneurs, identifying and pursuing the opportunities presented by innovation in this field. We aspire to increase Freedom by empowering, complementing and augmenting human cognitive, social and cultural skills, giving people more say over how they live their lives. Modesty keeps us mindful of the need to mitigate risks, building solutions that are robust, safe, and human-centric. Honesty underpins our commitment to transparency, and to creating solutions that are accountable and controllable. We consider Trust to be an essential basis for long-standing interdependent relationships with clients, users, and all members of our ecosystem; the value we place on Trust drives our efforts to create AI that protects privacy and ensures equal access rights and fair treatment. Our Code of Ethics for AI guides our organization on how to embed ethical thinking in our business. It is illustrated by concrete examples from projects or solutions that we deliver. Reference to its principles stimulates ethical reasoning and is intended to launch an open-ended process of discussion within the company, with our clients, and with all stakeholders. Our Code of Ethics for AI concerns both the intended purpose of the AI solution, and the way we embed ethical principles in the design and delivery of AI solutions and services to our clients. It should be read in combination with applicable legislations with which Capgemini is, of course, committed to comply. At Capgemini, we believe that human ethical values should never be undermined by the uses made of AI by business. We want AI solutions to be human-centric, which we define as follows: 1. AI with carefully delimited impact – designed for human benefit, with a clearly defined purpose setting out what the solution will deliver, to whom. 2. Sustainable AI – developed mindful of each stakeholder, to benefit the environment and all present and future members of our ecosystem, human and non-human alike, and to address pressing challenges such as climate change, CO₂ reduction, health improvement, and sustainable food production. 3 Fair AI – produced by diverse teams using sound data for unbiased outcomes and the inclusion of all individuals and population groups. 4. Transparent and explainable AI – with outcomes that can be understood, traced and audited, as appropriate. 5. Controllable AI with clear accountability – enabling humans to make more informed choices and keep the last say. 6. Robust and safe AI – including fallback plans where needed. 7. AI respectful of privacy and data protection – considering data privacy and security from the design phase, for data usage that is secure, and legally compliant with privacy regulations. 1. AI with carefully delimited impact ETHICAL CHALLENGE The very first and fundamental ethical question to be considered is the intended purpose of the AI solution and its impact on humans. Like any general-purpose technology, AI solutions can equally enable and negatively affect human fundamental rights. CAPGEMINI’S RESPONSE Capgemini cares about the intended purpose of AI solutions; our solutions must be mindful of the impact on humans and respect universal fundamental rights, principles and values, in particular the Universal Declaration of Human Rights and the UN Global Compact. AI must focus on improving life for humans and should neither exacerbate existing harm nor create new harm for individuals. The intended purpose of an AI application – what the AI solution will deliver, for whom, and to whom – must be clearly defined, and AI should be used according to its intended purpose. To this end, we are transparent about the intended purpose with our various stakeholders, notably the end users, and include appropriate provisions in our agreements, clearly describing the use for which the technology is intended. As AI is a highly evolutive technology, we believe that assessing the impact of AI solutions, notably on individuals, is important to help identify the overall impact, i.e. the likely benefits against the foreseeable risks, such as social impact or potential risk deriving from inadequate or inappropriate use. Assessing the potential impact that a new technology can have before adopting it helps identify undesired side-effects and consequent ethical risks and helps mitigate them. In situations where there is any doubt about a potential risk of affecting fundamental rights, a fundamental-rights impact assessment will be undertaken to ensure that such a risk is eliminated. 2. Sustainable AI ETHICAL CHALLENGE Beyond the direct impact on humans and human society, other beings and the environment can be impacted by AI solutions. The challenge goes beyond guiding “human-friendly AI”, to ensuring “Earth-friendly AI”. As the scale and urgency of the economic and health impacts from our deteriorating natural environment grows, we have an opportunity to look at how AI can help transform traditional sectors and systems to address climate change, deliver food and water security, build sustainable cities, and protect biodiversity and human wellbeing. Furthermore, AI cannot support a sustainable future if it is not itself sustainable by design. CAPGEMINI’S RESPONSE AI systems should benefit all human beings. This means that their design and development should take into careful consideration the social and societal impacts. Design and development must also be mindful of future generations, the environment, and all beings – human and nonhuman alike – that make up our ecosystem. They must be considered as stakeholders throughout the AI solution’s life cycle, so that AI solutions are sustainable and environmentally friendly. We support AI to address challenges in societal areas as diverse as climate change and CO₂ reduction, digital literacy and inclusion, environmental protection, health improvement, and sustainable food production. 3. Fair AI ETHICAL CHALLENGE In order to be effective, AI needs to learn from historical data. The more data, the more accurate an AI system will be in terms of categorizing, predicting, prescribing, and overall decisioning. However, training data for machines, notably statistics, may reflect an organizational or individual perspective on a given subject matter, or a historical picture of reality. This perspective may be biased or incorrect, as data can include various forms of bias, resulting in extrapolations that can conflict with or undermine current trends and desired evolutions, gradually building up over time. This can result in discrimination against certain population groups based on gender, ethnicity, or similar social factors. Likewise, unfair biases and discrimination can be built in the algorithms themselves, by design and development teams lacking appropriate diversity. CAPGEMINI’S RESPONSE We embed diversity and inclusion principles throughout the entire AI system’s life cycle: • AI design and development teams must be built as diverse teams, with diversity in terms such as gender and ethnicity, but also discipline, for multiple perspectives during AI design, and sensitivity to the fullest spectrum of ethical issues. • We seek to identify any unfair bias likely to lead to discrimination and inappropriate results in the context of decision making, and present possible correction scenarios to remove them. • We will advise clients to put in place an oversight process to analyze and address the system’s purpose, constraints, requirements, and decisions in a clear and transparent manner. • AI systems must entail and ensure equal access rights and treatment by people (regardless of ethnicity, disability, age, religious belief, sexual orientation, or other personal characteristics). • As an alternative to – potentially biased – historical training data, generated (i.e. synthetic) data or off-the-shelf industry data should be considered. 4. Transparent and explainable AI ETHICAL CHALLENGE The complexity of AI may amplify the “black box” concern. A “black box” is a device, system, or program that allows input and output to be seen but gives no view of the processes or workings between the two. For example, in tools using artificial neural networks, hidden layers of nodes process the input and pass their output to subsequent layers of nodes, while in deep learning, an artificial neural network “learns” autonomously by pattern recognition. As with a human brain, one cannot see the output between layers, how data has been analyzed, or what has been “learnt” – one sees only the conclusion. Where a conclusion needs to be checked and justified, because it is unexpected, incorrect, or problematic, it can therefore be highly challenging to understand. This is of greater concern where AI functionality plays a role in high-stakes decision-making areas, such as banking, justice or health, where the potential impact of decisions is more serious. CAPGEMINI’S RESPONSE AI must be transparent: its capabilities and purpose should be openly communicated. Decisions based on AI solutions should be explainable, with the degree of explicability dependent on the context and severity of the consequences if the output is erroneous. The data sets and processes used for the AI solution should also be documented to allow for traceability and, if required, for auditability. When interacting with an AI interface, individuals should be aware that they are communicating with a machine, and should not be misled into thinking otherwise, while being informed of the AI capabilities and limits. Individuals interacting with AI should be made clearly aware about the purposes of the AI system, how it works, with whom the information may be shared, the impact of the AI solution and any potential impact on their rights, if any, in relation to the AI system at stake. We will advise our clients to ensure with us that systems developed are explainable, especially regarding data selection and treatment, notably weightings. We will endeavor to indicate the limits that can exist in the understanding of their functioning. Working with technologies that we understand and control, we will provide documentation and training to users to explain the logic behind the functioning of the AI and to indicate the limits of understanding and testing scenarios, in a manner adapted to the different stakeholders potentially concerned. 5. Controllable AI with clear accountability ETHICAL CHALLENGE While the control responsibilities in any IT system depend on organizing accountability, several aspects complexify this with regard to AI. The production environment itself often involves many discrete contributors, including highly specialized third parties, rendering in-built controllability and oversight more difficult. Moreover, in a legal environment largely based on the assumption of human agents, AI systems depend to a greater or lesser degree on AI-driven intelligence, autonomous agents, and autonomous decisionmaking. Determining responsibility for AI outcomes is further complexified by techniques such as deep learning, which can make systems hard to control and outputs difficult to explain. While individuals need to know who will be answerable in case of malfunction, or should a system have unintended consequences or cause harm, tying an AI’s actions or decisions to a human, or group of humans, can therefore present a considerable challenge CAPGEMINI’S RESPONSE Humans should keep the last say on AI, and we should design AI solutions in such a way that AI cannot learn to circumvent the controls and voluntary interruptions by humans. The design of AI solutions should protect the human’s autonomy and decision making. As such, AI solutions should help humans make more informed choices. To ensure such respect, humans should be part of the AI governance mechanism in such a way that they always keep control over AI. Appropriate measures should be implemented from the AI solution’s design phase, with the appropriate level of human oversight depending on the AI solution application area and potential risks. AI systems cannot be the subject “per se” of legal responsibility for their own functioning, so the AI system design should embed accountability rules (to identify who is responsible for what) and trackability principles, allowing the AI-based decisionmaking process to be explained and audited, thus helping to identify and prevent future mistakes or bias. To achieve this objective, we will advise our clients that it implies to define and clearly identify together roles and responsibilities amongst the different actors involved in the design, manufacturing, integration, deployment and operation chain, including the designer of the AI solution, the data provider, and the company that adopts the AI solution or the final user. This would enable appropriate allocation of liability and effective recourse when needed. 6. Robust and safe AI ETHICAL CHALLENGE Like any tools or systems, those utilizing AI must be fit for their intended purposes, and resilient and secure from a technical perspective. As AI uptake increases, so does the scope for potential impact, and the need to also consider the broader social and environmental context in which AI-based tools and systems operate. From this arises the challenge to foresee measures to safeguard against any risks, such as unlikely mishaps or malevolent intent, that might prevent the AI from delivering the desired benefits. CAPGEMINI’S RESPONSE Robustness should be embedded throughout the life cycle of the AI systems, from the design and development to the deployment and use over their lifetime. AI systems should include, when achievable, fallback plans in case of failure of the AI system itself (e.g. allowing to adjust rule-based logic or even switch to human control, to avoid any wrong output), as well as being accurate, reliable and having reproducible results, to the extent allowed by applicable laws. 7. AI respectful of privacy and data protection ETHICAL CHALLENGE With AI taking off, the need for data is greater than ever, much of it driven by consumers. The opportunity for greater freedom presented by easily accessible data brings with it a related risk to data protection and informational privacy. Lengthy user agreements can tempt consumers to click “accept” without checking what rights they are giving away, while companies can be tempted to feed consumer and vendor data into advanced, AI-fueled algorithms, without the awareness and approval of the affected consumers and employees. Facial recognition, voice identification systems and smart homeappliances collect data about when we come and go; while many such functions provide a helpful service, the potential risks they carry are not trivial: Seemingly anonymized data can be de-anonymized by AI. Data collected can also enable tracking, monitoring, people profiling, and behavior prediction. By raising analysis of personal information to new levels of power and speed, AI magnifies our ability to use – and misuse – personal information, presenting a challenge for privacy and data protection. CAPGEMINI’S RESPONSE In agreement with the client, we must ensure, that we will put in place all the necessary means for our current perimeter of responsibility, to contribute to the Clients’ global AI objectives in terms of compliance with privacy regulations, data protection and proper data governance. AI and data protection are compatible as long as data protection and cybersecurity are taken into account from the design phase of any AI project. Besides ensuring full respect for privacy and data protection laws and regulations, adequate data governance mechanisms should also be put in place. In practice, this means that any AI project would need to ensure that only data that are strictly necessary are collected and processed. Indeed, the data collected and used shall be proportionate, accurate, and processed in a secure manner. Individuals will be provided with the relevant level of information on how their data is processed and they should be provided with appropriate means to exercise their rights as may be required by law. CAPGEMINI’S RESPONSE In agreement with the client, we must ensure, that we will put in place all the necessary means for our current perimeter of responsibility, to contribute to the Clients’ global AI objectives in terms of compliance with privacy regulations, data protection and proper data governance. AI and data protection are compatible as long as data protection and cybersecurity are taken into account from the design phase of any AI project. Besides ensuring full respect for privacy and data protection laws and regulations, adequate data governance mechanisms should also be put in place. In practice, this means that any AI project would need to ensure that only data that are strictly necessary are collected and processed. Indeed, the data collected and used shall be proportionate, accurate, and processed in a secure manner. Individuals will be provided with the relevant level of information on how their data is processed and they should be provided with appropriate means to exercise their rights as may be required by law.

33.  AI based anti-tailgating Solution

dormakaba presents technology innovation of an AI based anti-tailgating solution at BAU 2023

Interview with Marc Faresse: Marc is an expert in AI and Video Analytics within the Group Innovation Management Team at dormakaba. With his many years of experience, he gained within and outside the company, he is helping dormakaba to introduce AI-based technologies like face recognition into its offering.

What is tailgating and what challenges does it present?
Tailgating, the passage of an unauthorized person behind authorized personnel, is one of the most common physical security breaches. Also known as tailgating often results from a random act of kindness such as holding the door to a stranger. It can be seamless and a lot less suspicious to follow an authorized person rather than breaking into a building. Those with criminal intentions are well aware of this.

In high-traffic settings like large facilities, companies, or residential complexes, the risk of tailgating is higher. Tailgating can expose people and companies to perils like loss of revenue or goods, reputation, and even physical danger. Hence, it’s not a surprise that tailgating is among the top three security concerns of Fortune 1000 companies.

How is it possible to mitigate the risks of tailgating? 
One of the most efficient ways of mitigating tailgating risks is to install appropriate access control systems and manage them methodically — a suitable solution as such is turnstiles.
As the entrance control method of choice for bustling facilities, turnstiles allow only one person at a time, and only after the visitors present the appropriate entrance credentials.

Depending on the needs of a building, it’s possible to operate the turnstiles either with or without the assistance of the front-desk or security staff.

Integration of video surveillance in key security spots on a building, and particularly the main entrance, not only deters criminals but also helps law enforcement authorities to identify the tailgaters in case of a crime.

Thanks to technological advancements in biometrics and machine learning, some modern video security systems can even differentiate between people passing in the foreground and tailgaters.

Wearable identification, such as badges, ensures that anyone who carries it is authorized to be in the building. This includes all permanent staff and visitors, as well as temporary workers.

While visitor badges are inexpensive and ubiquitous, other tools such as a QR code generated from a mobile app or biometric credentials can also help to prevent the risk of tailgating.

While appropriate measures such as turnstiles, credentials, or surveillance can reduce the risk of tailgating, security is a collective effort.

Has dormakaba offered an innovative concept?
The idea is to cover all hiding areas and dead angles in airlock, room, corridors, or a gate with an AI system that could recognize a person and a part of body detection to determine if there is a tailgating situation in airlock or any space that has restricted access. In addition, this AI based system could recognize more objects and detects more situation like counting the number of people crossing the gate or counting the number of luggage passing by a self-boarding gate or a mom with her baby in a trolly, reacting according to each particular situation.
 
The innovation here is a supplement camera to count a lower body part in combination with a camera that could detect the person from the top.

What kind of anti-tailgating development is behind the concept of dormakaba?
The concept is based on AI. The camera sees and recognizes a person and other objects that could cross a gate or a door. Our patent application is currently being processed. 
 

We are currently working on the proof of concept. 

The camera only captures the lower part of the body to ensure privacy.

The sensors are small and compact, allowing for very short gates. 

We will receive additional information about objects passing through the gates and respond accordingly, such as detecting a person in a wheelchair or a person with their dog.

This data can provide more detailed information for better decision-making.

Our goal is to improve safety and enhance the user experience and to avoid false alarms.

Using cameras and sensors to detect people as they move through a space. The AI algorithm can analyze the data from the cameras and sensors to understand the situation and react as needed, while dealing with data privacy concerns.

This involves training machine learning algorithms to recognize patterns and can be particularly useful in situations where there are limited sensors or cameras available, or where the environment is more challenging.

This is continuing the AI development we have done with existing sensors we have in the ARGUS gates launched in 2016.

Is there a product that will incorporate this innovation?
At the BAU 2023 trade fair, we will present the new compact Argus V60 sensor gate. It offers safety, elegance, and efficiency in a very small space. Thanks to modern sensor technology, this compact sensor barrier, which has already won the German Design Award Special 2023, gives architects and users more freedom without compromising on personal protection and separation detection. Particularly in areas such as foyers with elevator systems or within office buildings.



34. Trustworthy AI

Trustworthy AI - What it means for telecom

Trust and reliance on modern telecom systems are widespread. However, the adoption of artificial intelligence (AI) introduces new risks and necessitates countermeasures. Governments, companies, and standards bodies worldwide are recognizing the need for trustworthy AI systems. The European Union AI Act and Ericsson's adherence to ethical guidelines demonstrate this commitment.

Introduction

Billions of people have come to trust and depend on modern telecom systems to support their needs and quality of life. As these systems adopt new technologies, it’s important trust is maintained by understanding and addressing any new risks. Artificial intelligence (AI) differs from traditional software in its construction and operation and may introduce new and varied risks, calling for new countermeasures and guardrails. For example, the large amount of data used for AI training raises the possibility of privacy risks. Development procedures must ensure that AI models learn what is intended. The model operation should be thoroughly understood, for example, using explainability techniques. In short, to maintain the trustworthiness of the overall system, AI must itself be trustworthy: meaning, it should operate as intended and do no harm physically or ethically.

Governments, companies, and standards bodies around the world are taking notice of these facts and creating requirements regarding the trustworthiness of AI systems. The upcoming European Union AI Act1 is one such effort. It follows principles written by the European Commission High-Level Expert Group in their “Ethics Guidelines for Trustworthy AI”2. Ericsson has adopted these guidelines. The framework breaks trustworthiness into seven specific areas. This paper explores how six of these areas apply to AI in telecom systems, some of which are depicted in Figure 1.

Human agency and oversight

Human agency and oversight requirements make sure that humans can always intervene in AI-controlled systems before things like fundamental rights and safety have the potential of being affected - in other words before they could pose any harm. Implementation of such requirements means having “humans in the loop”, the difficulty of which depends on the timescale of the decision and the criticality of the system.

Sometimes AI operates at timescales, which are much too fast for human intervention, such as the optimization of radio operations in a base station. In telecom, these uses typically do not have a direct impact on the rights and safety of individuals. However, they can dramatically impact network operations, which can subsequently affect humans, so they need careful assessment.

Implementing human agency and oversight in AI-aided network operations requires user research, product function design, and user testing to make sure that network operation engineers can detect and intervene when needed. The human-machine interaction design must be aligned with existing network operation processes, rely on existing interfaces, and provide actionable information to users.

The interface for human agency and oversight may vary depending on the use case and users. It can be a GUI, CLI, Rest-based API, or even a physical interface (for example, a light). For instance, an AI system that detects and predicts network-wide congestion may use a graphical interface embedded in a dashboard used for regular network design and operations. The system may also send alerts if AI performance deteriorates (for example, by reporting too many false network congestions) to the network operations center (NOC). The alerts should include everything the NOC engineer needs, including reasons for the problem, potential root causes, and solution suggestions. The engineer then has three choices: to switch to a non-AI based function; to understand and solve the AI issue; or to escalate it. Alert volume should be considered, to not overwhelm a potentially already heavily loaded NOC. Explainable AI methods can help in generating needed, and user-tailored, reasons for problem.

The use case, user knowledge, and persistence of the AI notification influence the actions to be taken. Intermittent or ephemeral events may need to be repeated. More serious events may need an escalation path.

Transparency

Trust can arise from understanding how a system works, or from experience using it over time. The complexity and black-box nature of AI can lead to suspicion, particularly when people feel that the AI’s own creators don’t fully understand how it makes decisions, and what exactly it has learned. Greater transparency can help build trust by understanding and explaining AI models to humans. Explainable AI (XAI) refers to methods and techniques that produce models which show why and how an AI algorithm has made a certain decision. It helps stakeholders understand how decisions are being made in different formats: by identifying what input factors were most important in making an inference and by providing explanations and responding to “why” and/or “what-if” questions. It also helps a human operator in decision-making. If the operator is not satisfied with a response, a further investigation can be performed, using computational argumentation techniques.

Creators of AI for telecom should provide XAI methods to help build the trust of their direct customers (for example, service providers), and in turn, enable them to build trust for their subscribers. The explainability of AI should start with design and continue through implementation, as a built-in feature, to ensure transparency throughout the AI development lifecycle. In addition, different XAI techniques should be researched, and developed to explain different types of machine learning (ML) methods.

The Ericsson white paper3 presents different XAI techniques applied to different AI/ ML methods, including machine reasoning (MR), and reinforcement learning (RL). The explanations generated by these XAI techniques not only help explain the decisions to humans but also support automation, for example, in root cause analysis when combined with other AI techniques.

Explainability of ML, that is, feature analysis techniques (including SHAP and LIME) can be used in multiple telecom use cases to identify and explain the problems and root causes of specific ML model outputs in addition to ensuring the overall correctness of the ML models. These techniques can be applied to ML-based predictions to investigate the most important features that contribute to certain prediction results and validate the correctness of the ML model. The results of these explainability techniques can support MR components in identifying the root cause of the problem. 5G slice assurance is one such use case where these techniques are thoroughly investigated and tested. In this use case, certain Quality- of-Service (QoS) requirements (such as throughput, latency, and availability) are agreed upon with the customer in a service level agreement (SLA) and must be met throughout the lifecycle of the slice. ML models are used to proactively identify any potential violation of the agreed QoS requirements. Upon a violation in prediction, explainability techniques are applied to identify the most contributing features which in turn helps NOC Engineers in identifying the root cause of the problem4. These techniques can be applied to multiple use cases in a similar manner, like cell shaping and key performance indicator (KPI) degradation prediction, focusing on latency- and network throughput–related optimizations.

Explainability of RL: RL is suitable to solve many cellular network problems due to its dynamic nature, online training, interaction with the environment, and outstanding performance over traditional rule-based techniques for the telecom domain. An RL agent performs an action (such as applying a policy) in an environment to maximize rewards. The explainability of RL includes methods applied to different RL components, such as rewards and policy explanations.

In a base station, the antennas are tilted up, down, or kept the same to optimize KPIs, that is, the coverage of the network, increased quality by reducing interference, and capacity/ throughput of the network. Coverage refers to the area from which a UE can access the cellular network, while capacity refers to the amount of traffic the cellular network can handle simultaneously. Remote electrical tilt (RET) refers to adjusting the tilt of the antenna by an RL agent to optimize the above mentioned KPIs. Increasing the down-tilt reduces the area covered by the antenna, with the risk of leaving a certain area without coverage but increases the capacity in the covered area due to a stronger signal. In contrast, up-tilt results in a larger area covered but lower capacity due to a weaker signal. Explainability is important in the RET optimization of antennas in a cellular network. Explanations help in understanding the reasons behind a specific adjustment. The following explainable reinforcement learning (XRL) methods are applied to this use case5:

Reward decomposition provides intuitive contrastive local explanations for the agent’s decisions by decomposing the reward into multiple sub-functions to adjust the tilt (see Figures 2 and 3), while achieving the same performance as the original DQN. The generated contrastive explanations are very user-interpretable, as they concisely answer questions in the form of “why did you decide to down-tilt instead of up-tilting?”

The Linear Model U-Tree (LMUT) reaches high performance while employing a fully transparent linear model capable of generating both local and global explanations (see Figure 4), however, it is less transparent than reward decomposition.

Autonomous Policy Explanation summarizes the trained policy and explains it in natural language, thus enabling the policy to be understood by everyone including non-experts.

Contrastive explanation through embedded self-prediction produces a local explanation about the internal representation of the RL agents (intermediate or inner layers of the deep neural network). It compares two different actions, such as why the antenna is tilted down and not up.

In addition to enabling transparency and AI automation, Ericsson has seen the potential of reducing the input feature set by using XRL. A novel method was developed for connecting explanations from both the input (feature analysis) and output (reward) ends of a black-box RL model, resulting in fine-grained explanations6. Reward prioritization, performed by the user, generates two different levels of explanation, and allows RL agent reconfigurations when unwanted behaviors are observed. Privacy and data governance

It is usually necessary or desirable to prevent AI systems’ data from being disclosed. If the data includes the personal data of individuals, it may be subject to stringent legal requirements. Business data might contain intellectual property or be subject to contractual constraints. Sometimes, the data or other information related to it can be inferred from an AI model, especially when combined with publicly available data sources. Laws and regulations requiring privacy for individuals, such as the EU General Data Protection Regulation (GDPR)13, predate the mass adoption of AI across industries. Despite being written in a manner to make them future-proof, such regulations don’t necessarily anticipate the extent of potential AI risks. There are also often contractual requirements for privacy. And even in the absence of such requirements, it is generally understood that maintaining privacy is ethically the right thing to do.

Ensuring privacy in telecom AI impacts the entire AI lifecycle and requires the application of Privacy by Design and Privacy by Default (as defined by GDPR7 and other global privacy laws), starting with controls where the training data is collected and continuing through model use (inference). Many controls relevant to AI are the same as or similar to those used in other types of data processing. The nature of the data, the purpose of the collection, who will use it, and how and when it will be used, should be clearly communicated. Only the minimum amount of data required for the intended purpose should be collected. Unnecessary fields should be redacted or masked. Controls like pseudonymization, encryption, authentication, and authorization should be used to ensure appropriate access. Uses of the data should be logged and auditable. Once no longer needed, data should be securely erased.

Since AI models learn from the training data, in some sense the data is encompassed within the model. Therefore, it’s unsurprising that attacks exist on models to extract the training data itself or make inferences about it. Privacy enhancing technologies (PETs), including differential privacy, exist to help make AI models less susceptible to such attacks.

Developers can also analyze model sensitivity to data extraction, for example, how many queries are required. Such metrics can inform decisions about the privacy risk involved in deploying the model.

Data can be exposed during training if it is improperly secured. When federated learning is used, multiple participants with independent datasets can contribute to building a single global model. This helps them keep their datasets private, but the protocols used must be carefully designed to ensure no information is leaked.

Diversity, non-discrimination and fairness

Bias in models can come from bias in the training data, which itself may stem from historical prejudices and inequities. It may also be caused by disproportional representation. One example is natural language processing systems used for interaction with subscribers, such as chatbots, or support ticketing systems. Even within a single language, training such systems should consider different speaking styles, idioms, and education levels. Not everyone speaks perfectly, but everyone deserves the same level of service.

Steps to avoid bias include:

understanding what categories exist in the input data that need to be treated equally

making sure each category is adequately represented in the training set, regardless of size

being aware of historical inequities that might be relevant to the problem at hand and adjusting the data accordingly

ensuring model robustness, including thorough testing

considering categories separately throughout the model development process, so that the model performs well for each

Even a seemingly solely technical problem can have a bias that impacts people, sometimes in subtle ways. Consider the use of AI to structure and operate mobile networks. This can be impacted (or biased) based on how and where that data is collected, or by cognitive bias already present in the system’s creators. If data collection is skewed toward a particular group (for example, people with a certain economic status, which might correlate to other factors such as race), the resulting system might be inadvertently biased. Telecom systems have to work everywhere in the world and should provide equally good service to all people.

If more investment (that is, equipment, optimization, effort) goes into certain areas, some of these areas might get disproportionately better service. Such areas could have different usage patterns. When ML that is used to plan, deploy, optimize, and operate networks is trained on data collected in some regions, networks (or products) might be created that work well only in those regions.

Technical robustness and safety

Human safety is not typically directly impacted by telecom operations but can be affected by situations like loss of service. Telecom systems can be important components in emergency communications and disaster handling. Another example is an application like autonomous vehicles, where the loss of communication might impact the ability to proceed safely.

AI can assist in these situations when it makes the network itself more robust. But this means that the AI itself must be robust. Careful attention to AI quality must be taken during training and deployment. Fallback mechanisms should be in place for cases where the AI cannot decide or makes an out-of-bounds decision to, for example, transfer the control to a human operator.

AI is also subject to new types of attacks. Data extraction attacks, which have already been mentioned, compromise the training data and potential privacy. Researchers have also demonstrated attacks on inference using adversarial examples. Carefully crafted inputs are fed to the model, causing its inference to be biased in a direction chosen by the attacker.

Where the attacker has access, poisoning the training data can be used to influence model operation. A motivated attacker might use these mechanisms to affect network operations or steal a service. The training pipeline, resulting models, and surrounding application context should be analyzed for susceptibility to such attacks.

Automated model quality assurance is crucial for technical robustness and safety. Models must be thoroughly tested against performance metrics that reflect potential real-world scenarios. These metrics are use case and model specific. For instance, in the case of a classification model, accuracy, false positive or false negative rates might be measured, while in the case of clustering, Silhouette Coefficient8 and Dunn Index9 might be chosen.

It is also important to communicate this information in an understandable and useful manner to model recipients, keeping in mind that they may not have expertise in AI. For example, when communicating a performance metric about a model, information should be included about its meaning and what values are considered good or bad.

Since a model’s performance depends on the data set used for training and evaluation, data quality is essential for model quality. A training data set should accurately represent reality and cover the events or objects of interest: the training data set must have the same statistical properties the real objects have, and if there are relationships between the attributes of the real object or among the real objects, those relationships must be preserved in the training data set.

The RET use case mentioned earlier shows that poor AI decisions could lead to interference and compromised network performance. Anywhere an AI algorithm is used in the operation of a network, a failure of that algorithm can lead to inefficiency, instability, or, at worst, downtime.

Another important concern is that AI models could, inadvertently or maliciously, take actions that are unsafe for humans. As already mentioned, telecom systems by nature are not safety-critical, and the danger could be reduced network performance. One scenario could be when the model explores the space of all possible states and actions. This becomes significantly important for RL, where space exploration is seen as an effective way to train an RL agent to capture a near-optimal policy. However, unchecked exploration can lead the system to visit a dangerous state, for example, when the system tries to tilt the antenna at an overly high angle. Safe RL methods provide a shield to block unsafe actions that might result from free exploration of state-action spaces. The intention is to allow the agent some state exploration of the environment while having boundaries using safety specifications defined by a human developer. The specifications, or boundaries, can be user dependent. One such use case where these techniques are successfully tested is RET Optimization10.

Conducting invariance and directional expectation tests is also essential to assess and assure the model’s robustness. In an invariance test, label-preserving perturbations are applied to inputs and the model prediction is expected to remain the same. In a directional expectation test, a set of perturbations are made to the input which should have a predictable effect on model output.

Social and environmental well being

AI can be used to create positive benefits for society, such as by helping protect the environment. Ericsson considers communication to be a fundamental human right, so the availability of the network is core to societal well-being. The findings of this white paper support this conclusion. AI, by helping service providers create telecom networks that are more reliable, ubiquitous, and inexpensive, contributes to the social goal of universal communication. But there are possible negatives, such as the privacy aspects discussed above. ML training can be energy intensive, so careful cost-benefit analysis is needed before it is employed. ML might be used where there are potential safety or societal risks (for example, control of critical infrastructure). Minimizing those risks is important, and this can be done using the techniques described in this white paper.

AI can help improve network operations and energy consumption. Large volumes of data can be used to optimize important goals like performance, reliability, capacity, and energy usage. Traditional optimizations, written by programmers, typically used only a few parameters and simple algorithms with limited results. AI allows large numbers of parameters to be used, better optimizations, and therefore better performance. Ericsson teams looking at specific use cases have shown that AI can lead to significant energy savings compared to traditional algorithms11.

AI in telecom can be useful beyond the simply better operation of the network. During the COVID-19 pandemic, Ericsson engaged in a joint project with a service provider, government officials, and two hospitals12. The service provider provided anonymized and aggregated data about people’s movements, taken from their network. This was combined with vaccination, antibody test, and hospital COVID patient admission data. A series of ML models used the data to predict admissions two to three weeks into the future. In eleven of sixteen weeks, the predictions had an error rate of less than thirty percent. Better resource planning for hospitals, especially during a crisis, leads to better patient care. This demonstrates that AI and telecom networks and data can be used to benefit society in novel and perhaps unexpected ways.


Conclusion

The benefits of AI in telecom networks are only just beginning to be leveraged but will clearly be an important and integral element in future networks. Trusting those networks requires trusting the AI, which can be achieved by following the presented guidelines. However, these present a number of challenges:

Ensuring that humans retain oversight and control over AI systems, even when highly automated and operating at high speeds.

Providing information about AI operations, using techniques such as XAI, while maintaining a fine balance between the privacy of models and data and transparency

Protecting the data of users and businesses, while still using it to deliver AI that benefits them

Understanding how AI might affect different communities and preventing adverse impacts

Making AI systems robust and safe, yet practical to train and

Considering how AI affects society broadly, both to prevent adverse impacts and to promote beneficial uses.

Trusting the AI, that is having confidence that it operates as intended and does no harm, requires diligence in addressing each of these challenges. Ericsson is working to continuously improve its AI systems, making them, and consequently their products, more trustworthy.

35. AI ethics & governance

What you can do

Establish AI governance and principles

Adopt responsible AI principles that include clear accountability and governance for its responsible design, deployment and usage.

Assess your AI risk

Understand the risks of your organization’s AI use cases, applications and systems, using qualitative and quantitative assessments.

Systematically test responsible AI

Perform ongoing testing of AI for human impact, fairness, explainability, transparency, accuracy and safety. Use top responsible AI tools and technologies to mitigate any problems.

Conduct ongoing monitoring and compliance

Consistently monitor AI systems and oversee responsible AI initiatives while executing mitigation and compliance actions.

Manage the impact on your workforce, sustainability, privacy and security

A responsible AI compliance program will need to engage cross functionally to address workforce impact and compliance with laws, sustainability, privacy and security programs across your enterprise.

What you’ll achieve



Reduced risk through compliance

A proactive strategy to use AI responsibly from the start will help organizations better prepare to manage risk and comply with emerging regulations.



Trust and value

AI systems that are compliant and regulation-ready will help attract new customers, retain existing ones and build brand and investor confidence.



Retained talent

Demonstrating a commitment to ethical practices will help attract and retain top talent who are motivated by a sense of purpose and shared 



36. SAP AI ethics

SAP delivers AI based on the highest ethical, security, and privacy standards

At SAP, we care deeply about the impact of AI on the well-being of people, the health of our customers’ businesses, and society and economies. We address bias and discrimination concerns when designing AI into our applications, and work to be transparent and explainable. We uphold our highest standards with respect to data privacy, data protection, and cybersecurity.

 

Building on the fundamental dedication to responsible AI that we started back in 2018, SAP is proud to further the commitment by affirming the 10 guiding principles of the UNESCO Recommendation on the Ethics of Artificial Intelligence. These principles cover proportionality, safety, fairness, sustainability, privacy, human oversight, transparency, responsibility, awareness, and multi-stakeholder collaboration, ensuring that AI solutions respect human rights and contribute to sustainable development.

What guides our approach to AI

SAP designed guiding principles to steer the development and deployment of our AI solutions.

Dynamic foundation for continuous engagement with the ethical and socioeconomic changes of AI

We are driven by our values

We design for people

We enable business beyond bias

We strive for transparency and integrity in all that we do

We uphold quality and safety standards

We place data protection and privacy at our core

We engage with the wider societal challenges of AI

Operationalizing responsible AI across our business

Our AI ethics steering committee comprises senior SAP leaders who review our approach, processes, and product capabilities to ensure operationalization and alignment with our policies and guidelines.

SAP’s commitment to protecting customer data in Artificial Intelligence

The strategic use of business data is integral to the success of AI, and by leveraging business data responsibly, we not only enhance the capabilities of our AI solutions, but also improve outcomes for your business. We remain firm in our commitment to prioritize data privacy and security as we release new AI capabilities.

 

Your data remains safeguarded within our ecosystem

Where permitted, we may use your data to help innovate and improve our products

We do not share your data with third-party LLM providers for the purpose of training their models

 

Our AI solutions are developed responsibly

The same rigorous standards that govern all SAP product development extend to our AI offerings

 

We stand by the security of your data

We employ advanced data security measures to protect your personal data at all times



37. Capturing the power of Generative Artificial Intelligence to enhance the passenger experience

Generative Artificial Intelligence (GAI) has dominated headlines of late, with the potential – and dangers – of this new technology highlighted around the world.

Many of you will have thoughts on the topic, or you may have been using the technology itself, while some of you may just be getting to grips with this fast-moving space.

Here I would like to shed some light on the work Amadeus’ is currently undertaking, looking at the impact we can have on our industry and, most importantly, the traveler.

 

What is Generative Artificial Intelligence

First, it is important to stress the significance of GAI.

While AI - the ability to perceive, synthesize and infer information by computers - has been in use across our business for some years, we are now seeing the next-generation of development. Unlike other types of AI, which are focused on classification or prediction, GAI can create original content (such as text, images, voice, or other media), and execute a variety of other tasks based on prompts.

The technology has huge potential to transform various industries by engaging with humans, especially customers, in a personalized and scalable way.

From our location at the center of the travel ecosystem, it has the potential to impact on every stage of the journey, meaning each solution, platform, or technology we currently produce could be impacted.

This is not an incremental change – it is a potentially revolutionary one.

 

Amadeus’ commitment for innovation

As you would expect, Amadeus is committed to exploring the potential of this new technology. We are currently investing in new tools to help us harness the power of GAI, with teams across the business working to exploit its capabilities.

The impact will be felt throughout the whole traveler experience, as already discussed in this blog from my colleague Rudy.

For example, in the planning phase GAI has the potential to create inspirational, traveler-centric search and shopping experiences. New tools will have the ability to search for, summarize and present information in newly accessible ways, driving up demand and lowering customer acquisition costs.

Stakeholders can get deeper insight than ever before into the motivations and actions of a traveler. With GAI, the purpose of a trip, expectations, willingness to pay – and much more – can be identified through chatbot conversations.

On-trip, travelers could see their needs met in more intuitive and personalized ways. Vivid content will be adapted to what the traveler is looking for, while conversational GAI chatbots can be used to ask the right questions to understand the traveler preferences. Disruptions to travel plans can be resolved faster by intelligent rebooking systems.

Finally, post-trip, GAI can help evaluate customer sentiment and allow travel businesses to respond to online reviews, boost their online reputation, stay in contact with customers in a more meaningful way, drive repeat business and maintain traveler relationships over sustained periods.

 

Guardrails for safe use of GAI

Alongside the excitement of GAI, we recognize the importance of addressing the potential concerns around the use of this technology.

Alongside the generation and proliferation of false information, other risks include the potential for GAI use resulting in bias and discrimination. There is also the risk of unintended sharing of sensitive data, while we must watch for any potential hallucinations (a confident response by a GAI that is not based on verified source data) in any created content, as well as cybersecurity risks.

To mitigate these concerns and to ensure Amadeus has a trusted, data-secure way to take advantage of these technologies, we are strengthening our existing policies, processes and governance, as well as employee awareness.

In addition, AI technology is required to be designed following Amadeus’ AI Ethics Principles in order to ensure ethical, transparent and responsible use (you can learn more on these principles here).

We are also closely monitoring the emergence of new regulations, in particular the EU AI Act, and taking steps to ensure our compliance. We also welcome legislation to protect fundamental freedoms, encourage responsible development in AI, while ensuring that innovation is allowed to flourish for the benefit of the industry and traveler alike.

 

Ecosystem Approach 

Amadeus is at the heart of the travel ecosystem and this is a fast-moving space. Our strategic partnership and joint innovation program with Microsoft allow us to work closely with leading experts on GAI to address travel industry needs with the latest foundation models and cutting-edge solutions. 

The Microsoft investment in OpenAI, and work to make sure the GPT models are enterprise ready in the Azure environment, means we can accelerate bringing out solutions that are ready for our customers to start using.

We are also working with Accenture to develop the GAI-powered interactive travel assistant for Cytric Easy.

 

 

By taking a responsible approach, by harnessing the combined power of the travel ecosystem, and by thinking beyond today, we can power progress across the entire travel industry.

We are excited about what the future holds.



38. Ai in product development

AI Isn’t a Shiny New Toy. Or is it?

How we got here–and how to realistically think about AI when building products.

Everybody is talking about Artificial Intelligence these days. In fact, I can’t remember the last time I attended a cocktail party or read anything on the Internet without hearing about ChatGPT and how it will save (or destroy) the planet.

A significant contributor to this newfound popular interest is the advent of the aforementioned ChatGPT (short for Chat Generative Pre-trained Transformer). If you haven’t heard about it (I know, unlikely), it is a chatbot that was launched by OpenAI in November 2022 to great public acclaim (and some consternation). But is it a good thing or a bad thing? Depends on your point of view and how you use it. If you’re a 6th grader who doesn’t mind bending the rules, it’ll write a pretty good term paper for you on Abraham Lincoln, the War of 1812, or Star Trek. 

On the other hand, it sometimes makes things up, for example:

I just asked it: “Talk about the technical director of LEGO Universe.”

ChatGPT: “The Technical Director of LEGO Universe was Mark Hansen. As the Technical Director, Hansen played a crucial role in the development and implementation of the massively multiplayer online game (MMOG) LEGO Universe, which was released in 2010.”

That’s kind of cool, but I was the Technical Director, not Mark.

It was also trained on data that is now a couple of years old, so there are a lot of things it just doesn’t know about. For example:

My question: “Who is in charge of Twitter?”

ChatGPT: “As of my last knowledge update in September 2021, Jack Dorsey was the CEO of Twitter. However, please note that executive positions can change over time, and it’s recommended to verify the current leadership by referring to the latest news or official Twitter announcements.”

As someone who studied artificial intelligence before most people had even heard of the term, it’s interesting to hear the swirling conversation about it in 2023. In this blog, I’ll give you a brief history of AI and the events that led us to where we are today–and how to use that information before proceeding with AI in your product development process.

A brief history of AI & how we got here

I received my Master’s degree in AI from Yale back in 1985, a year that fell between two notable and well-documented “AI Winters,” a term that refers to a period in the history of AI research when interest in the field declined significantly. This term was coined in 1984 at the annual meeting of the American Association of Artificial Intelligence (AAAI) and is a process that begins with pessimism in the AI community and then in the press, followed by significant reductions in funding, then followed by the end of serious research. Three years later, the billion-dollar AI industry began to collapse (again).

 There were, essentially, two different AI Winters:

1974–1980

1987–1993

What ended the first AI Winter? A number of factors contributed, including new theoretical and algorithmic developments, increased computing power, industrial and commercial applications, and government support and funding. And the second AI Winter? In short: it yielded massive, ongoing, real-world successes in a large number of fields like speech recognition, machine vision, face recognition, machine learning, logistics planning, decision-support systems, and the like.

Since the mid-1990s, AI has once again started receiving massive attention due to its potential to revolutionize various industries, from healthcare to finance to retail. Such businesses are looking to incorporate AI into their operations to improve efficiency, reduce costs, and gain a competitive edge. 

However, before jumping on the AI bandwagon, it’s essential to understand how we got here and how to realistically think about AI when building products. It’s also important to remember that systems that seem smart might not be as smart as we think. This error has previously been the major driver in the hype that has led to both previous AI Winters.

AI is not new, but it is shiny. How long it stays shiny (this time) depends on both its actual performance in industry and education and on its hype caused by misperceptions and misleading reporting.

So, what events and developments in AI brought us here? What works? What doesn’t? And what does the history of AI teach us as we build new tools and products? 

While AI has come a long way over the past 70 years, it’s essential to recognize that it’s not a magic solution to all problems. It is not a panacea that can be implemented without careful consideration of its limitations and potential risks.

Our own hype, over-reliance, and over-estimation regarding its current apparent success could cause a new AI Winter (or at least a cooling of enthusiasm and public support). ChatGPT is a good example of how this might occur: there are now claims that ChatGPT can diagnose diseases, pass the LSAT and MCAT, etc. What will happen when it fails as a super-human brain? This is something we should be aware of with this technology and related ones. As the Bard pointed out, not all that glitters is gold.

A critical aspect of realistically thinking about AI is understanding its limitations. Machine Learning systems are only as good as the data they are trained on. If the data is biased, the AI will make biased decisions. Therefore, it is essential to ensure that the data used to train the system is diverse and representative of the population it will be applied to. 

Additionally, it’s essential to understand the potential risks of implementing AI. AI can be used to automate processes, which can lead to job losses, and it even has the potential to manipulate people or make decisions that have a significant impact on people’s lives, like in healthcare or criminal justice. Therefore, it’s crucial to consider the ethical implications of using AI and have measures in place to ensure that the AI is being used responsibly in your business.

How to think about AI for product development

When building products with AI, it is crucial to understand what AI can and cannot do. AI is excellent at processing and analyzing large amounts of data, identifying patterns, and making predictions based on that data. However, it cannot replace human intuition and decision-making entirely. It is essential to have a clear understanding of the problem you are trying to solve and how AI can help you solve it.

So, what should you consider when bringing AI into your product development processes? Here are some questions you might ask yourself:

First, ponder the ethics of what you’re doing. Will the system you are hoping to build reduce public security or safety? Will it put people out of work for no reason? Also, consider if there is enough (and appropriate) data for the machine to learn what you want it to learn. Think about what biases it might be acquiring. Will the system infringe on personal privacy, liberty, happiness? Will any groups of people be disenfranchised as a result? Are you stealing anyone’s intellectual property?

And just how does one use AI in product development?

This is a very large topic, but to break it down a bit, here are a few important options (out of many):

Machine Learning: This is a branch of AI that involves training models on data to make predictions or perform specific tasks. It can be used to develop software with capabilities such as image recognition, natural language processing, recommendation systems, and more. Developers can build and train models using popular ML frameworks like TensorFlow or PyTorch.

Natural Language Processing: This technology focuses on enabling computers to understand and process human language. It can be used to develop software applications like chatbots, language translators, sentiment analysis tools, or even automated content generation systems.

Computer Vision: This involves training models to understand and interpret visual data, such as images and videos. It can be used to build software for tasks like object recognition, facial recognition, autonomous vehicles, or even augmented reality applications.

Intelligent Automation: AI can be used to automate repetitive or mundane tasks, enhancing productivity and efficiency. This can involve building software applications that leverage techniques like robotic process automation (RPA) or using machine learning algorithms to automate data analysis and decision-making processes.

AI-based Code Generation: Tools, including ChatGPT, are now able to take requirements (and other specifications) and generate working code.

Predictive Analytics: AI can analyze large datasets and identify patterns, enabling predictions and forecasting. This can be useful for software applications in various domains, such as finance, healthcare, sales forecasting, and supply chain management.

To leverage AI in product development, developers need to have a good understanding of AI concepts, programming languages, and frameworks relevant to the specific application. They should also have access to quality data for training AI models and a robust infrastructure to deploy and scale the software. Collaboration with data scientists and domain experts can further enhance the effectiveness of AI-driven software development.

In Summary

AI is a powerful tool that can help businesses improve their operations and gain a competitive edge. However, it is essential to understand that AI is not a magic solution to all problems. Realistically thinking about AI means understanding its limitations, potential risks, and ethical implications. When it comes to product development, it also means having a clear understanding of the problem you are trying to solve and how AI can help you solve it. By doing so, businesses can successfully incorporate AI into their product development operations and avoid potential pitfalls.

Today, AI is once again generating excitement and optimism about its potential to revolutionize many industries. However, the history of the AI Winter serves as a reminder of the need for realistic expectations and continued investment in AI research and development to ensure that the technology continues to progress and deliver on its promise.



39. Atlassian Intelligence is built on trust

Atlassian Intelligence is built on trust

Secure

Powered by our trusted platform, Atlassian Intelligence keeps your data secure and private.

Transparent

Stay in control of your data and get visibility into how each feature works.

Scalable

Accelerate critical business workflows with ease across your entire organization.

Atlassian Intelligence brings the power and magic of AI into Atlassian’s Cloud products. Built with our Responsible Technology Principles in mind, Atlassian Intelligence handles your data responsibly.

Frequently asked questions

Hide all

How does Atlassian Intelligence work?Hide

  

Atlassian Intelligence combines state-of-the-art models developed by OpenAI with the power and data inside the Atlassian platform. This means we provide a native artificial intelligence experience that is contextual to you, your teams, and your workflows, all in a way that respects the privacy of your data. To learn more about the technology that underpins Atlassian Intelligence visit this page. 

How should I use Atlassian Intelligence?Hide

  

Some of the models used as part of Atlassian Intelligence, including the models developed by OpenAI, generate responses based on your inputs and are probabilistic in nature. This means that their responses are generated by predicting the most probable next word or text, based on the data that they have been trained on.

Because of this approach, these models can sometimes behave in ways that are inaccurate, incomplete, or unreliable. For example, the responses that you receive could not accurately reflect the content they are based on, or generate content that sounds reasonable but is incomplete and should not be relied on.

We encourage you to think about the situations when you use Atlassian Intelligence — for example, not in cases where you need current and accurate information about people, places, and facts — and review the quality of the responses you receive before sharing them with others.

How do I start using Atlassian Intelligence?Hide

  

Atlassian Intelligence features are automatically activated for any site(s) on a Premium or Enterprise Cloud plan unless you have opted out of using these features.

Which Atlassian Cloud plans include Atlassian Intelligence?Hide

  

Atlassian Intelligence is a feature of Atlassian Enterprise and Premium Cloud plans, which include the following products: Jira Software and Confluence.

Which Atlassian Intelligence features will be automatically activated?Hide

  

Discover which Atlassian Intelligence features will be automatically activated by reviewing this table.

Can I turn off Atlassian Intelligence if my organization is not yet ready to use it?Hide

  

Yes. If you are an admin, you can log into Atlassian Administration (admin.atlassian.com) and remove Atlassian Intelligence at the individual product level. Once deactivated, you will see a confirmation flag in your instance. For more details please review our documentation or contact support directly.

If I deactivate Atlassian Intelligence now can I reactivate it in the future?Hide

  

Yes. You can reactivate Atlassian Intelligence now or anytime in the future. Once Atlassian Intelligence is activated across a Cloud product, any newly released Atlassian Intelligence features are automatically live by default.

For more details please visit our documentation.

Does the Atlassian Privacy Policy apply to Atlassian Intelligence?Hide

  

Our Privacy Policy covers all of our products and services, including Atlassian Intelligence.

Does the Atlassian Customer Agreement apply to Atlassian Intelligence?Hide

  

Yes, Atlassian Intelligence is covered by the Atlassian Customer Agreement. Additionally, we introduced Atlassian Intelligence-specific terms that are incorporated by reference into the Atlassian Customer Agreement, which govern your use of Atlassian Intelligence.

How does Atlassian Intelligence use customer data?Hide

  

Atlassian Intelligence processes your user’s inputs to provide the outputs your user has requested. We may also process organizational data from within your product instance that the user has permission to view and include that data with the user inputs, so that LLMs can provide more accurate, relevant, and contextual responses.

Our partner, OpenAI, neither retains your inputs and outputs nor uses your inputs and outputs to improve its services.

However, Atlassian may store your inputs and outputs for a limited period of time to reduce latency, such as when displaying a page summary, or when required to provide a feature, such as displaying a search history. To learn more about which features store data, please visit our transparency page.

If you would like to be notified of any material changes to this policy, please subscribe here.

Does OpenAI store Atlassian customer data?Hide

  

No, OpenAI does not store the data you submit or the responses you receive.

Does Atlassian send customer data to OpenAI’s platform to train its services?Hide

  

The data you submit and the responses you receive via Atlassian Intelligence are not used to fine-tune or improve OpenAI’s models or service. Each data request is sent to OpenAI individually, over an SSL encrypted service, to process and send back to Atlassian.

What are you using my inputs and outputs for?Hide

  

We process your inputs to provide you with the outputs you requested. We do not use your input or output for any other purpose.

Are you using my inputs and outputs to train Atlassian Intelligence?Hide

  

Atlassian Intelligence uses only data about how you interact with our features, such as the people you work with and the size and type of attachments, and feedback you provide. Atlassian Intelligence does not use your inputs or outputs.

Does Atlassian Intelligence use my data to serve other customers?Hide

  

The data you submit and the responses you receive are used only to serve your experience. They are not used to train models across customers or shared between customers.

Can we opt out of our data being used in training Atlassian Intelligence while still being able to use the features?Hide

  

You can opt out of using Atlassian Intelligence at any time. If you choose to use Atlassian Intelligence there is no fine-tuning on your inputs or outputs, only the feedback you provide.

Is any data transferred outside our current site?Hide

  

Data is transferred outside of the current site for specific Atlassian Intelligence features that use partners like OpenAI in order to generate a response. Even though the data is transferred, it follows existing Atlassian security practices. For Atlassian Intelligence, each data request is sent to OpenAI individually, over an SSL encrypted service, to process and send back to Atlassian.

Please refer to our list of data sub-processors for all LLM partners here.

Who can access my data in Atlassian Intelligence if we enable the feature?Hide

  

Atlassian Intelligence respects all of your existing permissions; a user may get a different set of results based on the content they have access to, which may differ from another user.

The data a user has access to is not limited to the site they’re working on. Due to the connected nature of our products, as long as a user has access to a Jira issue or Confluence page, information can be pulled from across those experiences to inform a response (or output).

To the extent you have installed or are using Marketplace apps, have granted access outside of your organization to other types of users, or are working with a solution partner, those second or third parties will have access to data (not specific to Atlassian Intelligence).

How are the Atlassian Intelligence capabilities ensuring my data is protected?Hide

  

Atlassian Intelligence currently leverages large language models from OpenAI. Atlassian Intelligence follows our existing security practices. For Atlassian Intelligence, each data request is sent to OpenAI individually, over an SSL encrypted service, to process and send back to Atlassian. OpenAI does not use Atlassian Intelligence inputs or outputs to train its models. For our existing Atlassian Intelligence features, Open AI does not log the inputs and outputs.

What additional assurances do we have that our data is protected?Hide

  

All requests from Atlassian services to OpenAI follow a central process and infrastructure to ensure that the same protections are applied. All services that use the central process go through an Atlassian standard for authentication and authorization.

All requests to OpenAI’s APIs from the OpenAI Gateway service are made over an encrypted connection using Atlassian’s OpenAI API keys. Beyond this, we inherit and rely on OpenAI's security and privacy controls.

Does Atlassian Intelligence respect data residency?Hide

  

Yes. If you have turned on data residency, all of your in-scope product data will remain stored in the region you’ve selected.

Does Atlassian Intelligence impact my compliance with GDPR?Hide

  

We are committed to helping our customers stay compliant with GDPR and their local requirements.  As we do today for all of our products, we will process and transmit data for Atlassian Intelligence in accordance with our Privacy Policy, Data Processing Addendum, and GDPR commitment.

Is Atlassian Intelligence SOC 2 and ISO compliant?Hide

  

Although many of the systems and services used by Atlassian Intelligence hold these certifications and adhere to the same internal policies and standards, Atlassian Intelligence itself has not undergone external assessment for SOC 2 or ISO certification. As we further develop our Atlassian Intelligence services and tools, we will aim to include them in our standard audit certification reporting cycle.

Is Atlassian Intelligence HIPAA compliant?Hide

  

No, at this time Atlassian Intelligence is not HIPAA compliant and our Business Associate Agreement (BAA) does not cover this feature. If you are required to comply with HIPAA, we recommend that you do not opt-in to this feature until we have expanded our coverage to include it.

Can I limit the data or restrict the data that is shared with Atlassian Intelligence?Hide

  

We currently only offer opt-out controls at the product level. We plan to offer more granular controls to customers in the coming months and will update this page once controls are available.

Does Atlassian Intelligence respect existing permissions?Hide

  

Atlassian Intelligence honors all existing permissions within each feature. Users will not be able to create or generate content based on resources they do not have access to.

Ex. #1. You would not see issues/projects that you do not have access to if you do a natural language search to JQL or you would not get Confluence pages sourced for an answer to a question if you did not have access to those pages.

Ex. #2. if a Confluence user executes a smart search, the results shown will take into account the pages and spaces the user has permission to view, and ignores restricted pages and spaces.

Where can I learn more about Atlassian Intelligence?Hide

  

Visit our guide on Atlassian Intelligence to learn how teams can accelerate work, increase efficiency, and provide value faster.



40. Flutterwave strategic agreement

 Flutterwave to further power global digital payments on the Microsoft Azure cloud platform.

LONDON, June 22, 2023 /PRNewswire/ -- Flutterwave a leading African payments technology company today announced it is working with Microsoft to build its next generation platform on Microsoft Azure, powering payments infrastructure across the African continent and beyond. This agreement reflects com Flutterwave's commitment to give businesses and individuals access to global-grade services across all of Africa and drive digital transformation around the world. With Flutterwave and Microsoft's plans to power payments to-and-from Africa, this collaboration is an incredible opportunity to impact growth across the continent.

Through this engagement, Flutterwave will support the accelerated growth of transactions processed on Flutterwave platform for global clients like Uber, Netflix, and Microsoft, solidifying Azure's role in facilitating a seamless, reliable, and secure payment experience.

Key Flutterwave products such as Flutterwave for Business, Send by Flutterwave, Flutterwave Store, and Flutterwave for Fintech Platform, are being developed and transitioned onto the robust Azure cloud platform. Moreover, Flutterwave uses Azure OpenAI Service capabilities, enabling the scaling of its product offerings to millions of merchants worldwide.

"Microsoft has been an invaluable partner, providing a platform that allows us to deliver consistently high-quality services to our clients," stated Olugbenga Agboola, Founder and CEO of Flutterwave. "As we manage high-volume payment processing, particularly during peak periods, the robustness, reliability, and scalability of Microsoft Azure become critical. As such, deepening our collaboration with Microsoft is the most logical step forward for us." It means Flutterwave will continue to drive the transformation of global commerce, taking full advantage of the diverse and expanding range of services offered by Microsoft.

"Our development on Microsoft Azure has set a strong foundation for Flutterwave," said Gurbhej Dhillon, Flutterwave CTO. "Their platform provides us with significant developer leverage, which we harness in service of our clients. Looking to the future, we're excited about the possibilities of scaling with Azure OpenAI Service, which will enable us to serve even more merchants worldwide," added Dhillon.

"We have proudly supported Flutterwave's core operations with Microsoft Azure for many years. We are excited to further fuel their growth and innovation through this expanded collaboration," said Mike Gaal, Microsoft Corporation General Manager. "Our mission is to empower every person and every organization on the planet to achieve more. Working with Flutterwave will take us a step closer to achieving our mission In Africa," added Gaal.

About Flutterwave

Flutterwave is the leading payments technology company that enables businesses across the world to expand their operations in Africa and other emerging markets through a platform that enables local and cross-border transactions via one Application Programming Interface (API). Flutterwave has processed over 400M transactions in excess of USD $25B and serves more than one million businesses, including customers like Uber, Airpeace, Bamboo, Piggyvest, and others. The company's key advantage is connecting businesses to various local and international payment types to enable them to expand globally. It also enables cross-border transactions from the diaspora to African countries via its SendApp product. Flutterwave processes payments via multiple payment modes, including local and international cards, mobile wallets, bank transfers, and Google Pay. The company has an infrastructure reach in 34 African countries. For more information on Flutterwave's journey, please visit www.flutterwave.com.