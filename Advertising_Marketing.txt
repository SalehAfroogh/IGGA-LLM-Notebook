141. AI regulation is about finding the right balance

AI regulation: it’s about finding the right balance

AI regulation is a government, industry, regulator, company and private individual issue – and it must be addressed collaboratively

How to regulate AI and to build AI models responsibly is one of the top business concerns for 2023 – and it will surely rank highly going forward. Governments worldwide (and private individuals) are also considering how and when to respond to the march of AI (on top of all the macro crises the world is enduring).

It is fair to say that AI regulation is on all our radars. That is what prompted WPP and BCW to host a forum that explored the future of AI regulation.

WPP is ahead of the curve. It has been investing in AI for the few years because AI is, as WPP CTO Stephan Pretorius calls it, “a transformation technology when applied to marketing and advertising”. He calls the pace of innovation over the last year “radical” – impact of AI on client work, business outcomes, the industry and society has become front and centre. There is no compromise in terms of values and integrity.

WPP’s AI company, Satalia, is at the core of innovation and has enabled WPP to push beyond just applying AI tools to be at the forefront of discovery. Importantly, WPP’s AI expertise the building of explain-ability into its use of AI tools.

BCW's own experience in building tools, such as Decipher, with its partner Limbik, and supporting clients on articulating policy and external communications perspectives through its Navigate team means that they too are at the forefront of work in this area.

The UK Government is taking a lead

Given the high penetration of AI expertise in the UK, the UK Government is in pole position to be a leader. Alexandra Leonidou, Head of Regulation and Governance at the UK Office for AI says that the UK Government has recognised the profound effect of AI globally.

She talks of its impact on public services like healthcare and education – not only its impact on marketing and advertising. While she identifies the risks – and they are profound (AI-enabled cyber-attacks is about finding the right balance) – she outlines that regulation is about finding the right balance.

In March 2023, the UK Government published its AI White Paper – AI regulation: a pro-innovation approach – which had six core characteristics at its heart: pro innovation, proportionality, trustworthiness, adaptability, clarity and collaboration. It has taken both a principles-led and context-led approach to regulation, and it expects, says Leonidou, to lean on existing regulators to oversee agreed principles.

The UK Government plans to start on its regulatory journey with a non-statutory approach, says Leonidou. The Government has consulted extensively – with 400 businesses and individuals, including WPP – and already established the central risk function in government.

Stealing the headlines has been the AI Safety Summit and the discussion paper launched just before the summit: Frontier AI: capabilities and risks. Funding for a digital and AI advisory service has also been secured. The outcome of the AI summit – with its roster of around 150 high-profile attendees from around the world – has been well publicised: to build a shared understanding of frontier risks, to establish a forward process, to agree appropriate measures, to identify measures for AI safety research, and to showcase how safe AI will enable AI for good globally.

Leonidou calls the Bletchley Declaration, published at the end of the summit, “ground-breaking”. A total of 28 countries, including China, signed this declaration, thereby agreeing to seize the opportunity of AI for peace and wellbeing, to affirm that all actors have a role to play, and to consider a proportionate and pro innovation approach to governing AI. The Emerging Processes document will help ensure conversations and collaboration continue to flow.

What is more, the AI Safety Institute has been launched to carry out AI research and build models. This organisation will support technical standard development in partnership with other jurisdictions – partnerships with Singapore and the US have already been inked.

Where are we at?

There are so many ways to think about AI but, at WPP, Chief AI Officer Daniel Hulme has a very clear train of thought. He points to the six applications of AI which helps us navigate governance, one application at a time.

He also points to the AI impact pyramid with disruption at its apex (where AI disrupts the commercial and operating model), production and services occupying the layer beneath that (with their AI embedded tools and processes), and a core productivity base supporting everything (with its AI embedded core productivity tools and back office).

But when we think about the risks associated with AI, Hulme refers to micro risks, malicious risks and macro risks in society, and the importance of distinguishing between these three risks. But, in the final analysis, says Hulme, there’s a series of important questions to consider in relation to how AI is used and the extent to which it should be regulated:

1. Is the intent appropriate? AIs don’t have intent but humans do.

2. Are the algorithms deployed opaque or transparent? There needs to be explain-ability.

3. What harm could an AI cause?

WPP is proud to have developed a set of principles, guidance, and legal advice, which underpin our internal generative AI platforms and tools and help our people and clients understand AI responsibility. These include WPP AI Policy and WPP Data & AI Ethics Principles and Guidelines, and specifically covering generative AI – Generative AI Principles.

Broadening the debate

From a wider perspective, what is becoming clear is that there is no real common understanding of the responsible use of AI. Yves Schwarzbart, Industry Relations Manager at Google UK and Co-chair of the ASA’s AI Taskforce, called for collaboration and coordination throughout the advertising and marketing industry.

Jesse Shemen, CEO and co-founder of Papercup – an AI translation company – talked about quality control in the use of AI tools and agreed there is no common understanding of the responsible use of AI (which is why a principles-based approach is the right one). Hulme concurred that there is no commonality of understanding of risk which is why WPP undertakes significant engagement with clients to help them understand training models, biases and risks associated with copyright violation.

In spite of the dearth of common understanding, Leonidou pointed out that we are already seeing emerging initiatives – such as the White House Executive Order – take similar approaches to each other. This is exactly what is needed if we are to avoid barriers to trading across borders.

Perhaps this is at the crux of AI regulation: how do you build a common understanding of the challenges business faces while also understanding the risks and not limiting the scope for businesses to reap the rewards? And how do we make sure companies that adopt AI technologies are not penalised by regulation compared with their early-mover peers?




142. Omnicom first mover access to AI insights

With Omni Assist, Omnicom Leverages First-Mover Access to Open AI to Accelerate the Timeline from Insights to Outcomes

Reveal Marks the Launch of Omni 3.0, Powered by Generative AI

CANNES, France, June 19, 2023 — Omnicom (NYSE: OMC) today unveiled Omni Assist, the inaugural Generative AI capability enabled by a first-mover partnership with Microsoft. Omnicom is the first agency holding company to have enterprise access to the latest Open AI GPT models.

Omni Assist is a virtual assistant providing insights, notifications, and recommendations across every step of the workflow of the Omni open operating system, from audience development to planning, activation, measurement, and optimization. Expected to reduce discovery from days to minutes, and with the ability to enhance communications across agency and client teams, Omni Assist will accelerate the timeline from insights to client outcomes.

In addition to accelerating campaign time-to-market, Omni Assist delivers on the primary promise of Generative AI as outlined by Omnicom CEO John Wren: improving workflow in a way that enables Omnicom’s knowledge workers to be more productive – and its agencies to add more value – in the marketplace.

“We’re embracing Generative AI as quickly as possible to enhance the capabilities of our best and brightest people, and deliver better outcomes for our clients,” said Wren.  

Revealed during a Cannes Lions event celebrating Omni’s fifth anniversary, Omni Assist is part of launch of Omni 3.0, the next generation of Omni where every experience is powered by Generative AI.

Launching the Next Generation of AI Powered Innovation 

AI technology and techniques – including computer vision, language modeling and facial recognition – have been core to Omni’s capabilities since its launch in 2018 as the industry’s first people-based, precision marketing and insights platform, distinguished in the marketplace by its open architecture and dynamic federation of data partners.

Recognized by leading research and advisory company Forrester as the holding company platform with the deepest integration across its network, Omni launched its second generation in April 2021, with an enhanced user experience designed to expand access to more users across Omnicom agencies. Omni 2.0 also saw the launch of healthcare and PR verticals; data collaborations and/or integrations with Affinity, Amazon, Google, Infosum, LG,  Teads and Yahoo; clean room partnerships with AWS, Disney, NBCU and TelevisaUnivison; and retail media partnerships with Albertson’s Media Collective, Instacart,  Krogers and Walmart Connect.

With Omni 3.0, Omnicom has deployed Microsoft Azure Cognitive Services to build its most powerful capabilities yet for its agency teams and clients, delivering benefits such as real-time conversational enablement for all Omni applications; automated audience intelligence that surfaces hard to identify audience behaviors; and performance intelligence that summarizes key trends and identifies drivers of performance with optimization recommendations.

“When we launched Omni in 2018, we described its mission as the democratization of data – a mission that we’ve met with more than 17,000 Omni trained and certified users across the holding company, and Omni integrations into agency-level planning processes across the network,” said Annalect CEO Slavi Samardzija. “With Omni 3.0 – powered by Omni Assist – we are going beyond the democratization of data, to the democratization of insights.”

“Most recently, I have had the privilege of working with Omnicom on their transformation efforts, leveraging Microsoft Azure Open AI to power Omni 3.0. We are at the heart of Omni Assist, a virtual assistant providing insights, notifications, and recommendations the Omni workflow, from audience development to planning, activation, measurement, and optimization. This platform is evolving quickly and is unique to anything I’ve seen in the industry,” commented Simon Crownshaw, Director of Media & Entertainment, Microsoft.

In addition to Omni Assist, Omni 3.0 launch capabilities will also include Omni Commerce, which was launched earlier today in Cannes. Omni Commerce, the industry’s first connected commerce orchestration solution, enables Omnicom to maximize brand awareness and increase the effectiveness of its clients’ retail media investments, driving product sales and profitability.

About Omnicom Group Inc.
Omnicom Group (www.omnicomgroup.com) is a leading global marketing and corporate communications company. Omnicom’s branded networks and numerous specialty firms offer services in advertising, strategic media planning and buying, precision marketing, commerce and brand consulting, experiential, customer relationship marketing (CRM), public relations, healthcare marketing and other specialty communications services to over 5,000 clients in more than 70 countries.



143.  Dentsu AI innovations

Dentsu Teams Up with AWS to Further Scale GenAI Innovation for Brands

Published on: 23rd January 2024

Rounds out Generative AI stack with adoption of Amazon Bedrock and Amazon SageMaker 

Dentsu today announced it has extended its relationship with Amazon Web Services (AWS) by adopting two key services to further scale its use of generative artificial intelligence (GenAI), driving new levels of innovation and opportunity for clients. Amazon Bedrock and Amazon SageMaker add new, differentiated technologies to dentsu’s full enterprise-grade, GenAI stack. Dentsu’s AI strategy, to provide safe and pervasive use of tools across the global business for the development of both client-ready products and operational innovations, is already yielding impactful results. With AI-driven client campaigns already in the field and the widespread adoption of AI-powered tools to boost workflows, drive efficiency and unleash creativity, dentsu is driving real, responsible results. 

Implementing GenAI will allow dentsu to help its team of over 72,000 employees globally to innovate faster. Using Amazon Bedrock and Amazon SageMaker will help dentsu to more easily and more quickly deploy third-party and open source models across its product and engineering teams. This gives dentsu employees access to a vast range of cutting-edge technologies from the external global technology community. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies via a single API, along with a broad set of capabilities for building GenAI applications which meet dentsu’s high standards for security, privacy, and responsible AI. Meanwhile, Amazon SageMaker enables dentsu’s data scientists and developers to build, train, and deploy machine-learning models on any scale, quickly and easily. It includes modules that can be used together or independently to build, train and deploy models. The result is game-changing levels of access to platforms that give dentsu’s client teams the ability to quickly innovative and prototype at scale, creating new products and services to drive client outcomes. It also provides dentsu access to best-in-class capabilities for deploying AWS proprietary models such as Amazon Titan, which provides a breadth of high-performing image, multimodal, and text model choice. 

The use of these new services within dentsu has been pioneered by Dentsu Digital, Japan, during a private preview programme with AWS. The team has worked in close collaboration with dentsu colleagues around the world to quickly upskill and support, as well as apply GenAI to prototypes. 

Saturo Yamamoto, Dentsu Digital Inc. Executive Officer in charge of AI, commented, “Dentsu has been developing multiple AI solutions on AWS, and we continue this tradition with our latest advanced customer experience enhancement service brand "∞AI (mugen AI)", some components of which also leverage AWS infrastructure. We have been actively evaluating methods to harness the potential of Amazon Bedrock immediately following its release, intending to integrate these AI capabilities into our service progressively. We eagerly anticipate continuing to contribute to our clients' growth and transformation through our products, as we have done so far.” 

“Generative AI is one of the most transformational technologies of our lifetime, significantly impacting productivity and creativity,” said Atul Deo, General Manager, Amazon Bedrock. “Using Amazon Bedrock, dentsu established new ways of improving productivity that combine technology with the knowledge of local teams. For example, dentsu is able to easily experiment with and evaluate top foundational models for advertising use cases, privately customizing them with their own data securely. This opens up new opportunities for dentsu to become more efficient and cost-effective, whilst increasing team creativity. We look forward to growing our relationship with dentsu in these next years and supporting the development of generative AI-powered tools that enable customers to boost productivity and power innovation.” 

Today’s announcement builds on news last year about dentsu’s deployment of a range of AI tools and services from Microsoft, Google, Salesforce and Adobe. 

About dentsu 
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society. 
 



144.  Interview with the CFO on artificial intelligence

Please tell us your basic approach to investment.

A.At the Hakuhodo DY Group, we view investments as strategic expenses for strengthening our business foundation, including the expansion of human resources well-versed in digital and other technologies. We also view investments such as mergers and acquisitions (M&A) as actions that directly impact the balance sheets.
Strategic expenses are geared toward realizing organic growth over the medium to long term. In a sense, they function as the seeds, fertile soil, and water needed for growing our business.
With the rapid progression of digitalization, it is crucial that we secure personnel who can implement digital marketing activities. It is also imperative that we increase the number of personnel well-versed in technologies to establish a foundation for future growth. Being able to respond to technologies such as artificial intelligence (AI) and extended reality (XR) is key. Furthermore, as we work to transform our business structure, we must reform our workstyles and establish a sustainable operating structure. The expenses we are currently investing serve as the cornerstone for steadily implementing such initiatives.
However, these kinds of investments take a certain amount of time to produce results. As we are currently in the phase of foundation building, our expenditure on investments is greater than usual. We therefore expect that our profit growth will be much more gradual than it has been in the past.
We will need to continue investment even after the conclusion of the current Medium-Term Business Plan. Once we have achieved a certain level of progress with establishing our business foundation, however, our basic approach will be to keep selling, general and administrative expenses within the range of gross profit growth and strive to steadily improve our operating margin.
In addition, to accelerate the speed of growth, we must incorporate external functions and capabilities through such methods as M&A and capital alliances.
With regard to global business domains, in particular, gross profit from our overseas businesses accounts for over 20% of our total consolidated gross profit. Accordingly, we need to execute M&A in order to further enhance these businesses.
Moreover, domestically, we are considering M&A as an option for further enhancing our product lineups in digital, marketing execution, and other domains. We also recognize capital alliances as an effective means for bolstering our technological capabilities.
I would like our investors to understand that we do not view M&A simply as a tool for expanding the scale of our business. Rather, the purpose of executing M&A is to augment our strengths in the areas in which we are lacking, enhance our product lineups, and enhance operating efficiency through the generation of synergies.

Q.You stated that, as a general rule, you aim to keep expenses within the range of gross profit growth. Could you please tell us your thoughts on investment discipline, including for capital expenditures and M&A?

A.Obviously, we aim for a return and profit that is commensurate with the amount we invest. As a basic policy, we aim for a return that exceeds the cost of capital.
We recognize that the Group’s current cost of capital stands at around 7%, and we determine hurdle rates for investment at major operating companies keeping this figure in mind.
Also, in terms of investment limits, we believe that we should be able to cover investment and shareholder return amounts through cash flows generated by our operating activities.
Timing is also an important element of investment, so we will not necessarily reject an investment if the amount exceeds cash flows generated by operating activities in the near term provided the investment meets certain criteria. In such instances, we will make use of funds procured from external sources and then aim to balance out the repayment of the investment amount over the medium to long term.
In other words, we will always strive to maintain a balance of net cash over the medium to long term.

Q.Under the Medium-Term Business Plan, you are actively promoting investment in order to reinforce your business foundation. Aside from expenses that directly impact profits, to what degree are you planning to execute investment?

A.To explain our investment plans over the period of the Medium-Term Business Plan, which runs through fiscal 2023, if we consider our results for investments in fiscal 2021 and the outline for investments in fiscal 2022, and in the event that we achieve our targets for fiscal 2023 (the final year of the Medium-Term Business Plan), total earnings before interest, taxes, depreciation, and amortization (EBITDA) will come to approximately ¥240.0 billion. Furthermore, if we assume over the same period that the total payment of corporate income tax and dividend payments trend at the same level as in fiscal 2021, we should have around ¥100.0 billion remaining at the end of the plan. This amount serves as the basis of our estimations of the investment limit that I mentioned earlier. If we factor into this limit the sales of investment securities conducted in fiscal 2021, the future sale of Group-owned assets, and the balance of net cash at the time we revised our Medium-Term Management Plan, I believe that we will be able to make investments at amounts that exceed the limit while still maintaining a sound financial position.
Meanwhile, in fiscal 2021, we invested approximately ¥10.0 billion in acquiring tangible and intangible assets. As we are aggressively investing in technologies and rethinking our office layouts, we will most likely invest greater amounts in acquiring such assets in the remaining two years of the plan.
Also, we acquired SoldOut,Inc. via a takeover bid (TOB) in fiscal 2021, and we will continue to actively enhance our functions through M&A and capital alliances going forward. However, as other parties are involved, I am unable to clarify exactly how much we will invest in such initiatives at this time.
In conclusion, however, I am able to say that we plan to invest over ¥100.0 billion in building our business foundation over the three-year period of the plan and that we have enough financial capacity to do so.

Q.You mentioned that you will maintain a balance of net cash even after investing over ¥100.0 billion throughout the three-year period of the plan. How do you view the Group’s current financial position?

A.Looking at our business cycle, we would like to maintain a balance of cash and cash equivalents totaling around one month’s worth of billings. On average, we record billings of at least ¥100.0 billion a month, so we intend to raise funds with an awareness of that level.
At the fiscal 2021 year-end, cash and time deposits stood at ¥183.9 billion, and interest-bearing debt was ¥126.4 billion, resulting in net cash of ¥57.5 billion.
Interest-bearing debt of ¥100.0 billion remains from when we turned D.A.Consortium Inc. (DAC) into a wholly owned subsidiary in 2018. In terms of fundraising, we will make relevant decisions while considering our cash level as well as our investment plans. The financial market is undergoing significant changes, and we understand that we must diversify our fundraising activities as a result.
At the end of fiscal 2020, which was directly before we revised our Medium-Term Business Plan, our balance of net cash amounted to ¥62.8 billion. As I mentioned previously, we expect expenditures to balance out during the period of the Medium-Term Business Plan, even with our strategic expenses and investment in M&A and other areas. To that end, we believe we can maintain financial soundness even if there are slight deviations to our investment plans or the timing of income and expenditures.
In September 2022, we received an A+ credit rating from Rating and Investment Information, Inc. (R&I), demonstrating that rating institutions also believe that the Group is in a stable financial position.

Capital markets have suggested that the Group has too many cross-shareholdings.

A.That is correct; the issue has been brought to our attention.
At the end of fiscal 2021, investment securities came to ¥136.6 billion. Not all of this amount constitutes cross-shareholdings, as it includes shares held in affiliates. However, the total of “special investment shares” disclosed in our fiscal 2021 annual report was ¥80.2 billion. I believe that the reason some investors have this perception is because this amount represents 22% of our equity.
Every year, we review the purpose and economic impact of our cross-shareholdings and gradually sell off those shares that we have no logical reason to retain. We understand that we are in an era in which there are strong demands for companies to reduce their cross-shareholdings, and we therefore intend to sell off our cross-shareholdings in a manner that does not inconvenience the investee companies, thereby enhancing our capital efficiency.
Actually, over the past five years, we have sold off a total of ¥71.7 billion in investment securities, with ¥68.6 billion of this amount constituting sales of investment securities conducted over the past three years. I ask that our investors understand our approach to addressing this matter.

Please tell us your approach to shareholder returns.

A.Our basic policy is to provide stable dividend payments based on our desire to offer long-term economic benefits to our shareholders. Even with the temporary decline in profits amid the COVID-19 pandemic, we have continued to maintain and increase our dividend levels. Although we forecast a decline in net income in fiscal 2022, we intend to leave our dividend levels unchanged. We are also constantly examining buyback of our shares as a means to provide additional returns and enhance capital efficiency.
I talked about how we aim to maintain a balance of net cash as an indicator of financial soundness, but I also feel that, in cases where we believe we have more than a sufficient amount of funds, we need to keep open the option of providing additional shareholder returns, even while taking various risks into consideration. We will examine whether or not we have sufficient funds to do so by, for example, observing the ratio of net cash to equity capital.
That said, we will also seek to flexibly acquire our shares in accordance with changes in the operating environment. We will decide on whether to buy back shares not by looking at the level of cash and time deposits we are maintaining but rather based on a comprehensive consideration of factors such as our financial position, business performance, demand for funding, and trends in our share price.

In closing, from your position as CFO, is there anything you would like to convey to the readers?

A.Investments for future growth are indispensable. To that end, I believe my role as CFO is to help the Group build a financial foundation that underpins the cycle from investment to growth and ensure that this effort leads to the enhancement of corporate value. I ask for the continued support and encouragement of all of stakeholders as we strive toward this goal.



145. PUBLICIS IS PUTTING AI AT ITS CORE TO BECOME THE INDUSTRY’S FIRST INTELLIGENT SYSTEM

SIX YEARS AFTER SHIFTING FROM A HOLDING COMPANY TO A PLATFORM, PUBLICIS IS PUTTING AI AT ITS CORE TO BECOME THE INDUSTRY’S FIRST INTELLIGENT SYSTEM

After significantly outperforming its industry for the fourth year in a row, with organic growth of +6.3% in 2023, Publicis Groupe [Euronext Paris FR0000130577, CAC 40] today set out its strategy to become the industry’s first AI-powered Intelligent System.

Click here to see the hour-long presentation by Arthur Sadoun, global CEO, Carla Serrano, global CSO, Nigel Vaz, CEO Publicis Sapient, Dave Penski, CEO Publicis Media & Sam Levine Archer, Chief Solutions Architect Publicis North America.

Presentation overview:

From a Platform to an Intelligent System Company

In the last 6 years, Publicis has truly become a partner in its clients’ transformation. Through 3 strategic bets – putting data and technology at the center with the acquisition of Sapient and Epsilon, implementing a country model, and building a single operational backbone – it has shifted from a holding company to a platform.

That platform organization has allowed Publicis to outperform the market on both financial and extra-financial KPIs. But it also now uniquely positions the group to fully harness the power of AI, to become an Intelligent System company capable of connecting every data point, from across every expertise, business unit and geography, and putting them into the hands of all of its people.

In short, thanks to the shift to an Intelligent System company, everyone within Publicis will become a data analyst, an engineer, an intelligence partner, with all the information they need at their fingertips to supercharge client growth.  

An ambition that is already a reality

Concretely, Publicis is infusing a layer of AI across its platform organization to connect its enterprise knowledge under one entity: CoreAI.   

The group is building this unifying AI-led foundation in-house and across its full enterprise, thanks to Publicis Sapient’s unrivalled AI expertise and partnerships, which span supporting Nvidia in designing networking chips for AI servers, to developing AI-powered digital consumer journeys across multiple industries.

Sitting at the center of the group, CoreAI unifies all of Publicis’ proprietary data including the leading consumer data across 2.3 billion profiles of people around the world, with trillions of data points about content, media, and business performance, and almost a petabyte of assets on Marcel, all combined with 35 years of business transformation data and coding owned exclusively by Publicis Sapient.

CoreAI makes those trillions of data points shareable and accessible to everyone at Publicis, super powering them across 5 key disciplines: 

Insight: Brilliant strategy, accurate analyses, and business consultant-level intelligence will power all marketing strategy and plans, transforming everyone into intelligence partners who architect client growth.

Media: Media planning, buying and optimization will deliver new levels of accuracy and outcomes positioning clients to win at commerce sooner and faster. 

Creative + Production: Personalized content will finally be realized at scale, with efficiencies and highly relevant, desirable creative.

Software: The best software and digitally enabled products will be brought to market at scale in days and weeks not months.

Operations: Groupe operations and client management systems will be boosted with speed, accuracy and efficiency.

Publicis plans to invest three hundred million euros over the next three years as it becomes a true Intelligent System. For 2024 alone, the group anticipates an investment of one hundred million euros, with 50% on people, focused on upskilling, training and recruitment, and 50% on technology, through licenses, IT software and cloud infrastructure.

The group began engineering CoreAI in the second half of 2023 and plans to iteratively roll out capabilities in the first half of 2024. It will present MVPs at Viva Tech 2024 this upcoming May.   

Arthur Sadoun, CEO & Chairman of Publicis Groupe commented: “Our journey from a holding company to a platform has not been easy, but it definitely paid off, as you can see with our 2023 organic growth outperforming the industry for the fourth year in a row. The platform organization we have built over the last decade, our proprietary data of unmatched breadth and accuracy, and the 45,000 engineers, consultants and data analysts at the heart of our model, uniquely position us to push the boundaries even further by leveraging AI.

Putting CoreAI at the heart of our organization and truly becoming an Intelligent System company will make our people more efficient and more productive. But way more importantly, it will allow everyone to do things tomorrow that no one can do today, guided by the highest ethical standards, and at the service of our clients’ growth.

As we enter our second century, we are confident that all of the efforts we have made to transform and the ongoing investment we are making in our people and technology will allow Publicis to continue to outperform its peers on organic growth, sustain the highest financial KPIs, and lead the change in our industry.” 

About the financial impact of our AI Plan

The investment in AI of 100 million euros in 2024 will be fully funded by internal efficiencies. It will have no dilutive impact on Groupe’s Operating margin in 2024 and it will be slightly accretive on Operating margin in 2025.

About 2023 Preliminary Full Year and Fourth Quarter Net Revenue (non audited)

Publicis Groupe today pre-releases its Fourth Quarter and Full Year 2023 net revenue. Full details of the 2023 Audited Annual Results will be published on 8 February 2024, before the market opens.

Full year organic growth came in at +6.3%, above the +5.5% to +6% guidance range last upgraded in October.

This included a stronger than anticipated finish to the year, with +5.7% organic growth in the fourth quarter. 

Media, one third of net revenue, grew by double digits organically on the year, accelerating in Q4 supported by a faster ramp up in new business. Data & tech activities, another third, were very solid on the year while seeing contrasting trends. Epsilon recorded circa +10% organic growth in 2023, further accelerating in H2 with double-digit growth in Q4, led by the rise in demand for first-party data. As anticipated, Publicis Sapient saw ongoing delays in digital business transformation projects, like all comparable IT consulting firms, posting +3% organic growth on the year despite a modest decline in Q4. Creative was again very resilient in both the full year and Q4, with low single-digit organic growth.

On a regional basis, the strength of our model was visible in all geographies in both the full year and in Q4. 

The U.S., 60% of revenue, delivered a remarkable +5.0% organic growth for the year, accelerating to +6.1% in Q4. Europe organically grew at +10.3% in 2023, with Q4 at +4.3% despite high comparables, notably in the UK. Asia posted +2.9% organic growth on the year, accelerating to a very solid +4.0% in Q4 led by China returning to growth. Middle East and Africa grew organically by +12.4% in the full year and +9.7% in Q4, and Latin America +8.9% in the full year and +13.9% in Q4.

Disclaimer 

2023 numbers presented today are preliminary and non audited. Certain information contained in this document, other than historical information, may constitute forward-looking statements or unaudited financial forecasts.  These forward-looking statements and forecasts are subject to risks and uncertainties that could cause actual results to differ materially from those projected. These forward-looking statements and forecasts are presented at the date of this document and, other than as required by applicable law, Publicis Groupe does not assume any obligation to update them to reflect new information or events or for any other reason. Publicis Groupe urges you to carefully consider the risk factors that may affect its business, as set out in the Universal Registration Document filed with the French Autorité des Marchés Financiers (AMF) and which is available on the website of Publicis Groupe (www.publicisgroupe.com), including an unfavorable economic climate, a highly  competitive industry, risks associated with the confidentiality of personal data, the Groupe’s business dependence on its management and employees, risks associated with mergers and acquisitions, risks of IT system failures and cybercrime, the possibility that our clients could seek to terminate their contracts with us on short notice, risks associated with the reorganization of the Groupe, risks of litigation, governmental, legal and arbitration proceedings, risks associated with the Groupe’s financial rating and exposure to liquidity risks.



146.  AI makes its mark at Havas group

The ways in which artificial intelligence will fuel agency growth are coming into sharper focus.

Havas Media Group is a prime example. AI, and the many derivatives of it, are already being deployed across the majority (over 90%) of the group.

For instance, machine learning tools are currently being used to catch anomalous budget inputs within social campaigns to flag them for second-level approval.

So if a campaign is extended by a month, but the extra budget has been erroneously applied to the final two days of the current month, the tool can flag this error and prevent a month’s worth of budget being spent over a two-day period. 

“It’s sort of biddable operations,” said Mike Bregman, chief data officer at Havas Media Group. “We constantly have to think about the billions of dollars that we have in our treasury at any one time so the idea of using AI to help with that bookkeeping is key. We’re able to use AI to help keep track of all of that at a real scale.”

Elsewhere, machine learning is also being used to recommend how much to spend on a given day based on a week or month level budget, as well as the seasonality and past spend patterns for that specific brand. 

Both examples speak to the biggest benefit the agency has reaped from AI to date: speed and time savings — hours of human work reduced to minutes or seconds. 

Take the aforementioned case of catching anomalous spending decisions, for example. Not only did it generate upwards of one million dollars a year in savings, according to Havas Media Group, it also freed up hours — time execs at the agency could spend working with clients or with one another.  

“The agency model has to be futureproofed and in order to do that there are some fundamental changes to the way organizations are designed, the way they operate and the technology that helps them do that, which need to be addressed,” said Bregman. “AI has the ability to unlock all of that.”

Creative — or rather the way it is optmized — is another process AI has permeated through at Havas Media Group. Nowadays, any dynamic creative optimization-driven campaign deployed in and around the purchase funnel by the agency has AI in the background. 

“These AI platforms can come up with different combinations of messages in a few seconds when it would take a copywriter a few days,” said Bregman.

Copywriters should breathe easy. Havas Media Group isn’t planning to replace them outright. Rather, it’s using AI to handle certain tasks. 

As Bregman explained: “it’s still smart to have a human that’s reading all the iterations, because some of them might just be ridiculous or outside the norms.”

This is just the start. 

Over the next two to three years, Havas Media Group wants AI more deeply embedded into the fabric of the agency. So not just using AI to optimize all media buys but also to create custom algorithms within a programmatic bidder or using the technology to identify the right training modules for a planner as well as making manual tasks like filling out timesheets.

Some of these efforts are already well underway, like for lower funnel performance optimization. Havas Media Group’s execs are using AI to power custom algorithms in the demand-side platforms — or ad tech used to place programmatic bids — to get 25%+ improvements in the cost per acquisition charged when the ads it buys lead to a conversion.

The application of AI is expanding to broader metrics like attention, where it’s used to model consumer interactions with ads — which is important as the industry moves toward attention-based planning. 

Even from this limited vantage point it’s easy to see where this nets out in the short-to-medium term: campaigns planned and bought by AI. The tools are there already, it’s just whether marketers are willing to trust the technology with their money. The optimization, on the other hand — the bit that comes after the ad has been bought and the client wants to sweat those dollars harder — remains a work in progress. 

Bregman expanded on the point: “The idea of funding the fluidity [of campaign optimization] that comes as a result of AI is something that not a lot of advertisers are comfortable with.”

Eventually, however, this stance will change — whether marketers are comfortable with it or not. The emergence of AI solutions from Google (Performance Max) and Facebook (Advantage+) make that all too clear. Both solutions can be whittled down to this: advertisers share their data with the tech, upload the creative assets, set the financial parameters (think daily budget cap, price per conversion etc.) and sit back and wait for it to report back aggregate results. 

It makes sense then the marketers are taking a trust but verify approach to many of these solutions. 

“For advertisers, they see us a bit like the United Nations when it comes to being able to look at all the different options on the table, test out the use cases of them and then figure out how to get the most out of them,” said Bregman. 

Fulfilling a role like this is tricky to say the least. Not only are the likes of Havas Media Group having to stay on top of thinking about what AI means for their own businesses, they’re having to help marketers do the same thing. And all while making sure they avoid AI becoming another existential crisis. No wonder the group is trying to gather up as much knowledge on the matter as possible. It’s not just building out its own already substantial team of data scientists, it’s also working with academia, startups and platforms too. Oh, and there’s the cost.

“We’re still trying to figure out the numerator denominator of the tech ROI equation, because none of this is cheap,” said Bregman. “Data scientists aren’t cheap and the ones we have here are very busy these days.”

It’s akin to being in an arms race, in many ways. 

Whether it’s Havas Media Group or another agency, they’re all trying to rewire their businesses around AI. The challenge is figuring out how fast they should do it. Move too fast, and the impact the technology has on the structure and subsequent operations of a holding group like Havas Media Group could prove to be too disruptive in a very transitory period. Go too slow, though, and the business could struggle to keep pace with counterparts that are more efficient and adaptive thanks to AI.

“I would love to find a way to make this pivot quickly but with tens of thousands of people to think about across our holding company, it’s going to be a journey,” said Bregman. “Agencies function through really good handoffs because there are so many different teams. The idea of having data at the core would fundamentally change how that works.”

That’s a big concept for any business to get its head around — let alone one where the data team still only accounts for a quarter of the group. That expertise either needs to find ways to permeate further and faster into the business or it needs to grow. Either way, the costs are steep. But they’re also necessary. AI is nothing but a data hog, after all. Indeed, the technologies thrive on ingesting large amounts of data.

“There are no AI-driven agencies today,” said Bregman. “I’m trying as much as I can to push us toward that direction so that we can be an agency of the future but we’re not quite there yet.”

Still, Havas Media Group has time to stick the landing on AI. Senior marketers haven’t even got to the point where they know what they don’t know — the preservation of agencies — when it comes to AI. AI hasn’t even come up in pitches, said Bregman. Eventually, it will, and when it does he wants to be able to respond accordingly.

“AI inputs should be viewed as a creative partner that can be leveraged to frame angels from a messaging perspective,” said Marc Hardgrove, CEO of search agency The HOTH. “But it takes a little practice to hone in which inputs work best for your team. At this point, there shouldn’t be a question as to whether to use AI– that horse is out of the gate and frankly, we all have been using AI tools for years, even before it was the big buzzword.”



147. AI resource guide

AI in Construction—What Does It Mean for Our Contractors?

Artificial intelligence is revolutionizing the construction industry by enhancing efficiency, safety and decision-making throughout the project lifecycle. AI in construction involves the application of advanced technologies like machine learning, computer vision and data analytics to various construction processes. Through AI, machines can learn and imitate human cognitive functions.

The importance of AI technology in construction should not be underestimated. It can help companies complete projects on time, minimize staffing challenges, save money and address safety concerns. AI learns from the data provided to it. It can adjust project plans based on the information it receives, allowing decision-makers to alter those plans or change them to improve safety or minimize inefficiencies.

It can enhance productivity, reduce costs, improve safety and promote sustainable practices, making it a vital tool for the industry’s future growth and development. The possibilities may sound endless, but as an industry traditionally looking from the outside in at technology, we must first step back to educate ourselves on the basics. This resource is meant to act as a starting point in your journey to understand AI and its potential impact on the construction industry. By reading through definitions, construction use cases and considerations, the reader should walk away with a level of knowledge to ensure they can actively participate in future conversations on AI in construction.

Artificial Intelligence

Per The National Artificial Intelligence Initiative Act of 2020: “A machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations or decisions influencing real or virtual environments.”

Machine Learning

Application of AI that allows a system to automatically learn and improve from experience. In other words, machine learning helps computers do tasks like recognizing colors, finding pictures of cats on the internet or even suggesting what to watch on TV. It’s like teaching the computer to be smart and make decisions by looking at lots of examples and learning from them. One common example of this are the Large Language Models.

Deep Learning

Per IBM: “Deep learning is a subset of machine learning, which is essentially a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to ‘learn’ from substantial amounts of data. While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy.” Deep learning has achieved remarkable success in various applications, including self-driving cars, medical diagnosis, recommendation systems and more. Its power lies in its ability to automatically learn and adapt to new data, making it a cutting-edge technology in the field of AI and data analysis.

Generative AI

A type of AI that can creates new data or content, such as images, text, music or even videos, by learning patterns and structures from existing examples. It works by understanding and mimicking the patterns and styles it has seen in the data it was trained on. The most publicly recognized tool in the last year is ChatGPT, built by the company OpenAI. ChatGPT is an artificial intelligence chatbot that can process natural human language and generate a response. It has revolutionized how we interact with computer systems and has influenced the evolution of AI. Additional generative AI tools from other major technology companies include: Meta’s Llama 2, Microsoft’s Copilot, Google’s PaLM 2, Amazon’s Bedrock and Dall-E 2, also from OpenAI.

Predictive AI

A type of AI that uses data and machine learning algorithms to forecast future events or trends. It helps businesses and organizations make informed decisions by analyzing historical data, identifying patterns and making predictions based on those patterns.

Project Lifecycle Impacts/Examples





Preconstruction

Predictive Analytics: Analyze historical project data and current conditions to optimize construction schedules, resource allocation and task sequencing.

Alice Technologies—End users can harness the power of artificial intelligence to enhance construction planning and scheduling abilities to keep jobsite labor moving

Optimized Design Development: Allow project stakeholders to identify the best design for a building based on real-world data; Rapidly create and explore a variety of unique design options.

Hypar’s artificial intelligence function lets you describe a building and turn your text into a quantifiable building model

Augmenta—A fully automated building design platform in the cloud, built from the ground up around generative AI. It creates highly cost-, labor-, time- and energy-efficient designs that are fully code compliant, error-free and constructible

Construction Drawings:
Blueprints AI—An advanced artificial intelligence-powered tool designed specifically for the construction industry to automate the takeoff process. This tool significantly reduces the time and effort required for construction estimators to quantify materials and costs from blueprints and plans

Stack—STACK Assist, its Artificial Intelligence functionality automating takeoff tools for contractors, will allow contractors to use measurements specific to the trades they need, and AI will perform takeoff and counts automatically.

Togal.AI-—After uploading construction drawings, state-of-the-art artificial intelligence modeling will complete as much of the takeoff as possible

Supply Chain: Throughout the procurement process for self-performing contractors, artificial intelligence will empower the purchasing team to quickly identify availability and best pricing within a certain region.

SubBase—Effortlessly streamline invoice reconciliation through a centralized inbox, utilizing AI for automated logs, cost code confirmation and a custom-digitized approval workflow

Kojo—simplifies the complex task of material sourcing, saving time, reducing costs, and enhancing project outcomes. Using state-of-the-art technologies like OpenAI's GPT-4 and Hugging Face transformer models, along with comprehensive statistical and machine learning methods, the Kojo Intelligence Layer helps contractors efficiently find project-appropriate materials at optimal prices

Contract Review: Empower legal teams to quickly identify critical risk factors in construction contracts


Construction

Autonomous Equipment:
AIM—Enable existing equipment to run at full utilization every day of the year, in any weather, without an operator and with 360-degree safety technology preventing any accidents

Project Management:
SmartBuild—At the core, Smartbuild’s intuitive and easy system has the necessary process and performance timelines to manage projects for success. In partnering with Microsoft, Smartbuild users gain access to seamless operations and informed decision making through the Azure AI platform

Procore Copilot—Artificial intelligence-powered conversational and predictive experience that will provide customers the ability to automate time-intensive, manual processes across the Procore platform

Autodesk—Construction IQ delivers automated risk analysis of quality and safety data from Autodesk Construction Cloud to help projects advance faster and with less risk. AutoSpecs in ACC is an automated submittal process that helps users generate submittal logs in minutes and leverages construction IQ to suggest potentially missing items

Computer Vision/Intelligent Site Monitoring: Increase safety and security on jobsites. Through machine learning, video footage is trained to detect things like the number of workers entering/exiting the jobsite, workers in proximity of heavy construction machinery and even safety violations, such as the lack of face protection while saw-cutting concrete

Safety:
Dozer—Computer vision for jobsite equipment; With a 360-degree view, the cameras can see the cabin, bucket and everything in between. Proprietary artificial intelligence models calculate a depth map of the surrounding area and constantly monitor various elements of your jobsite

Labor Tracking:
AlwaysAI—Computer vision enables existing cameras to immediately identify and interpret objects in the physical world; Manage direct labor and material costs more efficiently and provide a safer and more secure working environment by leveraging existing camera infrastructures with practical AI for construction

Jobsite Mapping:
DroneDeploy—Mapping application that uses artificial intelligence to process images; DroneDeploy uses machine learning to decode images and find patterns that are invisible to the human eye





Building Maintenance

Energy Management: Analyze energy usage patterns and optimize HVAC systems to reduce energy consumption and overall costs

Predictive Maintenance: Through the expanded use of building automation and control networks, AI can predict when building equipment is likely to fail, allowing for a proactive response

HR Office Considerations





Per insights from Littler professionals, the following are things to consider when drafting inter office AI policies. Please also note that ‘insights’ do not constitute legal advice.

As construction technology continues to be a driving voice in the industry, there are also discrete factors that need to be considered, particularly around the use of artificial intelligence within our work environments.

Some of the examples listed in the prior sections, such as the use of generative AI, may be more commonly embraced quicker than others in the construction industry. HR professionals will need to consider asking questions on the use of AI in the office. Before laying out a blanket policy, clearly define the purpose of the AI usage policy, which may include what AI technologies are covered and how it applies to employees and/or outside stakeholders.

An AI usage policy should include a purpose or mission statement, an AI definition section, an explanation of who the policy applies to and a policy that allows for open use or prohibits or limits AI use. Designate certain point people to oversee AI usage, to troubleshoot problems if they arise and who can approve of AI use. Clarify a policy that instructs employees that programs like ChatGPT still makes a lot of mistakes and that these programs should be used to assist employees and not serve as a substitute.

Training and awareness are key to ensure employees are well-informed about the AI usage policy and how it impacts their roles and responsibilities. Consider training managers on AI use. As is the case for most technologies, human interaction is still an important factor. Consider an overall approach that monitors AI use and encourages innovation, but ensures that AI is only used to augment internal work and with proper data.

Conclusion





Artificial intelligence has been in the background of some of our everyday technologies, but in the last year has come closer to the surface thanks to strategic marketing and perhaps a more consumer-friendly approach. There are still a lot of unknowns on what the impact will be and what the technology could look like in the next few years.

First, we need to consider how AI will increase productivity and eliminate many of the manual entry tasks that bog down our days. We can obtain the ability to augment the search for knowledge and completing tasks.

Secondly, we should continue to push closer and closer towards becoming a digital workforce. Artificial intelligence and the many layers involved in its functions rely heavily on clean and consistent data. This leans into the transformation of our workforce: Who is managing this data? How are we managing the data? How are we using the data in an effective manner?

The existing gap between academia and skilled trades is closing in, which offers the industry immense opportunity to continue to evaluate how we deliver projects on time, within budget and safely. As Director of Product Management at Autodesk Pat Keaney puts it, “AI undoubtedly has wide-reaching implications for the construction industry. In construction, how you build should be as rewarding as what you build, and implementing AI into the construction process will help the industry improve the quality of construction jobs and make workers safer and more productive”.



148. Domino's (DPZ) Boosts AI Capabilities With Microsoft Partnership

Domino's Pizza, Inc. DPZ recently announced a collaboration with Microsoft Corporation MSFT to generate AI solutions. The initiative will likely enhance the pizza ordering process and simplify store logistics. The company intends to pilot the AI-powered solutions within the next six months.

Per the agreement, Domino's and Microsoft will establish an Innovation Lab, pairing both companies' executives with engineers to accelerate the time-to-market for smart store and ordering solutions. The company will leverage the Microsoft Cloud and Azure OpenAI Service to boost loyalty and engagement for customers, franchisees and employees.

Meanwhile, the company stated that it has progressed in modernizing store systems and is in the early stages of developing an Azure OpenAI Service-powered generative AI assistant. The solution is meant to assist store managers with daily activities like staff scheduling, ingredient ordering, and inventory management. Also, it emphasized streamlining pizza preparation and quality controls with more predictive tools.

As consumer preferences rapidly evolve, generative AI has become a game-changer for fulfilling new needs and improving the customer experience. Nevertheless, the company is optimistic concerning the strategic partnership and anticipates the initiative to drive growth in the upcoming periods.

Emphasis on Digital initiatives

Domino’s invests heavily in technology-driven initiatives like digital ordering to boost sales. The company continues to innovate across all aspects of its business — including GPS, e-bikes, AI in-store technology, great food and an evolving digital experience.

In June 2023, the company rolled out a new Pinpoint Delivery service in the U.S. market. The concept is based on providing food deliveries to locations based on PIN and without a standard address.

During the first quarter of fiscal 2023, the company initiated the rollout of electric vehicles for pizza delivery. Apart from this, enhanced make-line and cut-table technology and AI-enabled forecasting are being rolled out to better match demand with capacity. The initiatives will likely enhance the speed, accuracy and efficiency of services in the future.

In the past year, shares of the company have gained 10.2% compared with the industry’s 4.2% growth.

Domino’s currently carries a Zacks Rank #3 (Hold).

Some better-ranked stocks in the Zacks Retail-Wholesale sector include:

Arcos Dorados Holdings Inc. ARCO sports a Zacks Rank #1 (Strong Buy). It has a trailing four-quarter earnings surprise of 35%, on average. The stock has gained 14% in the past year. You can see the complete list of today’s Zacks Rank #1 stocks here.

The Zacks Consensus Estimate for Arcos Dorados’ 2023 sales and EPS suggests rises of 19.2% and 13%, respectively, from the year-ago period’s levels.

El Pollo Loco Holdings, Inc. LOCO currently carries a Zacks Rank #2 (Buy). It has a trailing four-quarter earnings surprise of 23.7%, on average. Shares of LOCO have dropped 5.6% in the past year.

The Zacks Consensus Estimate for LOCO’s 2024 sales and EPS indicates a 3.5% and an 18.3% growth, respectively, from the year-ago period’s levels.

149.  Bad News? Send an AI. Good News? Send a Human

Abstract

The present research demonstrates how consumer responses to negative and positive offers are influenced by whether the administering marketing agent is an artificial intelligence (AI) or a human. In the case of a product or service offer that is worse than expected, consumers respond better when dealing with an AI agent in the form of increased purchase likelihood and satisfaction. In contrast, for an offer that is better than expected, consumers respond more positively to a human agent. The authors demonstrate that AI agents, compared with human agents, are perceived to have weaker intentions when administering offers, which accounts for this effect. That is, consumers infer that AI agents lack selfish intentions in the case of an offer that favors the agent and lack benevolent intentions in the case of an offer that favors the customer, thereby dampening the extremity of consumer responses. Moreover, the authors demonstrate a moderating effect, such that marketers may anthropomorphize AI agents to strengthen perceived intentions, providing an avenue to receive due credit from consumers when the agent provides a better offer and mitigate blame when it provides a worse offer. Potential ethical concerns with the use of AI to bypass consumer resistance to negative offers are discussed.

Marketing managers currently find themselves in a period of technological transition, wherein artificial intelligence (AI) agents are increasingly viable replacement options for human representatives in administering product and service offers directly to customers. AI agents have been adopted across a broad range of consumer domains to handle both face-to-face and remote customer transactions (Davenport et al. 2020; Harris, Kimson, and Schwedel 2018; Huang and Rust 2018; Wirtz et al. 2018), ranging from traditional retail and travel (Mende et al. 2019) to ride and residence sharing (Hughes et al. 2019) and even legal and medical services (Esteva et al. 2017; Turner 2016). Given AI agents’ advanced information processing capabilities and labor cost advantages (Kumar et al. 2016), the transition away from human representatives in administering product and service offers seems predominately advantageous for firms. Despite AI’s potential, scant research has examined how consumers evaluate AI systems in relation to equivalent offerings delivered by humans.

The increasingly pervasive use of AI raises the possibility that offers administered by AI agents may impact consumer response in novel ways as compared with offers administered by human agents. For example, Uber uses an AI machine learning system to estimate travel elasticities and administer ride price offers (Newcomer 2017). Imagine a customer who pays for an Uber ride downtown but, for the return trip, unexpectedly receives a price offer triple the original. Does administration of sthis worse-than-expected offer by an AI agent, rather than a human agent, have implications for purchase likelihood, customer satisfaction, or intentions toward the use of Uber in the future? What if the return trip was unexpectedly much cheaper than anticipated, resulting in a much better-than-expected offer for the consumer? The present research theorizes and demonstrates an interaction between the type of agent and outcome expectation discrepancies, such that consumers respond less negatively to worse-than-expected price offers when transmitted by AI agents (compared with human agents) and more positively to better-than-expected price offers when transmitted by human agents (compared with AI agents). In addition, this interaction is further moderated by anthropomorphic characteristics of the AI. The more humanlike the AI is in terms of its appearance or cognitive functions, the more it reduces the expectations discrepancy gap between human and AI agents. This moderator represents a strategic managerial input that can be used to manage customer satisfaction on the basis of the customer’s price expectations.

Our theory and findings have key implications for firms enlisting both human and AI agents who administer outcomes that are discrepant from expectations. These findings are relevant for price offers, in addition to other situations where consumers learn of unexpectedly negative outcomes along dimensions other than price, such as cancellations, delays, negative evaluations, status changes, product defects, rejections, service failures, and stockouts. Our findings are also pertinent to instances where consumers receive unexpectedly positive outcomes such as expedited deliveries, rebates, upgrades, service bundles, exclusive offers, loyalty rewards, and customer promotions. Managers can apply our findings to prioritize (vs. postpone) human-to-AI role transitions in situations where negative (vs. positive) discrepancies are more frequent and impactful. Moreover, our results suggest that even when a role transition is not holistically passed to an AI, the selective recruitment of an AI agent to disclose certain discrepant information can still be advantageous. Firms that have already transitioned to consumer-facing AI agents, including the multitude of online and mobile applications that use AI-based algorithms to create and administer offers, also stand to benefit from our findings. Our research reveals that AI agents should be selectively anthropomorphized (i.e., depicted as either machinelike vs. humanlike) depending on whether an offer will be worse or better than expected.

Our research contributes to the literature in multiple ways. First, we show that AI (vs. human) agents asymmetrically alter the effects of outcome expectation discrepancies on purchase, satisfaction, and reengagement intentions, thereby broadening the associated literature examining human–AI transactions in marketing contexts (e.g., Huang and Rust 2018; Kim and Duhachek 2020; Longoni, Bonezzi, and Morewedge 2019; Mende et al. 2019; Srinivasan and Abi 2021). Whereas the growing literature on technology in marketing has shown that consumer engagement tends to decrease when an AI agent administers a transaction (e.g., through a perceived lack of human mental or emotional attributes on the part of the representative; Longoni, Bonezzi, and Morewedge 2019; see also Dietvorst, Simmons, and Massey 2015), we reveal how transactions with AI can either improve or undermine purchase tendencies and reengagement intentions depending on the valence of deviation from consumer expectations.

Second, whereas previous research has shown largely positive consequences of anthropomorphized products in the form of increased consumer engagement and product liking (Aaker, Vohs, and Mogilner 2010; Aggarwal and McGill 2007; Waytz, Heafner, and Epley 2014), or generally negative compensatory responses (Mende et al. 2019), we show an asymmetric pattern such that consumers transacting with anthropomorphized AIs are more likely to react negatively to worse-than-expected marketing offers but respond more favorably to better-than-expected offers from anthropomorphized AIs owing to the role of perceived intentions. Third, we provide evidence that our effects are not driven by potential alternative explanations, including uncanny valley theory (Mori, MacDorman, and Kageki 2012) or AIs’ superior market tracking ability. We note that prior research exploring the anthropomorphism of AI agents has argued for an uncanny valley effect that leads people to avoid agents that appear too humanlike, ostensibly due to eeriness that emerges after a certain inflection point (Kim, Schmitt, and Thalmann 2019; Mende et al. 2019; Mori, MacDorman, and Kageki 2012). Our research demonstrates AI anthropomorphism patterns that are ostensibly incompatible with an uncanniness explanation and that emerge even when empirically controlling for uncanniness perceptions. Instead, we reveal how AI (vs. human) agents are inferred to have weaker selfish and benevolent intentions when developing discrepant offers, with implications for subsequent consumer response. In doing so, we suggest a new and important process driving both aversion to and engagement with anthropomorphized AI agents.

As a fourth contribution, our research reveals that the use of AI agents to administer offers can simultaneously present ethical dilemmas alongside opportunities for marketing firms. On the one hand, our work reveals opportunities for firms to receive due (and perhaps otherwise overlooked) credit for offers that are better than expected, with corresponding improvements in customer response. On the other hand, our work also reveals the possibility of AI misuse as a darker tool through which marketers can increase the acceptance of offers that fall short of expectations while simultaneously increasing intentions to reengage with the offending firm.

Theoretical Framework

Expectations Discrepancy Theory in Marketing Transactions

We examine the impact of AI versus human product and service offer administration through the theoretical lens of discrepant outcome expectations—that is, consumer reactions to offers that are better or worse than expected on a key dimension. Although expectation discrepancies have been examined in a variety of interpersonal sales transaction contexts (e.g., Darke, Ashworth, and Main 2010; Evangelidis and Van Osselaer 2018; Oliver, Balakrishnan, and Barry 1994), to our knowledge, no studies have investigated the implications of AI agents as transaction administrators. Although no research has specifically examined this phenomenon, extant research into robotics and artificial intelligence suggests that AI (vs. human) agents could have either positive or negative effects when administering price offers that are better or worse than expected. We next consider the extant literature on expectancy theory, followed by a conceptual integration with recent AI research.

The broad body of research that examines expectation discrepancies for product and service offers assumes that a set of expectations exists prior to entering a consumption event, and offers trigger a comparison against expected referents (Evangelidis and Van Osselaer 2018; Oliver and DeSarbo 1988). For example, a buyer entering into a potential Uber transaction could have expectations for a price of $20 based on historical experience but then receive a ride price offer of $30 (worse than expected) or $10 (better than expected). Worse-than-expected offers elicit an adverse reaction and have been demonstrated to lead to lower purchase likelihood, satisfaction, and desire to reengage with specific sales agents, whereas better-than-expected offers produce favorable consumer responses (Oliver, Balakrishnan, and Barry 1994; Spreng, MacKenzie, and Olshavsky 1996). These expectations can relate to many aspects of service and product experiences, including temporal dimensions such as product/service delivery time, attribute performance, price, and so on. Any dimension of consumer experience for which outcomes deviate from experience can potentially be used to form evaluations of the agent.

The current research aims to conjoin the extant literature on technology and expectations discrepancy theory by highlighting the important role of AI. The previous literature on expectations discrepancy has assumed a human agent during offer administration. AI is perceived differently than human agents in many aspects because it is an artificially created technology (Dietvorst, Simmons, and Massey 2015; Longoni, Bonezzi, and Morewedge 2019; Mende et al. 2019). Moreover, AI technology endowed with machinelike versus humanlike features (e.g., a name or face) appears to trigger different perceptions about its capabilities (MacDorman, Vasudevan, and Ho 2009; Waytz, Heafner, and Epley 2014), thereby potentially altering responses to offers that are discrepant from expectations.

Transactions with AI Versus Human Agents: Inferred Intentions Matter

Previous research has shown that human agents are more liked and trusted than AI agents across a variety of contexts (Dietvorst, Simmons, and Massey 2015; Longoni, Bonezzi, and Morewedge 2019). For example, consumers are more likely to prefer a medical service provided by a human versus an AI agent because AI agents are considered incapable of considering each patient's unique, individual, and situational characteristics (Longoni, Bonezzi, and Morewedge 2019). Thus, one could predict that the administration of an offer, such as offering a price for an Uber trip, by an AI (vs. a human) agent could negatively influence purchase, satisfaction, and reengagement outcomes regardless of how well the offer matches expectations. Moreover, extant research has shown that people have less aversion to inflicting physical punishment on an embodied AI (i.e., a robot) than a human (Złotowski et al. 2015), suggesting in particular that a worse-than-expected offer elicited by an AI could be met with a more adverse customer response. However, extant research has not considered transactional contexts that vary in the valence of outcome expectation discrepancy, for which we theorize that a different pattern of effects will emerge. We extend previous research revealing an AI aversion phenomenon in the contexts of prediction tasks and medical decisions (Dietvorst, Simmons, and Massey 2015; Longoni, Bonezzi, and Morewedge 2019). In doing so, we contrast AI and human agents in the dimension of intentions inferred from their actions and theorize that transactional offers with the same face value could be evaluated as more acceptable when provided by an AI (vs. a human) agent, depending on the valence of the offer.

According to established models of intentionality, an action is perceived to be driven by intentions when it involves three key elements: a desired outcome based on one's own motives, a belief that the action will lead to that outcome, and one's autonomous decision to take the action (Brand 1984; Bratman 1987; Malle, Moses, and Baldwin 2001). Research has shown that individuals are capable of inferring intentions from others’ actions, and an action that suffices for all three elements is perceived to have stronger intentions. A growing body of research suggests that AI (vs. human) agents are considered to have a lower capacity for self-motivated decision making and, by extension, lack the capacity to form their own intentions that drive subsequent actions. Because an AI agent is a nonhuman machine made by humans to serve humans (Russell and Norvig 2010), lay beliefs have formed such that AI agents’ actions serve human goals and fulfill extrinsic human desires (Huang and Chen 2019; Kim and Duhachek 2020; Russell and Norvig 2010). Put differently using intentionality theory, AI agents lack the primary element of intentionality, namely self-motivated “desired outcomes,” and thus, they do not qualify as agents that possess their own intentions (Brand 1984; Bratman 1987; Malle, Moses, and Baldwin 2001).

Social information processing theory has revealed that when approaching an interpersonal engagement with meaningful outcomes—such as a marketing transaction—individuals attempt to judge the other party primarily on inferred intentions (Abele and Wojciszke 2007; Wojciszke, Abele, and Baryla 2009). Of particular importance to determining response is whether the intentions driving the other party's decision are inferred to be selfish (i.e., placing the self first) or benevolent (i.e., considerate of others; Wojciszke, Abele, and Baryla 2009). Inferred selfish (benevolent) intentions decrease (increase) favorable evaluation of an agent's actions. For example, imagine that a human driver of a vehicle swerved into a wall to protect a child who jumped into the street chasing a ball. The benevolence of the action, if conducted by a human driver, would be praised, but the same action would receive less praise when conducted by an autonomous vehicle's AI system (Awad et al. 2018; Gray, Gray, and Wegner 2007; Gray, Young, and Waytz 2012). This explanation is also consistent with research demonstrating weaker perceptions of goodwill from benevolent actions conducted by humans (e.g., corporate social responsibility, prosocial behaviors) when those actions are not self-motivated (Barasch et al. 2014; Bolton and Mattila 2015; Huang and Chen 2019; Du, Bhattacharya, and Sen 2007; Rand, Newman, and Wurzbacher 2015). Moreover, inferences that a human agent's better-than-expected offer is driven by benevolent intentions tends to improve the recipient's evaluation of that offer (Hilbig et al. 2015; Radke, Güroğlu, and De Bruijn 2012).

Conversely, we propose that a worse-than-expected decision driven by ostensibly selfish intentions (e.g., hitting the child instead of swerving into a wall) would be criticized more when conducted by a human (vs. an AI) agent. Such judgment is echoed across multiple contexts indicating that intentional violations by humans toward others are perceived to be more egregious than unintentional violations (e.g., in sports, criminal behavior, and resource management; Gray 2012; Güroğlu, Van den Bos, and Crone 2009). For example, electric shocks are less painful when administered unintentionally compared with when administered with a malicious intent (Gray 2012). Intentions are also codified in criminal justice systems, such that acts committed while lacking intent of malice or selfishness are often met with milder punishment and individuals often receive less blame for their actions. A similar pattern emerges in marketing contexts for human agents that administer worse-than-expected offers, which are evaluated more negatively if inferred intentions are selfish, but less so if unintentional (Tsiros, Mittal, and Ross 2004). Extending this to our present research, and based on the premise that AI lacks the capacity for intentions, we propose that consumers will be less likely to infer benevolent intentions (in the case of a better-than-expected offer) and selfish intentions (in the case of a worse-than-expected offer) for the same offer transaction administered by an AI agent versus a human agent.

We theorize that this difference in inferred intentions will impact consumer purchase responses such as purchase likelihood and satisfaction. In the case of a worse-than-expected offer (vs. a better-than-expected offer), we propose that a human agent's intentions will be inferred as driven more by selfish intentions (vs. benevolent intentions), which, in turn, reduces (vs. improves) the likelihood of offer acceptance and subsequent customer satisfaction.

Stated formally,

H1: The effect of a price offer that is worse (better) than expected in lowering (raising) purchase likelihood and satisfaction is weakened when administered by an AI agent versus a human agent.

H2: The effect proposed in H1 is mediated by the inferred intentions of the offering agent. Specifically, in the case of a better-than-expected (vs. worse-than-expected) price offer, greater inferred benevolent (vs. selfish) intentions improve (vs. undermine) offer response. Both mediating intention pathways are attenuated when the offer is administered by an AI agent versus a human agent.1

Empirical Overview

We evaluate our theory over a series of five studies. In Studies 1a and 1b, we examine our basic effect for worse-than-expected (Study 1a) and better-than-expected (Study 1b) offers to test H1. Study 2 tests our full model, including underlying process mechanisms, thereby testing H1 and H2. Studies 3a and 3b introduce a third hypothesis on the moderating effect of anthropomorphizing AI agents.

Study 1a: Response to a Worse-Than-Expected Offer from an AI Agent Versus a Human Agent

The primary objective of Study 1a was to test our proposition that acceptance of a worse-than-expected offer will be higher when administered by an AI agent versus a human agent. This study utilized a product resale context (resale of an aftermarket concert ticket) in which participants imagined receiving a price offer that was administered by either an AI agent or a human agent.

Method

Participants and design

The experiment was a 2 (agent type: human, AI) × 2 (offer type: worse than expected, expected) group, between-subjects design. A total of 174 undergraduate students (Mage = 21.2 years; 56% female) participated in this study in return for course credit. We referred to previous AI research with similar experimental designs and determined our sample size based on the effect sizes and samples sizes reported in these studies (Longoni, Bonezzi, and Morewedge 2019; Mende et al. 2019). The sample size for this and subsequent studies was determined to provide sufficient power to detect a medium size effect if it exists (Bausell and Li 2002). In addition, sample size was influenced by the size of the participant pool made available to the authors in the given semester.

Procedure

Participants were first instructed to provide their favorite musician whom they would most like to see in concert. Next, participants read a scenario in which they were asked to imagine that all the tickets for an upcoming concert of their favorite musician were sold out and the only way to attend the concert was to buy a resale ticket through an online ticket resale website. Participants in the human (AI) agent condition were told that the ticket was being sold by “another person” (“an artificial intelligence”). All participants were told that the agent was providing a price offer of $140. Offer type was manipulated by telling the participants that the ticket was sold either at the same or a lower price to a different customer. Participants in the worse-than-expected (expected) offer condition were told that a similar ticket was recently sold for $110 ($140) to someone else. This approach of disclosing a lower sale price for the product to a different consumer is consistent with long-standing practice for eliciting a negative expectation discrepancy (e.g., Fisk and Young 1985). The offer type manipulation was validated with a pretest that showed a significant difference in price expectations between the two conditions (the pretest results are presented in the next section; further details on the stimuli and pretest are available in Web Appendix B). Then, participants indicated whether they would purchase the ticket (yes/no). Finally, participants responded to background questions (e.g., gender).

Results

Pretest

We conducted a pretest of offer type to assess our operationalizations of worse than expected (ticket sold to someone else for $110; participants were offered $140) and expected (ticket sold to someone else for $140; participants were offered $140). One hundred fifty respondents from Amazon Mechanical Turk (MTurk) completed the pretest in return for monetary compensation. After reading the ticket scenario without agent descriptions, respondents answered the two-item expectancy disconfirmation scale from Oliver (1980) adapted to this context: “Think about what your expectations were for the price offer. How does the price that you were offered compare to your expectations?” (1 = “I expected a much lower price,” 4 = “Price was as I expected,” and 7 = “I expected a much higher price”; 1 = “Much worse offer than I expected,” 4 = “Offer was as I expected,” 7 = “Much better offer than I expected”). Analysis of the two-item composite (r = .80) revealed that the worse-than-expected offer condition was significantly below both the “as expected” midpoint (i.e., 4) (Mworse = 2.90, SD = 1.36; t(75) = −7.06; p < .001) and the expected condition (Mexpected = 3.98, SD = .96; F(1, 148) = 31.52, p < .001, 𝜂p2 = .18), indicating a successful manipulation.

Offer acceptance

First, offer type (0 = worse, 1 = expected), agent type (0 = AI, 1 = human), and offer acceptance (1 = yes, 0 = no) were contrast coded and submitted to a binomial logistic regression. We observed a significant main effect of agent (χ2 = 7.92, p = .005) and no main effect of offer type (χ2 = .40, p = .53), both subsumed by a significant two-way interaction (χ2 = 6.92, p = .009). Further chi-square analysis revealed that the worse-than-expected offer was accepted more in the AI condition (49%) than in the human condition (19%; χ2 = 8.39, p = .004), thus confirming H1. In the expected offer condition, there was no effect of agent on offer acceptance (χ2 = .60, p = .44; see Figure 1).



Figure 1. Offer Acceptance Rate as a Function of Offer and Agent Type (Study 1a).

Notes: Error bars =  ±1 SEs.

OPEN IN VIEWER

Discussion

Consistent with our theory and H1, the results of Study 1a indicate that consumers are more likely to accept a worse-than-expected product offer from an AI agent versus a human agent. This result is evidence that consumers respond differently to AI and human agents with respect to price expectation discrepancies stemming from marketing transactions. Our theory proposes that a better-than-expected offer will also provide a different pattern of results for offers administered by an AI agent versus a human agent. In Study 1b, we examine whether better-than-expected offers elicit a different pattern of response when administered by AI versus a human agent.

Study 1b: Response to a Better-Than-Expected Offer from an AI Agent Versus Human Agent

Study 1b addresses the case of a better-than-expected offer. We posit that offer administration by a human leads to more positive consumer outcomes as compared with AI.

Method

Participants and design

Two hundred ninety-nine MTurk participants (Mage = 41.9 years; female = 53%) completed this study for monetary compensation. All participants were randomly assigned to one of the conditions in a 2 (agent type: AI, human) × 2 (offer type: expected, better than expected) between-subjects design.

Procedure

The procedure was a ticket resale context adapted from Study 1a, with the exception that the worse-than-expected offer was replaced with a better-than-expected offer (for stimuli and pretest details, see Web Appendix C). Specifically, subjects in the better-than-expected condition were told that a similar ticket was sold recently for $170 and were offered a price of $140. Subjects in the expected condition were told that the ticket recently sold for $140 and were offered a price of $140. After receiving the offer, participants indicated whether they would purchase the ticket (yes/no), then responded to background questions (e.g., gender).

Results

Pretests

As in Study 1a, we conducted a separate pretest to validate our offer type manipulation with 150 MTurk participants who received monetary compensation. Using the same two items from Study 1a, we measured the extent to which participants perceived that the offer was better than they expected. Analysis of the two-item composite (r = .72) revealed that the mean score of the composite was higher in the better-than-expected offer condition (Mbetter = 5.83, SD = 1.25) when compared with the expected offer condition (Mexpected = 4.18, SD = 1.00; F(1, 148) = 80.8, p < .001, 𝜂p2 = .35), indicating a successful manipulation.

Offer acceptance

First, offer type (0 = expected, 1 = better), agent type (0 = AI, 1 = human), and offer acceptance (1 = yes, 0 = no) were contrast coded and submitted to a binomial logistic regression. We observed a significant main effect of agent (χ2 = 5.94, p = .02) but no significant main effect of offer type (χ2 = .99, p = .32), both subsumed by a significant two-way interaction (χ2 = 4.79, p = .03). Further chi-square analysis revealed that the better-than-expected offer was accepted more in the human condition (89%) than in the AI condition (76%; χ2 = 6.30, p = .01), thus confirming H1. In the expected offer condition, there was no effect of agent on offer acceptance (χ2 = .17, p = .69; see Figure 2). This pattern supports H1.



Figure 2. Acceptance of Better-Than-Expected Offers by Agent Type (Study 1b).

Notes: Error bars =  ±1 SEs.

OPEN IN VIEWER

Discussion

Study 1b demonstrated that consumer response to better-than-expected offers are systematically influenced by the administrating agent. Consistent with our theory, we found that the same better-than-expected offer elicits a more positive evaluation when administered by a human instead of an AI agent. Coupled with the results of Study 1a, these findings provide direct support for H1.

Study 2: Inferred AI Intentions Alter Offer Acceptance

Study 2 served multiple objectives. First, in this study we build on Studies 1a and 1b to test our full model by simultaneously considering both worse-than-expected and better-than-expected offers. Second, we assess our proposed process model, thereby testing H2. Specifically, we investigate whether the intentions of an AI (vs. a human) agent are inferred to be less selfish (in the case of a worse-than-expected offer) and less benevolent (in the case of a better-than-expected offer), which in turn influences offer evaluation. Study 2 also tests for several alternative explanations, including uncanniness. Study 2 employs a ride-sharing service context to build on the results of Study 1b.

Method

Participants and design

All participants were randomly assigned in a 2 (agent type: human, AI) × 3 (offer type: worse than expected, expected, better than expected) between-subjects design. Six-hundred ninety-eight undergraduate business students (Mage = 21.2 years; female = 59%) completed this study for partial course credit.

Procedure

Participants first read an introduction, which explained that they would participate in a study containing a scenario related to Uber, a ride-sharing service. First, participants read a short passage explaining that the price offered at Uber is determined by an Uber agent, with the description of that agent manipulated between conditions. We adapted agent type descriptions from previous research on AI and service robots (Kim and Duhachek 2020; Mende et al. 2019). Participants were told, “This is your Uber agent who will personally determine the offer price of your Uber rides, named [Alex/XT-1000].” In the human condition, an image of a human was presented, adapted from Mende et al. (2019), whereas in the AI condition an image of a robot pretested to be moderately humanlike was presented (stimuli details in Web Appendix D; Development procedures for AI stimuli in Web Appendix G).

Next, participants read about and evaluated a price offer from the Uber agent. The scenario explained that the initial cost of a ride from home to a restaurant for dinner was $20, and for the return trip home they were later offered the price of $20 in the expected condition, $30 in the worse-than-expected condition, and $10 in the better-than-expected condition. As the dependent measure, we asked participants their likelihood of accepting the offer (1 = “not at all likely,” and 7 = “very likely”).

To assess process, we collected measures of inferred agent intentions when administering the offer. The measurement items for the selfish and benevolent intentions were developed based on previous studies of valenced intentionality (Gray 2012; Gray and Wegner 2008). Specifically, we collected three items measuring inferred benevolent intentions (“To what extent do you agree with the following statements about Uber agent's own, personally created intention when forming your individual ride price offer? Uber agent had their own …” “benevolent intentions,” “generous intentions,” “good intentions” [1 = “strongly disagree,” and 7 = strongly agree”]; the three items were averaged to create an index of benevolent intentions, α = .84) and three items measuring inferred selfish intentions (“Uber agent had their own …” “selfish intentions,” “greedy intentions,” “bad intentions” [1 = “Strongly disagree,” and 7 = “Strongly agree”] averaged to create an index of selfish intentions, α = .93).

We also recorded measures for alternative processes, including uncanniness (“I feel uneasy,” “I feel unnerved,” “I feel creeped out”), whether respondents may have perceived the original price to the restaurant to be overpriced (“Do you feel that you overpaid for the first Uber trip that you took to the restaurant?”), whether respondents perceived different price tracking capacity between the two agents (“To what extent do you think the Uber agent was able to track the real-time price changes of other people's rides and made an offer to you based on this market information?”), and whether respondents perceived that the offer is controlled by someone else (“To what extent do you think that the Uber agent's price offer was controlled by another person behind the scene?”). Finally, participants responded to background questions (e.g., gender).

Results

Pretests

Just as in the previous studies, we conducted separate pretests to validate our manipulations of agent type and offer type (for details, see Web Appendix D). The pretest of offer type assessed our operationalizations of a worse-than-expected offer ($30 price offer with a $20 precedent), better-than-expected offer ($10 price offer with a $20 precedent), and the expected price offer ($20 price offer with a $20 precedent). Three hundred respondents from MTurk completed the pretest in return for monetary compensation. Respondents answered the same two-item expectation discrepancy scale used in previous studies. Analysis of the two-item composite (r = .81) revealed the main effect of offer type (F(2, 297) = 82.85, p < .001, 𝜂p2 = .36). Additional contrasts revealed that the mean score of the composite was significantly higher in the better-than-expected offer condition (Mbetter = 5.19, SD = 1.47) compared with the expected offer condition (Mexpected = 3.99, SD = .94; F(2, 297) = 46.05, p < .001, 𝜂p2 = .19), and the mean score of the composite in the expected offer was higher than the worse-than-expected offer condition (Mworse = 2.84, SD = 1.40; F(2, 297) = 46.27, p < .001, 𝜂p2 = .19). These results indicate a successful manipulation of offer type.

Offer acceptance likelihood

A 2 (agent type: human, AI) × 3 (offer type: worse than expected, expected, better than expected) analysis of variance (ANOVA) revealed a significant main effect of offer type (F(2, 692) = 150.61, p < .001, 𝜂p2 = .30) and no main effect of agent type (F(2, 692) = .06, p = .80, 𝜂p2 < .01), both subsumed by a significant two-way interaction (F(2, 692) = 7.18, p = .001, 𝜂p2 = .02). A series of planned contrasts revealed that acceptance of the expected offer was not significantly different between agent types (Mhuman = 3.69, SD = 1.99; MAI = 3.40, SD = 1.75; F(2, 692) = 1.71, p = .19, 𝜂p2 = .006). In contrast, and as we predicted, acceptance of the better-than-expected offer was significantly higher in the human (vs. AI) condition (Mhuman = 5.58, SD = 1.19; MAI = 5.23, SD = 1.62; F(2, 692) = 4.28, p = .04, 𝜂p2 = .02). Also as predicted, acceptance of the worse-than-expected offer was significantly higher in the AI (vs. human) condition (Mhuman = 2.53, SD = 1.59; MAI = 3.17, SD = 1.90; F(2, 692) = 8.46, p = .004, 𝜂p2 = .03) (see Figure 3).

Figure 3. Offer Acceptance Likelihood as a Function of Offer and Agent Type (Study 2).

Notes: Error bars =  ±1 SEs.

OPEN IN VIEWER

Moderated mediation via intentions

To assess the underlying process, we first evaluated selfish and benevolent intentions via ANOVA. The 2 (agent type: human, AI) × 3 (offer type: worse than expected, expected, better than expected) ANOVA of selfish intentions revealed a significant main effect of offer type (F(2, 692) = 18.39, p < .001, 𝜂p2 = .05) and no main effect of agent type (F(2, 692) = 1.95, p = .16, 𝜂p2 = .003), and the predicted two-way interaction matched the results for offer acceptance (F(2, 692) = 7.60, p = .001, 𝜂p2 = .02; for the profile plot and pairwise comparisons, see Web Appendix D). The 2 × 3 ANOVA of the benevolent intentions measure revealed a significant main effect of offer type (F(2, 692) = 20.53, p < .001, 𝜂p2 = .06) and a marginally significant main effect of agent type (F(2, 692) = 3.64, p = .057, 𝜂p2 = .01), both subsumed by the predicted two-way interaction (F(2, 692) = 9.93, p < .001, 𝜂p2 = .03; for profile plots and pairwise comparisons, see Web Appendix D, p. 9).

To assess mediation via intentions, we conducted a moderated mediation analysis using a Hayes (2017) PROCESS model (Model 7) in which offer type (worse than expected, expected, better than expected) served as the multicategorical independent variable, inferred intentions (i.e., selfish and benevolent intention perceptions) served as the two mediating variables, offer acceptance likelihood served as the dependent variable, and agent type (human, AI) served as the moderating variable. This analysis revealed that the index of moderated mediation for both selfish and benevolent intentions did not include 0 (indirect effect for selfish intention = .12; 95% confidence interval: [.05, .21]; indirect effect for benevolent intention = .14; 95% confidence interval: [.07, .23]), indicating that the offer type → inferred intentions → acceptance likelihood path is moderated by agent type.

Alternative mechanisms

The first alternative mechanism assessed was the uncanniness index (three-item average, α = .91). An ANOVA of the uncanniness index revealed that AI was perceived as more uncanny than a human only in the expected offer condition (MAI = 2.98, SD = 1.29; Mhuman = 2.50, SD = 1.35; F(2, 692) = 6.96, p = .01). More germane to our inquiry, the two-way interaction between the agent and offer type was not significant, resulting in a pattern inconsistent with explaining our results. The second alternative mechanism assessed was whether the perception the original fare (i.e., $20 to get to the restaurant) was overpriced. An ANOVA of the overpricing perception revealed a significant main effect of offer type (Mworse = 4.14, SD = 1.70; Mexpected = 5.10, SD = 1.53; Mbetter = 5.52, SD = 1.39; F(2, 692) = 49.00, p < .001). However, there was no interaction between the agent and the offer (F(2, 692) = 2.19, p = .11), precluding it as a driver of the observed pattern of mediation. Third, we assessed the agent's market-price-tracking capability as an alternative mechanism. An ANOVA of price tracking capability revealed a significant main effect of agent (F(2, 692) = 22.90, p < .001) and offer type (F(2, 692) = 3.38, p = .04). However, there was no interaction between the agent and the offer (F(2, 692) = .16, p = .86), resulting in a pattern inconsistent with explaining our results. Finally, ANOVA of the perception that the offer is controlled by another person revealed only a significant main effect of agent (F(2, 692) = 6.03, p = .01). The main effect of offer (F(2, 692) = 2.03, p = .13) and the interaction (F(2, 692) = 1.60, p = .20) were not significant, also resulting in a pattern inconsistent with explaining our results. Taken together, these results rule out multiple alternative explanations and lend further confidence to our conceptual model.

Discussion

The results of Study 2 extend our understanding of how offers that are discrepant from expectations systematically elicit different responses between human and AI agents. We replicate the differential human versus AI effects observed in Studies 1a and 1b, supporting H1. Specifically, worse-than-expected offers are more likely to be accepted when administered by an AI, whereas better-than-expected offers are more likely to be accepted when administered by a human. Moreover, our results reveal that inferred intentions mediate the observed effects, in support of H2. Specifically, a worse-than-expected (better-than-expected) offer from a human increases (decreases) inferred selfish intentions and decreases (increases) inferred benevolent intentions, which in turn decreases (increases) offer acceptance. However, an AI agent attenuates these pathways, thereby leading to an acceptance increase in the case of a worse-than-expected offer, but an acceptance decrease in the case of a better-than-expected offer, compared with a human agent. In addition, this study rules out several potential alternative process mechanisms.

Studies 3a and 3b: Anthropomorphism of AI Agents

In our final two studies, Studies 3a and 3b, we extend our work beyond the human–AI dichotomy to consider different forms of AI agents. We propose that AI agents may be anthropomorphized through their presentation to be perceived as more humanlike (vs. machinelike), thereby eliciting patterns more consistent with human agents. In doing so, we provide insights for managers on how to best depict their AI agents (i.e., as more humanlike vs. more machinelike) in situations where consumers will receive better- or worse-than-expected offers.

Research has revealed that endowing an AI with humanlike features typically makes people perceive the AI more positively. For example, consumers assessing an AI agent that controlled an autonomous vehicle became more trusting as that agent was presented as increasingly humanlike, rather than machinelike (Waytz, Heafner, and Epley 2014). Indeed, the marketing literature has shown that anthropomorphized products are predominately liked more and improve consumer engagement. For example, thinking about a product (e.g., a car) through an anthropomorphic perspective increases affection toward that product and leads to less willingness to replace it (Chandler and Schwarz 2010). Research has also shown that a product that resembles a human face in its design could increase perceived friendliness, leading to a positive evaluation of the product (Landwehr, McGill, and Herrmann 2011). As another instance, socially excluded consumers prefer anthropomorphized brands due to their need to establish relationships (Chen, Peng, and Levy 2017).

However, prior research investigating anthropomorphic AI has predominately focused on nonpurchase situations in which AI agents have been in a supporting role to consumers, such as providing safe transportation or providing medical advice. Our present inquiry builds on previous research by examining AI agents in a more adversarial role as administrators of product and service offers—that is, the consumer is sitting across the transaction table from the AI agent. In such transactional contexts, we propose that AI anthropomorphism will not have the generally favorable effect observed in many other contexts but will instead systematically either improve or worsen the impact of offers that differ from expectations.

The influence of AI anthropomorphism on response to discrepant offers will be driven by changes in the extent to which AI is perceived to be capable of intentionality (i.e., as defined by the previously discussed three elements of intentionality models; Brand 1984; Bratman 1987; Malle, Moses, and Baldwin 2001), thereby altering the strength of inferred AI intentions. As an AI agent's humanlikeness increases, so too should its perceived capacity for intentionality. Indeed, a mind capable of intentionality has been argued to be a defining aspect of what it means to be uniquely human, and items specifically measuring related constructs are embedded in multiple established anthropomorphism scales (e.g., Kim and McGill 2011; Waytz, Cacioppo, and Epley 2010). Extant research has revealed that individuals are surprisingly willing to attribute advanced humanlike capabilities to anthropomorphized AI agents (Złotowski et al. 2015), potentially due in part to an overestimation of AI technical advancement stemming from fictional literature and film (Pollack 2006) and to the nature of human inference that assumes similar internal properties between entities that share common external appearances (Rozin and Nemeroff 2002). As such, AI agents that are viewed as increasingly anthropomorphic should be perceived to possess a greater capacity for intentionality, thereby generating stronger inferred intentions that increasingly elicit a response akin to a human agent.

We thus posit that anthropomorphizing an AI agent through the depiction of humanlike (vs. machinelike) properties will asymmetrically influence responses to discrepant offers. In the case of a better-than-expected offer, an anthropomorphized AI agent will lead to more positive downstream consequences (e.g., offer acceptance, satisfaction). Conversely, we hypothesize that the response to a worse-than-expected offer will become more negative when the AI agent is endowed with humanlike (vs. machinelike) properties. Stated formally,

H3: Anthropomorphism of an AI agent moderates the response to price offers that are better or worse than expected: a humanlike AI agent makes worse-than-expected offer responses more negative but makes better-than-expected offer responses more positive.

We test H3 in Studies 3a and 3b. In doing so, we provide valuable insights to managers who are faced with decisions regarding customer interactions with AI agents. If H3 is supported, our results suggest that selective anthropomorphism of AI agents could be applied to improve overall customer response to both worse- and better-than-expected offers.

Study 3a: The Moderating Role of Consumer Trait Anthropomorphism

In Study 3a, we extend our findings to consider the implications of anthropomorphism of AI agents for worse-than-expected offers. We do this utilizing an ecologically valid and managerially relevant segmentation measure: technology anthropomorphism (Waytz, Cacioppo, and Epley 2010). Technology anthropomorphism is the individual tendency to view technological products as possessing humanlike mental and emotional qualities. Consistent with our theory and H1, we predict that consumers will be more likely to accept a worse-than-expected offer from an AI than a human. Moreover, in concordance with the role of anthropomorphism specified in H3, this effect will be even stronger for individuals who perceive technological objects to be less humanlike, whereas the effect will attenuate for consumers who view technological objects as more humanlike. We also use a different context to test the robustness of our theory: an ultimatum game setting that examines the likelihood to accept both expected and unexpected allocations (Morewedge 2009; Thaler 1988). Prior research has demonstrated that the ultimatum game is a highly controllable context in which human offers may be contrasted with offers proposed by a nonhuman agent (e.g., Sanfey et al. 2003). We advance beyond previous research by examining the role of anthropomorphism of AI agents in the ultimatum context. Moreover, prior research has demonstrated that individuals have predictable and strong expectations regarding offer terms (Suleiman 1996), making this a context to which our theoretical predictions regarding worse-than-expected offers are particularly applicable.

Method

Participants and design

The experiment was a 2 (agent type: human, AI) × 2 (offer type: worse than expected, expected) between-subjects design. A total of 403 members of an MTurk online panel (Mage = 34.9 years; 64% female) participated in this study in return for financial compensation.

Procedure

Participants first completed the full Individual Differences in Anthropomorphism Questionnaire (IDAQ) scale for trait anthropomorphism of nonhuman agents (Waytz, Cacioppo, and Epley 2010), which includes a subscale specific to technological devices. The five-item technological anthropomorphism scale measures the extent to which technology is viewed as having humanlike mental and emotional qualities (e.g., “To what extent does the average computer have a mind of its own?,” “To what extent does a car have free will?”; 0 = “not at all,” and 10 = “very much”; for the full list of IDAQ items, see Web Appendix E). We averaged the five items to form an index of technology anthropomorphism (α = .85). Then, participants were invited to participate in an ostensibly unrelated study on decision making. Participants were introduced to an ultimatum game in which the other player was either a “human” or an “artificially intelligent machine” who would administer an offer determining how $100 should be allocated. Previous research in behavioral economics has shown that an approximately even split (50%/50%) is both the modal offer by allocators and expectation by receivers, with splits even modestly outside of this range falling short of expectations and leading to an increasing tendency to reject the offer (Suleiman 1996). Thus, participants in the worse-than-expected offer condition received an offer of $10 to themselves and $90 to the other player, whereas participants in the expected offer condition received a $50/$50 offer. Participants then indicated their acceptance or rejection of the offer (1 = yes, 0 = no). Finally, participants responded to background questions (e.g., gender).

Results

Pretest

We conducted a pretest of offer type to assess our operationalizations of worse than expected ($10/$90) and expected ($50/$50). One hundred fifty respondents from MTurk completed the pretest in return for monetary compensation. After reading the ultimatum scenario, respondents answered an adaptation of the two-item expectations discrepancy scale as in the pretests for Studies 1a, 1b, and 2 (for pretest stimulus details, see Web Appendix E). Analysis of the two-item composite (r = .92) revealed that the worse-than-expected condition was significantly below the “as expected” midpoint (Mworse = 1.68, SD = 1.21; t(72) = −16.36; p < .001) and the expected condition (Mexpected = 4.10, SD = .59; F(1, 148) = 245.89, p < .001, 𝜂p2 = .18), in support of a valid manipulation.

Offer acceptance

A binomial regression of offer acceptance (1 = yes, 0 = no) as a function of agent type (0 = human, 1 = AI) and offer type (0 = expected, 1 = worse than expected) revealed a main effect of agent type (χ2 = 6.65, p = .01) and main effect of offer type (χ2 = 73.69, p < .001), both subsumed by a significant two-way interaction (χ2 = 4.00, p = .045). In the case of an expected offer (i.e., an expected $50/$50 split), the offer was invariably accepted in both the AI (100%) and the human (100%) conditions, consistent with extant research in the ultimatum context. More germane to our theory, we conducted a chi-square analysis to compare the acceptance of worse-than-expected offers in the human and the AI conditions. As we predicted, acceptance of the worse-than-expected offer was significantly higher in the AI condition (78.6%) than in the human condition (60.4%; χ2 = 7.73, p = .005). These results of the purely human versus AI comparison were consistent with our theory and previous studies.

Moderation by trait anthropomorphism

We next tested how individual consumer tendencies to anthropomorphize AI (i.e., perceive AI as more human) altered offer acceptance. Consistent with H3, we expected that individuals who view AI as not possessing humanlike characteristics would be more likely to accept a worse-than-expected offer from an AI, whereas those who do attribute humanlike characteristics would be less likely to accept a worse-than-expected offer. For expected offers, anthropomorphism index and agent type necessarily had no effect on offer acceptance, as expected offers were invariantly accepted at 100%. For worse-than-expected offers, we assessed the interaction between anthropomorphism index (Manthro = 2.01, SD = 1.57) and agent type (human = 0, AI = 1) in determining acceptance through Johnson–Neyman (JN) analysis. Analysis revealed no main effect of anthropomorphism (Z = 1.14; p = .26) and a significant main effect of agent (Z = 3.37; p = .001), both subsumed by a significant interaction between the agent and anthropomorphism (Z = −2.09; p = .037). The result showed that for individuals with an anthropomorphism score below the JN point of 2.59—constituting 79.4% of the sample—acceptance of worse-than-expected offers was significantly higher from an AI versus a human. However, as anthropomorphism increased to the JN point of 2.59 and above—constituting 20.6% of the sample—no significant difference between agent type emerged (i.e., p > .05). No other regions of significance emerged at any higher levels of anthropomorphism (for JN graph and supplemental table illustrating interactions, see Web Appendix E, p. 14). That is, the positive effect of an AI (vs. human) agent on accepting a worse-than-expected offer emerges for individuals who are less likely to imbue the AI with humanlikeness, but does not hold for those individuals who increasingly attribute humanlike qualities to technology. These results support H3.

Discussion

The results of Study 3a provide further evidence that worse-than-expected offers are more likely to be accepted when administered by an AI (vs. a human) agent, in support of H1. Moreover, in support of H3, individual perceptions of AI anthropomorphism alter this pattern of response; decreasing technology anthropomorphism (i.e., the AI is seen as less humanlike) increases the likelihood of accepting a worse-than-expected offer from an AI agent, whereas higher levels of technology anthropomorphism (i.e., the AI is seen as more humanlike) result in responses similar to those elicited by an actual human.

These results have several important implications. First, our findings provide managers with a new, easily measurable variable for segmentation when attempting to understand and predict how consumers will interact with their AI systems—individual tendency toward technology anthropomorphism. Second, our results suggest that anthropomorphism of the AI through means other than innate consumer tendencies, such as physical embodiment or descriptions of the AI mind, could impact response to unexpected offers. Indeed, the results of Study 3a reveal that most individuals tend to view technology as less than humanlike (e.g., the scale mean for technology anthropomorphism was substantially and significantly below the midpoint; p < .001), suggesting that the default view held by most consumers is that technology does not possess humanlike characteristics. In the next study, we examine how AI anthropomorphism can be manipulated, rather than measured, by marketers to lead to a differential acceptance of discrepant offers administered by AI.

Study 3b: Anthropomorphism, Satisfaction, and Reengagement

Study 3b expands on previous findings to examine how customer satisfaction and desire to maintain a relationship with the offering marketing agent varies depending on the level of humanlikeness of the AI agent administering a discrepant offer. Customer satisfaction is critical for firms (Anderson, Engledow, and Becker 1979; Williams and Naumann 2011), and evidence revealing how AI anthropomorphism alters the impact of (dis)satisfaction stemming from better- or worse-than-expected offers would provide valuable insights for managers of AI agents. Regarding relationship maintenance, building a rapport between consumers and AI agents has become an important issue for companies utilizing AI agents as a primary contact point, with important implications for customer retention and relationship management (Huang and Rust 2018; see also Mende et al. 2019). As such, in this study, we provide participants a choice to maintain (vs. replace) the relationship with the current agent following a worse-than-expected offer. Consistent with our theory, we predicted that the effect of a better-than-expected offer on the tendency to maintain versus replace the relationship would be stronger for a humanlike (vs. machinelike) AI agent and that the opposite pattern would result in the case of a worse-than-expected offer. To assess this prediction, we selected a novel pair of AI agents differing only in their perceived humanlikeness (i.e., no differences in perceived uncanniness), based on a comprehensive study of multiple robots drawn from the ABOT (http://abotdatabase.info) database (development of AI stimuli detailed in Web Appendix G).

Method

Four hundred MTurk participants (Mage = 36.7 years, female = 48%) completed this study for monetary compensation. All participants were randomly assigned to one of the conditions in a 2 (agent type: machinelike AI, humanlike AI) × 2 (offer type: better than expected, worse than expected) between-subjects design. The overall procedure was similar to Study 2, which utilized an Uber ride-sharing context and explored customer satisfaction as the dependent variable. Participants read a short passage explaining that the price offered by Uber was determined by an AI agent (an initial $20 trip followed by a return trip price offer of $30 in the worse-than-expected condition and $10 in the better-than-expected condition). The images used to depict the two AI agents were drawn from the ABOT database (http://abotdatabase.info) and pretested regarding their humanlikeness and uncanniness (pretest reported in the study “Results” subsection, additional details in Web Appendices F and G).

Participants then answered three items related to offer satisfaction (identical measures to Study 1b; “satisfied,” “appreciative,” and “grateful” regarding the offer; 1 = “not at all,” 7 = “very much"; the three items were averaged to create the index of satisfaction, α = .93). We next asked participants the extent to which they wanted to interact with the same Uber agent in the future, which served as a measure of willingness to reengage with the agent. Participants were given an opportunity to stay with the current Uber agent in future interactions or discontinue the relationship with the current Uber agent and replace it with a different agent in a binary choice paradigm. The replacement agent stimulus was adopted from previous consumer research on AI (Mende et al. 2019; for the exact replacement choice stimuli, see Web Appendix F). Finally, participants answered background questions (e.g., gender).

Results

Pretests

Separate pretests supported the validity of our offer type and agent type manipulations. The pretest of agent type assessed the extent of perceived anthropomorphism (i.e., humanlikeness) for each agent. Using the same three-item humanlikeness (α = .86) and three-item uncanniness (α = .89) measures from our previous studies, a pretest of 100 MTurk respondents revealed that the humanlike (vs. machinelike) agent was perceived to be higher in humanlikeness (Mmachinelike = 1.65, SD = .83; Mhumanlike = 2.47, SD = 1.07; F(1, 98) = 18.05; p < .001) but not in uncanniness perceptions (Mmachinelike = 1.76, SD = 1.13; Mhumanlike = 2.07, SD = 1.51; F(1, 98) = 1.35; p = .25; additional pretest details and offer type pretest available in Web Appendix F).

Offer satisfaction

The satisfaction index was submitted to a 2 (agent type: machinelike AI, humanlike AI) × 2 (offer type: better than expected, worse than expected) ANOVA. The main effect of offer type was significant (F(1, 396) = 301.19, p < .001, 𝜂p2 = .43) and the main effect of agent was not significant (F(1, 396) = .03, p = .87, 𝜂p2 < .01). Consistent with our theory, we also found a significant interaction between agent and offer type (F(1, 396) = 10.14, p = .002, 𝜂p2 = .03). In the better-than-expected offer condition, satisfaction with the offer was higher in the humanlike agent condition (M = 5.79, SD = 1.31) than in the machinelike agent condition (M = 5.29, SD = 1.21; F(1, 396) = 5.62, p = .02, 𝜂p2 = .02). As predicted, we found the opposite pattern in the worse-than-expected offer condition. Satisfaction was higher in the machinelike agent condition (M = 3.15, SD = 1.84) than in the humanlike agent condition (M = 2.69, SD = 1.59; F(1, 396) = 4.55, p = .03, 𝜂p2 = .02, see Figure 4). This pattern is consistent with our theory, providing direct support for H3.



Figure 4. Offer Satisfaction as a Function of Offer and AI Agent Type (Study 3b).

Notes: Error bars =  ±1 SEs.

OPEN IN VIEWER

Choice of agents

Offer type (0 = worse than expected, 1 = better than expected), agent (0 = machinelike, 1 = humanlike), and agent replacement decision (1 = replace, 0 = remain) were contrast coded and submitted to a binomial logistic regression. We found a significant main effect of offer type (χ2 = 10.34, p = .001) and a significant main effect of agent (χ2 = 9.25, p = .002), both subsumed by a significant interaction between agent and offer type (χ2 = 12.41, p < .001). Further chi-square directional tests were conducted to assess the simple effects of agent type at each level of offer type. A worse-than-expected offer led to a significantly higher proportion of replacement decisions in the humanlike agent condition (83%) than in the machinelike agent condition (63%) (one-sided test, p = .001; two-sided test, p = .002). In the better-than-expected offer condition, the proportion choosing replacement in the humanlike agent condition (28%) was lower than the machinelike agent condition (40%) (one-sided test, p = .04; two-sided test, p = .07). This pattern is consistent with our theory and demonstrates that humanlike AI is more (vs. less) likely to result in a continuation of the marketing relationship in the case of an offer that is better (vs. worse) than expected. Thus, we demonstrate that utilization of an appropriate AI agent leads to stronger relationships with that agent, which is a critical factor influencing customer retention and relationship management.

Discussion

Study 3b demonstrated that consumer response to better than expected and worse-than-expected offers are systematically influenced by the administrating AI agent. Consistent with our theory, we found that a better-than-expected offer elicits greater satisfaction and reengagement when administered by a humanlike (vs. a machinelike) agent. These results are particularly important for companies utilizing AI as direct points of customer contact, with implications for the application and depiction of AI agents.

General Discussion

The present research investigates how consumer responses to offers that are discrepant from expectations depend on whether the administering marketing agent is AI or a human. Our theoretical framework proposes that in the case of worse-than-expected offers, consumers respond more positively when dealing with an AI agent versus a human or more humanlike AI agent, in the form of increased purchase likelihood, satisfaction, and reengagement. In contrast, for better-than-expected offers, consumers respond more positively to a human or more humanlike AI (vs. machinelike AI) agent. Moreover, we propose that this pattern emerges because AI (vs. humans) is inferred to have weaker benevolent and selfish intentions, which in turn influences offer responses.

A series of five studies provides support for our theory across product and service contexts. In each study, an expectation-discrepant offer administered by an AI agent elicits a different response relative to an identical offer administered by a human or more humanlike AI agent. Study 1a demonstrates in a product purchase context that consumers are more likely to accept worse-than-expected offers from an AI agent than a human agent. Study 1b examines the other side of our effect and demonstrates that consumers are more likely to accept better-than-expected offers from a human agent than an AI agent. Study 2 reveals that the effect is driven by stronger inferred intentions on the part of human versus AI agents; that is, stronger benevolent intentions in the case of a better-than-expected offer and stronger selfish intentions in the case of a worse-than-expected offer. These intentions, in turn, alter consumer response. Study 3a reveals that an anthropomorphized (vs. a machinelike) AI agent leads to a lower acceptance of worse-than-expected offers. Finally, Study 3b examines how the physical embodiment of AI (i.e., a robot form) influences anthropomorphism, with implications for responses to discrepant offers. Administration of a better-than-expected offer by a humanlike (vs. machinelike) AI results in higher satisfaction and reengagement intentions. Together, these findings contribute to the literature examining human–AI interactions and technology in marketing, in particular those areas examining preference, service, satisfaction, and anthropomorphism, while identifying important implications for consumers and marketers.

Broad Implications of Differential Responses to AI Versus Human Agents

Our works reveal that fundamental differences exist between the interpretation of negative and positive outcomes administered by AI versus human agents. The underlying mechanisms of perceived intentionality apply to a broad range of interactions beyond just the price discrepancies in our present work. Given this, other marketing and nonmarketing contexts that deal with the administration of negative and positive outcomes will likely demonstrate similar patterns of results as those proposed by H1–H3. For example, consumers are commonly faced with unexpected delays or cancellations (e.g., flights, lodging stays, package deliveries), personal evaluations that differ from expectations (e.g., work performance, credit scores), discrepant financial offers (e.g., loan interest rates, credit line amounts), and unexpected school admissions and workplace hiring outcomes, in addition to a broad range of other contexts in which AI plays an increasingly common and customer-facing role. Future research could also examine if the perceived fairness of an offer is affected by the type of agent. For example, the positive or negative response to unexpected offers (e.g., an unexpectedly low or high loan interest rate) may polarize more extremely when administered by a human (vs. AI) (for initial evidence in support of this prediction, see the supplemental analysis of Study 2 in Web Appendix D). Future research could further examine the role of perceived fairness in explaining how perceived intentions influence offer acceptance.

When and How Marketing Managers Should Use Human Agents Versus AI Agents

Two particularly important marketing implications stem directly from the present research, both rooted in the ongoing transition within firms from human agents to AI agents as marketing representatives. First, our findings give insight into how this transition can be systematically and selectively managed to improve customer purchase tendencies, satisfaction, and reengagement. Indeed, whereas extant works regarding customer-facing AI have revealed primarily main effects of human-to-AI transitions on consumer engagement—that is, either humans or AI are generally superior in a given product or service domain (e.g., Longoni, Bonezzi and Morewedge 2019; Mende et al. 2019)—our work reveals that the nature of the transaction (i.e., offers that are discrepant from expectations) is important. In other words, our findings inform when and where the transition should occur in product and service offer situations involving discrepant expectations. Managers could apply our findings to prioritize (vs. postpone) human-to-AI role transitions in situations where better-than-expected (vs. worse-than-expected) offers are more frequent and impactful. In addition, our results suggest that even when a role transition is not holistically passed to an AI, the selective recruitment of an AI agent to disclose worse-than-expected offers should still be advantageous—that is, our results reveal that an AI “bad cop”/human “good cop” approach to managing discrepant expectations should have beneficial outcomes for the firm. Moreover, our research builds on brand crisis research that explores consumer response to algorithmic errors (Srinivasan and Abi 2021); however, our work reveals differences in perceived intentions, outside the context of algorithmic errors, related to whether an offer is administered by a human or AI.

Second, and perhaps even more impactful, our work has implications for firms that have already transitioned to consumer-facing AI representatives, including the multitude of online and mobile applications that use AI-based algorithms to create and administer offers (e.g., Myer's “6-second sale”; Green 2017). In such posttransition instances where AI is already customer facing, our research reveals that AI may be depicted or otherwise framed as either machinelike or humanlike. Our manipulations show that this can be accomplished through differences in the embodied appearance of the AI (e.g., a robotic form shown in Study 3b). Moreover, the results of Study 3a suggest that the majority of contemporary consumers consider AI to be inherently machinelike (though Study 3a reveals that a minority of consumers do already consider AI to be moderately humanlike) unless humanlike imagery or verbiage is introduced. Thus, marketers should anticipate a default “machinelike” response toward AI agents unless specifically humanlike attributes are depicted. With this understanding, marketers should depict an AI agent as machinelike when disclosing a worse-than-expected offer but should depict more humanlike attributes in the case of a better-than-expected offer.

Ethical Implications

The aforementioned recommendations carry ethical implications, as our findings reveal potential opportunities and concerns for customer–firm relationships and consumer well-being. On the one hand, our work reveals opportunities for firms to receive due, and perhaps otherwise overlooked, credit for better-than-expected offers and potentially other behaviors that are ostensibly benevolent. On the other hand, regarding threats to consumer well-being, our work does reveal a tool through which marketers can increase the acceptance of worse-than-expected offers made to a customer, while also increasing intentions to reengage with the offending firm. In those instances where the worse-than-expected offer is objectively detrimental to consumers, the use of this approach does raise ethical concerns. By documenting these effects, we intend to benefit consumers and policy makers through strengthening their understanding of the nature of differential consumer responses to discrepant outcomes.

Importantly, we propose that this ethical dilemma predominately emerges in situations where consumers are ostensibly harmed—that is, where natural consumer resistance to unexpectedly high prices is bypassed by the selective use of AI. However, some situations do exist where increased consumer purchase would not result in objective consumer harm, and there are even select contexts where consumers and firms could jointly benefit from this effect. For example, consumers often hold unrealistic expectations for product and service offers due to self-serving biases or lack of information. For example, a customer could anchor on a historic promotional price (Lin and Chen 2017) or observe that another customer received a lower price offer without knowledge of that other customer’s greater earned loyalty status (Mayser and Wangenheim 2013). Such misperceptions or biased interpretations could lead consumers to irrationally abandon transactions that are objectively beneficial. In such cases, consumers and firms would benefit from shifting consumer tendencies toward preserving the marketing relationship, thereby minimizing switching costs and maximizing consumer utility. Thus, the current research indicates ways managers can strengthen their customer relationships through AI agents when these findings are applied ethically.

The results of our current studies reveal that most consumers currently tend to see AI as not inherently possessed of benevolent or selfish intentions when administering offers. However, there is the possibility that some consumers infer that humans within companies are manipulating AI offers behind the scenes or otherwise have programmed the AI to indirectly carry out selfish human intentions. In such instances, the observed increase in acceptance for worse-than-expected offers from AI agents could be attenuated. Exploring whether certain segments of consumers hold such skeptical beliefs, or whether similar beliefs may increase over time as AI administration of offers becomes even more common, is a fruitful avenue for future research.

Anthropomorphism Theory

Our findings also contribute to the literature on anthropomorphism in product marketing and technology. Our research reveals that anthropomorphism of AI agents—that is, making AI agents more humanlike—can actually lead to decreased preference and increased disengagement. This is in contrast to previous research, which has shown predominately positive consequences of anthropomorphized products in the form of increased consumer engagement and product liking (Aggarwal and McGill 2007). Whereas extant research suggests that humanlike AI is more comforting and trusted (Longoni, Bonezzi, and Morewedge 2019; Waytz, Heafner, and Epley 2014), our findings reveal situations where humanlike AI may serve as a liability for maintaining the marketing relationship, particularly in contexts of discrepant expectations. Future research should explore other psycholinguistic and personality psychology factors incorporated into AI agent design (e.g., altering the gender or perceived cultural origins of the AI) and resulting changes in consumer perception and reactions.

Beyond contextual effects due to AI design elements, our research also indicates that consumers individually vary in their tendency to anthropomorphize AI agents, with subsequent implications for consumption. Our results reveal that individual differences in perceptions of technology anthropomorphism moderate offer acceptance in cases of worse-than-expected offers (Study 3a). Managers should consider integrating trait measures of technology anthropomorphism when developing consumer marketing segmentations relevant to AI interactions. Moreover, it is possible that other consumer-level personality traits will influence consumer–AI interactions (e.g., big five personality factors)—a potential topic for further research.



150.  Creativity, business and society in the age of AI

AI will fundamentally change advertising and marketing, industry verticals, and society at large. Private, university, and public research has contributed to steady progress in AI, but awareness of this technology’s potential blazed into the public imagination with the release of several generative AI tools. This drove rapid adoption—faster than any previous technology—and made clear that individuals, enterprises, and society would have to reckon with the impact of generative AI. The latest AI tools are probabilistic engines that generate human-like language, images, and video. They, like analytical AI and other applications of machine learning, operate using a trained neural network. Because they excel in a specific area, these are artificial narrow intelligences (ANI) as opposed to the artificial general intelligences (AGI) that replicate human-level intelligence and are, perhaps, on the horizon. Despite their limitations, AI have proved invaluable to many fields—notably science, coding, business, and media. Given that generative AI can produce art, copy, and code, the advertising industry must engage with these engines, and Ogilvy has been on the leading edge of that. We have explored a range of generative and analytical AI in order to serve our clients better, and we have learned that the incorporation of it into advertising and marketing will not herald the death of the agency or the sunset of human creativity. Instead, AI will lead to a creative and strategic renaissance. AI will spur better ideas and deeper insight by speeding up creative iteration and mechanizing aspects of execution and production, freeing creative and strategic minds to focus on big ideas and solutions to client problems—to, in short, imagine. The legal and ethical aspects of AI are unsettled, and Ogilvy will always err on the side of caution; we will respect the rights of artists and the confidentiality of client information. Analytical AI is already embedded in performance marketing and the marketing tech stack, and as it advances and couples with generative AI, agencies and clients will be able to personalize and scale with better precision, speed, and efficiency. Realizing the gains of AI will require better collaboration inside of agencies and between agencies and clients. Fortunately, AI will help here, too, and while this new era is likely to result in job changes and even losses, industry growth and new professions will, ultimately, lead to better outcomes for our people. Disruption will hit all of society, and few businesses will be immune. Three hundred million full time jobs are exposed to automation worldwide, and two-thirds of current occupations are susceptible to some degree of collaboration with or replacement by AI. However, productivity gains will ensure those displaced by AI will find re-employment and be part of a significant economic boost. Since the field is rapidly evolving and the stakes are high, enterprises need a framework for integrating AI into their businesses. AI can help interpret, interact, create, operate, and decide. Individual businesses will explore which of those functions are best suited for their needs by comparing potential business value with the feasibility of the solution and managing the change in three areas: marketing, technology, and organizational dynamics. Society, too, must work out how to integrate AI. The technology is developing faster than our ability to adapt—especially at the level of legislation, policy, and governance. This has left the AI community policing itself and shaping the narrative around regulation, which is problematic. As the innovation and energy in AI has shifted from academia to industry, several big players have emerged: Google, Microsoft, and Meta. It appears that, for the time being at least, they will control this powerful technology, how it develops, and how we interact with it. Their for-profit status complicates the push for safer development, and that is cause for concern. After all, AI that is poorly aligned to human values, desires, and priorities may be an existential threat. Nevertheless, AI can do enormous good. It can help mitigate (or, dare we hope, solve?) climate change, revolutionize medicine, lift billions out of poverty, improve food security, and much more. If there’s a problem, even a hard one, AI can help us solve it. Along the way, we’ll raise a new generation of people who have grown up in a world where AI is a constant companion. These AI natives will be the ones, ultimately, to show us how we all will adapt to this powerful force we have unleashed. Artificial Intelligences (AI), like the people who created them, are interplays of light and dark. Digital creatures of great power and potential, they can entrance and enhance humanity. Generative AI like Chat GPT and Stable Diffusion have stormed through education, the creative industries, and media, and their output feels a bit like magic. The inclusion of a conspicuously labeled paragraph of AI-generated writing is a trope in the thousands of think pieces being written on the subject as are the clearly AI-generated ad images that seem, for now, novel. These clumsy early uses of AI will give way to its seamless integration into our personal and professional lives, helping us in ways prosaic and profound. From taming inboxes and simplifying our daily lives to improving logistics, making medical and scientific breakthroughs, and everything in between, AI will remake our world. Humans have adopted these new tools faster than any technology in history because, in part, the benefits are easily glimpsed and vast. So are the perils. AI, in the popular imagination, might rid itself of its creators, use our metabolisms for energy, or turn the world into a giant paperclip factory.02 Less fantastical worries are still vivid: wholesale job loss and transformation, metastasizing disinformation and propaganda, and handing the keys to our world over to entities that may soon surpass our capabilities—and see us as surplus. We must also grapple with risks of emergent sentience, should that happen and our ethical responsibility and vulnerability to it. And just as unconstrained nano machines could turn the world into grey goo, unrestrained AI could bring about a deluge of mediocre content, drowning out anything truly original—a giant reversion to the mean that would suck the life out of culture. It could also perpetuate the bias long present in human society, which is a flaw, of course, that pervades what AI is trained on. These worries are real enough that leading experts in AI and digital technology issued in March of 2023 an open letter suggesting a pause in giant AI development and a refocus on, “making today’s powerful, state-ofthe-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.” Caution makes sense when dealing with technology that has existential impact, yet we are conducting a civilization-wide experiment with AI. Investment in the space has moved from academia to the private sector,03 where it continues to make a great business case for its ongoing development, while being free from the constraints of institutional research. After all, we’re all talking about it, licensing it, using it, and reformatting our businesses around it. As Mark Read, CEO of WPP, put it in The Guardian, AI “is fundamental to WPP’s business in the future.” There’s no doubt about that. AI is poised to revolutionize creativity, becoming as essential a tool as digital image and video processing. It will occasion a similar restructuring of creative talent, raising the premium on originality and innovation at the expense of more mechanical execution, while giving rise to new creative subskills like prompt engineering and model training. AI will transform strategy and account management and will make it easier to get to breakthrough, well-researched strategies while easing the friction in the agency/client relationship. AI will bring about leaps in efficiency in production, media, and hyper-personalized delivery. It will up-end the whole search economy and make the race for the data AI scrapes first (AIEO, perhaps?) a whole new subindustry. And all that? That’s just one industry. Elsewhere, pioneering AI engines like IBM's Watson are already remaking everything from coding and memo writing to healthcare and law. Agencies and clients alike must think deeply about the ethics of AI and its use and what our responsibilities are to consumers. Despite the pleas of leading AI firms for oversight, governments will not be able to keep up with the technology, and until regulations have caught up, it’s up to us to uphold our honor. We must also anticipate our obligations in an evolving AI legal environment. This is no easy task, and we will err. So will our competitors and our clients. We will also benefit enormously and will find competitive advantage in the ethical, shrewd, and carefully considered use of AI—but only if we go in with our eyes wide open. OGILVY 13 The Art of Trending— Woods Art Institute Art is born of its time, and part of its power is the conversation it invites between art and culture. Sometimes that dialog is eternal, but as the world speeds up, art has a place in the rapid-fire news cycle, too. That’s where the Woods Art Institute came in. With Ogilvy’s help, the Institute launched an experiment in September 2022 to connect art to the topics of the moment. The campaign entitled, “The Art of Trending,” took Twitter’s trending topics and used it to prompt DALL·E 2 to create the most contemporary of contemporary art—an exhibition curated entirely by social media users, executed by AI, and displayed OOH—that surfaced the role generative AI will have in art and culture. So, what is AI anyway? “You can go crazy thinking about all the possibilities, because these are very, very powerful technologies…. AI is the most profound technology humanity will ever work on…. I think it will get to the essence of what humanity is.” —Sundar Pichai04 Hands-on exploration with AI is the best way to understand in your guts how transformative it is, but for those who have been working with it for a while, that sense is old news. They point out that the technology has evolved consistently; the world at large is just now catching on. After all, in a feat of natural language processing, IBM's AI Watson beat the best human champions on the game show Jeopardy! all the way back in 2011. The explosion of AI into the collective consciousness is due to a breakthrough in accessibility, not a revolution in the field. The computer itself provides a good metaphor. Computers quietly enhanced efficiency in the background for a long time before they suddenly seemed to be everywhere, and for that, we can thank the graphical user interface. That took a technology that required a lot of specialized knowledge and turned it into one that anyone could use. Generative AI has done the same thing for machine learning. For all the simplification that the new tools have brought, understanding how AI fits in our world still requires a grasp of what AI is. AI combines huge datasets with computer processing to create inorganic systems that can solve complicated problems. These systems sometimes (but not always) appear human-like in the way they approach and resolve problems. They often meet or exceed human problem-solving capacity. For the sake of not getting tied up in philosophical debate about sentience, self-awareness, or cognition, we refer to them as intelligences since they act like intelligent entities: They learn from their inputs (their “environment”), reason, generalize, perceive, communicate, and evolve—sometimes in unexpected ways. The most sophisticated in a long line of assistive tools invented by humanity, AI are likely to be our partners, for good and ill alike, in most aspects of life. As Ogilvy’s executive creative director and experience creative lead EMEA David Raichman puts it, “The first social unit that will appear—even before the family—will be the human and a generative AI partner.” But if we delegate all of our tasks to it, we will become passive recipients of culture, not its creators. Instead, we use AI as a “sparing partner,” in Raichman’s memorable phrase, “that will tempt us to create in a new way, creating a new form of art and a new form of advertising,” that could not be created before. Raichman’s Ogilvy colleague Roberto Fara, chief creative officer Spain and global creative experience lead, puts it like this: “Generative AI can imagine new things that our minds cannot right now achieve.” At the center of it all, both Fara and Raichman believe, lies the idea, and that, for now at least, remains the province of the human. “Machines don’t have creativity yet,” Fara says. “They cannot answer if something is good or wrong. They don’t know what it feels like to travel, to taste a tomato.” They lack, to use the technical term, qualia: introspectively available moments of subjective, conscious experience. But since we do, imagine the potential of these tools for our business and that of our clients. “One of the biggest challenges as a creative agency,” says Ab Gaur, Ogilvy’s global chief data and technology officer and the founder of Verticurl, “is scaling our smartest people and the work they do. If we could, we could have more impact in the world for our clients, creating brand experiences and moments that AI will help us execute.” Understanding AI Scientists create AI using machine learning, especially a subset of that called deep learning that takes place in artificial neural networks. In essence, these systems analyze large amounts of data using algorithms that mimic the human brain to make predictions or decisions, and they are broadly classified as artificial narrow intelligence (ANI), artificial general intelligence (AGI), and artificial super intelligence (ASI). ANI is a weak AI, meaning that it can perform specific tasks. AGI is a strong AI that is similar in ability to a human, while ASI would surpass us. Though remarkable, the current crop of AI are all ANI.05 There are several types of AI. The one that’s getting all the attention now, generative AI, is either a text-to-image model or a large language model (LLM). The latter has been fed an enormous dataset of human knowledge in the form of text, images, or both. Text-to-image models, such as Midjourney or Stable Diffusion, use pairs of images and the text descriptions thereof to predict the kind of image a given natural language prompt should produce. It then applies some data noise into this probabilistic engine to produce original, high quality images. LLMs work similarly. They use the billions of words on which they’ve been trained—and all the biases, intolerances, poor behavior, and unrepresentativeness encoded in them—to form predictions about what word is most likely to appear next in a series, a task at which it far exceeds human capacity. They are, to quote linguist Emily M. Bender, “stochastic parrots”—probabilistic content generators that have no ties to meaning. These systems are designed to mimic likely human responses and persuade us to believe them. LLMs are, as many have pointed out, A+ bullshitters, and that is one of the reasons they appear so sentient to us. We apply our own theory of mind to them and, as Bender said to reporter Elizabeth Weil in New York06, “We’ve learned to make ‘machines that can mindlessly generate text…but we haven’t learned how to stop imagining the mind behind it.’” That said, LLMs continue to display emergent behaviors (also known as agentic behaviors) that their designers did not and could not anticipate, which complicates the picture. While generative AI has sucked up all the media attention of late, it’s far from the only type of AI. Neural networks and machine learning have been used to form everything from specialized expert systems to recommendation engines and from fraud detection to logistics load balancing. They’ve integrated with robotics, computer vision, facial, speech, and audio recognition, and the natural language processing (NLP) that is most familiar. One specialized AI, AlphaFold2, took a mere 18 months to crack one of the hardest problems in biology07: predicting the structure of nearly every protein known to science. This breakthrough will lead to new drugs, improve treatment outcomes, and contribute mightily to basic science. AI is already helping with resource allocation, personalized marketing, efficiency boosts, and even strategy. AI already has a years-long history with marketing and advertising through analytical AI engines enhancing sales enablement, CRM, personalization-at-scale, customer behavior prediction, and more. One way through this thicket is to see AI in three big buckets, as Dickon Laws, global head of innovation for Ogilvy, does. The first is generative, which will produce a golden age in creative expression and speed the production of personalized, even atomized, assets. Then there’s the analytical bucket. It powers marketing automation and sales enablement, enabling us to connect communication to context at the individual level and work within a rich landscape of triggers and behaviors. The third is what Laws calls novelty. This is where AI becomes invisible, disappearing into the background as it delivers a holistic experience that, to quote Arthur C. Clarke, “is indistinguishable from magic.” AI adoption AI has launched think-pieces by the thousands in the past few years, but the uptick in hot takes corresponded to the hockey stick in adoption that happened after the release of ChatGPT, a chatbased generative AI from OpenAI. Now embedded in Microsoft’s Bing search engine and soon to roll out more broadly, ChatGPT focused the public’s mind on the potential (and peril) of generative AI. The hubbub pushed AI development out into the open and turned it into something of race. A massive burst in adoption followed. Two months after its release, ChatGPT reached 100 million users—faster than any technology in history. The AI market is projected08 to increase from just under $200 billion in 2023 to $1.8 trillion by 2030, and that’s probably conservative. Even before the public’s imagination caught up, however, enterprises were well on their way to making AI an essential part of their businesses. IBM notes in its Global AI Adoption Index 202209 that global adoption of AI continues to grow steadily, and it now stands at 35%. That relatively modest figure obscures that, “in some industries and countries, the use of AI is practically ubiquitous.”10 Business is using AI for everything from memo writing to process automation. In fact, AI governance will join cybersecurity and compliance as a board-level topic. Data from Forrester shows that 46% of data, analytics, business, and technology decision-makers seek out partners to implement AI critical to the business11. That said, McKinsey & Company12 found in its annual survey of companies that AI adoption, after doubling since 2017, has largely plateaued. Companies that have adopted AI, however, are “realizing meaningful cost decreases and revenue increases.”13 Accenture is much more bold, claiming that AI will increase the developed economies’ productivity by 50% in the next two decades14. AI development Bill Gates gave Open AI, the developers of ChatGPT, a task. He thought it would keep them busy for two or three years, but it took them only a few months. This AI flashover has propagated throughout the space and ignited the public’s imagination. One imperfect measure for the complexity, and therefore the potential, of an LLM is the number of parameters in the model. At 170 trillion parameters, the model for GPT-4 is 100 times larger than its predecessor, and, unsurprisingly, vastly more powerful, too. It’s made news for acing Advanced Placement tests and the Bar Exam while polishing off benchmark test after benchmark test with flying colors. LLMs are already working their way into most of the apps people use every day. Microsoft is introducing AI into its Office suite of products. Google is, too, but so are uncounted developers using LLMs as a spine for a bewildering array of AI-powered apps. It’s going the other way, too. OpenAI has launched a plugin service, instantly turning their LLM into a platform hosting travel recommendations from Expedia, restaurant reservations from OpenTable, and shopping enablement from Klarna—and that’s just the beginning. Amazon, too, is in the platform game, enabling customers to build and scale generative AI applications on a range of different models. The latest generation of AI is also showing increased flexibility, which means these can perform multiple tasks, accelerating scientific research, and, like any good overachiever, self-improving. Image generators like Stable Diffusion, Midjourney, and DALL-E-2 are refined enough to produce professional-looking work. The same is true on copy side of the house with tools like ChatGPT-4, Jasper, and Hemingway. AI coding assistants, such as Codex, CoPilot, and CodeWhisperer, are growing in sophistication and popularity. According to Forrester Research, AI will write 10% of the world’s code in 2023.15 Art. Copy. Code. If the penny hasn’t dropped yet, here’s the import: the major functions of many businesses—including creative ones like advertising and marketing—can now be done at a passable level by AI, and that means standing out will be harder than ever. As AI engines become commodities, which they will16, the kind of out-of-nowhere creativity that spawns breakthroughs will be ever more in demand. Marrying that uniquely human ability to a suite of powerful AI tools will produce more (and more remarkable) creativity. Relying on the machine alone will produce a sea of mediocrity Is this the end of creativity? Or the beginning of a golden age? A German artist named Boris Eldagsen won the creative open category at the 2023 Sony World Photography Awards, an honor he declined after revealing that his prizewinning photograph wasn’t what it said on the tin. His image, which emerged on top of 415,000 entries, was generated by AI. Eldagsen, in a statement, said, “Something about this doesn’t feel right, does it?”17 No, it doesn’t. This is proof positive that if even a panel of the world’s premier photography experts can’t tell the difference between a photograph and the work of AI, then human creativity is dead. That’s the easy answer. And the wrong one. Eldagsen, a world-class photographer, created the image to spark a discussion in the world of photography about the impact of AI and the definition of what was—and was not—photography. Not only did he succeed beautifully in that aim; he also showed that the world of creativity is not in the incorporeal hands of AI. Rather, it lies in the mind and expertise of gifted artists and writers who now have a powerful new tool to extend their creativity. AI isn’t the death knell for creativity. It’s a renaissance Creating with AI Imagine you’re in a scriptorium as the 15th century comes to a close or an ad agency creative department as the ‘80s did. A powerful new tool debuted—the printing press for those early modern monks and Photoshop for the creatives who were listening to this song without irony—and with its arrival, you felt your world shift. Both of those tools brought about a flowering of creativity, stretching the bounds of what humans could do. So it is now with generative AI. Generative AI threatens to dethrone humans as the kings of creativity, but keep in mind Elizabeth Bender’s message. These systems have been trained to produce human-like responses, and while their work may appear creative, they generate it out of a dance of diffusion and probability that is divorced from meaning. It seems creative without actually being so, because creativity is born of meaning. Who cares, right? So long as the output accomplishes the job, what difference does it make if the machine knows what it is doing? When it comes to the more mechanical parts of creativity—the test runs of ideas, the variations on a theme, the stimulus to get the creative juices flowing—that question of meaning doesn’t matter a bit. Nor does it matter at the other end of the process, when fully realized creative work is tweaked for multiple environments or hyperpersonalization. Though done by people now, that is work that can, and probably should, be done by machine. What about the people? We’ll come to that, but first let’s take a look at the three different futures the Harvard Business Review believes generative AI might bring the creative world. 1. The AI partner. In this outcome, “AI will support humans to do the work they already perform.” It would just be faster, easier, and cheaper to produce. Prompt engineering—the art of getting what you want out of the AI—will become the only crucial skill, and AI-generated art, copy, and code will flow like the Nile, fertilizing creative departments and even all of advertising and marketing. 2. The AI master. “Unfair algorithmic competition and inadequate governance” crowds out human creativity in this scenario, leading to a flood of work competing for attention and driving the costs of creation down so far that humans become an uneconomical, inefficient relic. 3. The human boutique. Overwhelmed with algorithmically generated content that just isn’t good enough, people turn toward the human-made variety, especially since humans, in this possible future, maintain a creative dynamism machines can’t match.18 We believe the future is far more nuanced than that. We are deeply engaged with all forms of generative AI, and the near-term seems clear: AI will augment human creativity—profoundly—but the organic mind will be the source of the breakthrough idea, the creative leap, and the work that rises above A CREATIVE RENAISSANCE the clutter. As David Raichman puts it, “Creativity will be empowered by the machine and won’t be replaced by it.” Generative AI lets anyone turn ideas into writing, images, video, audio, and code using nothing but words. It can take a script and turn it into a mockup film in just hours. New tools (and it seems absurd to call out these bleeding-edge ones as “new” when the “old” ones haven’t hit their first birthday) are now able to generate new images and videos from visual references, not just written ones. Ogilvy is using generative AI to go beyond the boundaries of what humans can achieve alone. “The magic occurs,” says WPP’s Stephan Pretorius, “when you combine human insight—and cultural insight—with this ability to create content with machines.” “Here, with all the dogmatism of brevity,” as David Ogilvy wrote, is how we do it. Build new teams. We have relied on creative teams from the dawn of the creative revolution, but no longer will they be an art director and copywriter working as a pair. To that, we now add AI, and more. These teams will grow and contract according to the demands of the project, and they will seamlessly include people from outside the traditional creative enclave: strategists, coders, account leads, and anyone in the organization who can contribute. For these teams to unleash the power of AI, they, “need to explore, research, play, and learn. They are learning a new language,” as Robert Fara suggests. But what is that new language? Prompt engineering. When a creative person sits down to produce something, they do so by creating a piece of visual art, writing a block of copy, crafting a bit of code, or designing a strategy. Describing that task The 7 creative commandments for generative AI to a machine using very specific words is, Fara says, “when it turns into a nightmare.” Some can deconstruct their creative process into a description that a machine can follow, and they—and the others who can learn this skill—will become a vital new part of the creative workforce: creative prompters. These will be the people who can play generative AI like an instrument, eliciting from it the beautiful realization of the notion in their hearts, and they will need to be given room to learn, to play and, for the first generation at least, define their very profession. AI isn’t creative. It can only work with what we give it. If we ask for a picture of an apple, we’ll get something serviceable. If, instead, we ask for a photograph of an apple shot with a Summilux lens on grainy film in the style of a Surrealist version of Man Ray, we’ll get something more interesting. And if that careful description is paired to a creative concept, perhaps “fruit this good is so rare it might as well be unreal,” and a strategy that animates the brand at its best, then we are on to something that may be useful to the brands at its best. Then we are on to something that may be useful to the brands in our care. In other words. AI is in thrall to the big idea, just like we are. Become a connoisseur. A CREATIVE RENAISSANCE “Just as the internet democratized information,” says Antonis Kohelis, president of advertising for Ogilvy, “generative AI is democratizing inspiration.” It takes no leap of imagination to picture what that means. The internet flooded us with disintermediated information, much of it raw, irrelevant, or just plain wrong. Generative AI will drown culture in a tsunami of mediocrity. Trained on what exists, not on what has yet to be imagined, the output of generative AI will steadily regress to the mean. You can already see it. The Midjourney- or Stable Diffusiongenerated image is already something of a trope in blogposts and PowerPoints. To counter this, we must develop a talent strategy that prizes discernment everywhere in the organization: creative, account, and strategy. Ogilvy’s mantra of Divine Discontent no longer just applies to our own performance. It applies to what we work with AI to create. Because these engines are able to produce carefully crafted output, we can be lulled into praising soulless executions and not pushing ourselves hard enough to uncover the dramatic idea that should animate it.

“AI is force multiplier for creative expression.” That’s David Raichman’s phrase, and he’s right. By treating these inorganic intelligences as navigators into the land of possibility, we can uncover dimensions of our ideas we simply did not have the time or energy to explore before. That’s not laziness; it’s a function of our biology. The human mind is a pattern-recognizing machine, and it is ruthlessly efficient in pruning away that which is extraneous to the task at hand. As good as creative people are at opening their minds to serendipity, even their mental hardware innately edits out countless possibilities before they ever rise to the surface of their thoughts. AI, unencumbered by meaning or judgement, can propose iterations our minds would have never found. Protect artists and brands. AI will create economic upheaval for creative people. A whole layer of creative output may be wiped out as agencies automate as much of the mechanistic work of versioning, personalizing, and production as possible. Jobs will change, and, yes, some will be lost, but new opportunities will open as well. We will enhance those opportunities and nurture artists. The output of AI trained on uncompensated work is off-limits to us. Not only does it pose legal issues for us and for our brands; it steals from creators. We will train AI on our own work and use engines trained on datasets composed of rightfully compensated work such as those from stock houses and individual artists. This is a thornier problem than it seems at first glance since all LLMs are trained on datasets scraped from the internet, which, naturally, includes copyrighted work. Even fine-tuning and training generative AI may layer on top of existing training of uncertain provenance. Since legal and ethical issues remain unresolved, we will take the most conservative approach. We will also take care with what we tell generative AI. At present, many of the engines scrape and store the content of prompts, and we will never compromise client confidentiality by being careless with them. Speed production and personalization, not creation. Much of what is done in the production phase will be automated. Production work will move from production companies to agencies where it will join personalization and versioning in being done by machine. So will market research. Some of this work won’t involve the agency at all and will, instead, be done directly by clients. This will increase efficiency and cut costs. Rob Hill, chief executive officer, Ogilvy Social.Lab Brussels, recalls shooting “for 19 days to get the right packaging across 18 countries in Africa.” With AI, “You can change logos, packaging, and make things relevant very quickly.” AI-enabled market research will speed up response to creative stimulus and validate the work at the same time. It can also aggregate huge quantities of market feedback for creatives, expediting fine-tuning and cutting back on rounds of review. This leaves Ogilvy the freedom concentrate our efforts on our creativity and our skill so that we can work in partnership with AI to create unmistakable value across the spectrum of creative, from strategy to assets. Advertise your ethics. When is AI a legitimate addition to the creative process? When it is disclosed. When it doesn’t deprive creators of their rights. When it doesn’t court legal jeopardy for us or our clients. When it doesn’t deceive. We will be a beacon for how to use AI to reach new creative territory, uncover deeper human connection, and do so ethically. Scaling with AI Generative AI allows us to scale our talent. When we use these new tools to reduce the time we spend on repetitive tasks, we can increase the time we spend on craft, much in the same way that programs like Photoshop freed designers to spend more time exploring ideas instead of pasting up boards. Talent “If I can try 100 pictures in less than an hour, then I can spend more time thinking about the pitch,” says Fara. At the scale of Ogilvy and of our clients, he continues, “that’s a lot of money and a lot of time.” It can launch copywriters down more pathways in less time, broadening the exploration process. In fact, generative AI allows the entire organization to scale, boosting the productivity of account leads and strategists just as much as creatives. To quote Gaur, “It helps us scale the smart work done by our smart people.” AI additions to familiar tools like Photoshop and Illustrator, for example, dramatically accelerate creative output, especially in the production phase, but there’s another side of scaling. It has to do with what happens when great assets meet great data. Personalization One of the ways we’ll do that is by delivering relevant content to the right people in the right context at scale—something long discussed in marketing but imperfectly realized so far. This is the true potential of marketing automation and sales enablement, and a combination of AI engines are coming together to make that possible. “The A CREATIVE RENAISSANCE adjacent technologies are jumping into the space to equip us in industry in a new way,” say Hill. Advertising has many creative tools available, “to create beautiful images, content, and templates, etc.,” says Gaur. On the other end are the outputs: TVCs, brand experiences, dot com channels, mobile, social, and the like. In the middle is data. By combing them (along with powerful workflow tools), AI can assist in the production of personalized assets in real-time. Analytical AI can help here. An Adobe AI called Sensi can give marketers recommendations based on the performance of a similar campaigns, helping identify segments, targets, and individuals to engage with. That lets the agency make a better recommendation to the client in less time. Or consider the sales side and, for example, the Salesforce AI, Einstein, which can qualify leads, manage opportunities, and even provide lookalike targets for salespeople to approach. No matter the assistive AI, “it reduces the cost of operation for the agency,” Gaur points out, making us less expensive to the client while being even more effective. Collaboration With everything operating at greater speed, better collaboration is essential, and AI can help here, too. AI-enabled workflow management links the digital asset management, the analytical intelligence, and the people using them into a unified environment for real-time cooperation. Comms teams, marketing teams, social teams, and digital teams can come together in a single platform, and that streamlines work and makes it easier to take in client feedback—leading to faster turnarounds and increased efficiency…and efficacy. Disconnected systems and teams are becoming a thing of the past. The press has been filled with worry about the human cost of this automation: “The people who were part of a repetitive task that this automation takes away,” notes Gaur. But that doesn’t mean mass redundancies in his mind. Gaur sees a world where companies invest in making their people future ready, not just their tech. “There will be a lot of jobs out there that we haven’t even thought of yet,” he says. Growth in what has been a contracting industry will more than absorb any disruption. Strategizing with AI “Many of these tools are interesting for analysis: summarizing articles, research, and extracting key themes.” For planners, “AI is a productivity leap board,” or so says Hill. Strategizing and planning is a researchand analysis-heavy part of our business, and that learning enhances inspiration, helping the agency get to the insights that underlie brand building efforts. Generative AI will occasion rapid change in strategy and planning, much in the same way the internet revolutionized those tasks starting in the ‘90s. The improvements fall in two categories: productivity and quality. “AI,” Hill says, “can help us as an industry accelerate and do basic things quicker.” He’s referring to category, consumer, audience, and journey research. Inorganic intelligence can chew through the laborious part of that by directing planners to key sources and answering deskresearch questions with useful summaries. Transcription, the bane of many a planner and account executive’s life, is now an AI afterthought, shaving off hours of mechanical labor. Just by automating the basic information gathering and analysis that enables insight, AI allows humans to spend more time thinking while still improving productivity and lowering costs. That’s essential in a business beset by margin pressure. Hill believes that if we embrace AI, Ogilvy can improve underlying profitability. We’ll “work smarter and have fewer people doing what we used to do.” Disruption will follow, of course, but perhaps that’s exactly what advertising needs to enhance its future viability, and growth, Hill joins Gaur in hoping, will absorb the impact on people. An upsurge in quality will come as Ogilvy integrates AI into planning and strategy. Beyond the obvious fact that planners will have more time to generate better ideas if they’re not spending time doing what a machine can now do, generative AI opens new paths to inspiration. With imaginative prompting, they can suggest new audiences, themes, or expressions. They can explain complex topics, allowing planners to connect dots through areas that were once opaque. AI can help people get unstuck, nudge them toward new avenues they may not have considered, and assist them in organizing their thoughts. They can suggest ways to improve writing, and that improves the clarity of our communications with each other and our clients. AI shines when it comes to finding just the right image to express an idea, designing a beautiful presentation, and even composing, performing, and creating a video for a song that livens up what would have been a boring topic—so long as people know how to use it well. Creatives aren’t the only ones who will need time to explore AI. Strategists, planners, and account executives do, too, if they’re to master this new technology. Formal training will follow when we get to the point where AI has redefined industry processes and led to bespoke AI-enabled workflows, but until then people need room to experiment and allowance for the time and costs of satisfying their curiosity. That’s how we nurture expert users and learn how to tell better stories faster and cheaper. As more employees use generative AI, incidents like this will become more common. A recent survey by Kizen found that 90% of workers earning over $100,000 reported using AI in their work20. In case the implication isn’t clear, that means AI is coming for white collar jobs—something you’ll learn more about in the next section. AI adopters have a downpour of how-to-use-AI content to help them do their jobs, from prompt templates and AI work hacks to breathless newsletters and databases of AI tools. No longer a specialized subset of the IT world requiring heavy training and investment, AI has shouldered into the workplace through platforms and public interfaces. Technology enterprises are striking partnerships to help them roll D And most of those jobs are in industry, not government or academia. The rest of the economy may not be so lucky. Three hundred million full time jobs are exposed to automation worldwide, and two-thirds of current occupations are susceptible to some degree of collaboration with or replacement by AI25. Fortunately, Goldman Sachs anticipates, “that many workers that are displaced by AI automation will eventually become reemployed—and therefore boost total output—in new occupations that emerge either directly from AI adoption or in response to the higher level of aggregate and labor demand generated by the productivity boost from nondisplaced workers.”26 Lest you think this is Pollyanna-like thinking, consider that 60% of workers are currently employed in jobs categories that didn’t even exist in 1940. In other words, “over 85% of employment growth over the last 80 years is explained by the technologydriven creation of new positions.27” There’s ample precedent for the positive effect of disruptive technology, too. Goldman Sachs notes that both electrification and personal computing resulted in substantial productivity booms. Perhaps the confidence Gaur and Hill have isn’t misplaced. While the GDP impact from AI will be enormous, it won’t be evenly distributed. Judging by their investments, private investment has bet that the biggest payoffs initially will be in medical and healthcare; data management, processing, and cloud; fintech; cybersecurity and data protection; and retail.28 But those aren’t the only industries to benefit. Making AI work for your business The combination of just two foundational technologies—computer vision and natural language processing—provides nearly endless possibilities. The AI we already have offer so much more. Getting from that promise to the productivity and GDP gains on the horizon will involve a lot of questions, many of which, unfortunately, are still unanswered. The first, and most important may be, as Ashley Wood, global principal, brand innovation and insights at Ogilvy Consulting, asks, “Is AI truly business-ready yet? Do you really trust your business with it?” And, for that matter, how do you even make those judgements? How do you use it in your business and for your products? How do your employees use it to work better? What policies do you apply? Where do you place your investment now and in the coming years? And so many more. Yet, there is a way through the woods: while AI is disruptive in ways we don’t yet understand, it falls into a familiar framework. AI is the next chapter in the large-scale digital transformation that began at the end of the last century. As a result, some of the same ways of looking at change may help businesses adopt AI wisely. The first step is to look at what your business needs in light of what AI can do now and what is on the horizon. The core layer of machine interpretation is well developed (even while it continues to advance). While slightly more advanced, interaction is also substantially developed already. AI creation, as the mania surrounding generative AI demonstrates, still leaves people astounded, even though the tools have a long way to go to catch up to the hype. On the simpler end of the scale, autonomous operation is starting to come into its own, but reliable, full AIdriven mobility in complex environments is still in the future. AI decision making, too, remains immature, even if it is getting better fast. Consider BloombergGPT, a 50-billion parameter LLM built especially for finance. It’s likely the first in a series of domain-specific models that will help business leaders run companies better in the future. Rare, however, will be the company that can fund such an effort on its own. LLMs are hugely expensive to train, in both monetary and carbon costs. Choosing to build one from scratch might break the company budget while also putting a crimp in its sustainability story. And yet the temptation will be strong because the benefit to business decision making from having specially trained LLMs will be vast. So will the potential to sell access to insights born of the huge data sets large companies have. Every vertical will have its bespoke large language models that are better than the generic ones. Nevertheless, the existing ecosystem offers plenty for a company to access, and for Salmenkivi, this opportunity falls in three buckets: distribute, capture, and create. 1. Distribute—Businesses can license existing AI technology which they then exploit in a middleman model. They build it into their core offering or create services or products that run on top of it. 2. Capture—Open market AI services can cut costs, improve products or services, or enhance customer satisfaction. With proper safeguards in place, staff can use existing platforms outright. Alternatively, a business could purchase a tailored AI solution or build AI agents on existing APIs to automate tasks in research, marketing, customer service, or e-commerce. AI can even help enhance a business’s own API. 3. Create—This is the most expensive option, and companies can choose to use some of what’s already created or elect to build it all from scratch—and everything in between. This provides the opportunity to develop custom LLMs, neural networks, and algorithms, which can fuel decision-making and insight. The outcome, or for that matter, the training sets and underlying hardware, can be sold as part of an AI-specific offering, not unlike how Amazon Web Services (AWS) emerged from Amazon’s internal IT systems. Armed with this broad view, a business can look more deeply into which AI applications offer the best combination of AI and feasibility. Christopher Brewer, president of Ogilvy Consulting in Asia, likes to consider if, “this disruption is going to change a leader’s world tomorrow or if it is something that has a longer tail to it,” and to determine that he’s built a framework and fleshed it out with examples from three verticals: marketing, B2B sales, and finance29. A quick glance shows that a few things are emerging now—in Brewer’s words, “customer insight, finding audiences, finding leads…the stuff that fuels business today. That’s where AI is going to have an immediate impact.” Others, like CLV analysis, financial decision support, or strategy, may have huge potential for business but are further down the development line. The trick, Brewer believes, is to focus on the tip of the pyramid while waiting for the technology to catch up to the other tasks of high business value. Extracting that value requires change management in three areas: marketing, technology, and organizational dynamics. Before you freak out—and if you’re not a little nervous, you haven’t been paying attention—remember this: these are the early days and progress is slower than it appears. Sure, AI is going faster than the internet, but 30 years on from the introduction of the internet, e-commerce only accounts for 15% of buying and selling. People have been formulating the future for AI for over a decade now. IBM Watson, for example, debuted in 2010. It's no stretch to say that ten or so years more will be needed for AI to mature in business. This is your moment to gain first-mover advantage, not the time to realign your whole enterprise to AI. Improving inter-corporate relationships That said, AI may soon be able to help your business get in tighter alignment with your partners. Building cooperation between two organizations, either for the purpose of M&A or just a client/ vendor relationship such as the one that powers advertising and marketing, is a complex problem. Systems have to be merged. Metrics shared. Individuals directed. Cultures meshed. Expectations set. The list goes on, but this is a place for AI to assist. One of the most challenging times between an agency and a client is during the first phase of working together. There’s a lot of friction, not because of bad feelings but rather because, as Dickon Laws says, “You’re bringing a bunch of new people together.” Not everyone on the client side will have been involved in the pitch, and they need to see that the company has made the right decision. On the agency side, people need to be “speed dipped into what the brand is,” as Laws rather vividly pointed out. A single AI, however, trained on the particulars of the pitch, the needs of the client, and the information gaps in the agency could smooth this onboarding process considerably. Such a vision suggests that AI could also be used, Laws suggests, to determine which agencies are a good match for the needs, culture, and talent of a client (and vice versa) and do so in a much more data-driven way than any un-augmented pitch consultant could muster. That AI could even predict the chances that each organization—client and agency—will end up in and alliance, allowing each to better apportion their resources. As the rapid adoption of AI makes clear, we, as a culture, have decided that the present and future benefits of this technology outweigh its potential harms. Disruption is coming, and it will cut across nearly all industries. As AI integrates with robotics, that disruption will spread to verticals that may be spared the initial upheaval. Though productivity projections suggest that job losses and displacements will be absorbed by growth, that’s little comfort to the individual who has been turned out on the street. Projections may be wrong, too. After all, the internet was expected to produce a massive spike in productivity, but it didn’t alter the labor productivity curve much at all. W That’s not the only worry. We are weakening the foundations on which shared information, authenticity, and veracity rest—and that comes on top of the damage that has already been done there. Given the economic incentives, it’s unlikely we’ll stop anytime soon, and even though open source AI engines like HuggingFace and Stability are in the market, “the way AI is going to work is through a competitive dynamic between [sic] Google, Microsoft, and Meta30,” as New York Times columnist Ezra Klein puts it. Governments and policymakers are awakening to the destabilizing force that is AI. China, mindful of its internal security and social placidity, has promulgated draft regulations that would place generative AI within the nation’s censorship regime. Questions of copyright for AI generated work and usage rights for training data are working their way through the legal system at a rate seven-times higher than in 2016;32 the resulting decisions will have significant impact on how we use AI.33 Legislatures and parliaments are keen to weigh in, too. The speed of AI development and its uncanny imitative power will easily overmatch the glacial pace of government. From our creative agency standpoint, we worry about the tide of mediocrity that AI ensures will soon wash in, but from a societal point of view, the risk of high quality, magnetic, convincing, and even deep-faked misinformation is much more troubling. Recall that AI is designed to seem human and convince us of the authenticity of their responses. Allowing corporations that depend on the manipulation of people to wind up in effective control of that technology is foolhardy. To put it baldly, the better Klein’s oligopoly of Microsoft, Meta, and Google persuade and manipulate humans, the better their financial performance will be.34 Therefore, they will optimize for that variable. Now imagine our polarized ideological landscape and the ubiquity of irresistible falsehoods. It doesn’t take a great leap of logic to see that we need fear not a sea of sameness but rather an ocean of untruths. AI won’t just corrupt the public’s taste. It will rot its already-flimsy ability to discern truth. The AI-generated hit song featuring fakes of Drake and TheWeeknd is a catastrophe for intellectual property,35 but the right (or, rather, wrong) kind of AI-generated faux presidential video could be a threat to civic order. Consider, too, the environmental cost of AI. Training runs for LLMs use significant amounts of electricity, cooling, and, of course, electronics. Even one of the most efficient models, BLOOM, emitted 25 times more carbon than a flight from New York to San Francisco.36 Daily use can be even more costly to the environment. One estimate has ChatGPT using as much energy as 175,000 people in January 2023 alone.37 During its training runs, generative AI drank deeply from all the bias and hate humanity has vented into publication and the internet. “It’s going to replicate those biases and going to tuck them away in a black box that makes them harder to uncover,” says Klein.38 They creep into the responses we get from generative AI, even if the prejudice is covert. As AI grows in popularity, its responses will shape normative ideas of race, gender, sexuality, and representation, further cementing corrosive biases just when society was beginning to deal with them. The filters and safeguards companies put on AI help, but they are not perfect. Clever prompt engineers have induced filtered AI to write porn and induce mayhem. Unfiltered or loosely filtered AI might be far more destructive. We just don’t know. Why not? Well, Klein’s mention of the black box is important. We know how AI is built but not what happens in its cold, silicon heart. This is the problem of interpretability. Researchers maintain that they don’t know why their algorithms do what they do, and that’s one reason AI develops skills the designers never imagined. Until we can see inside the models, we can never adequately predict their abilities or be fully confident in their results. If AI is going to make HR decisions, cybersecurity improvements, financial transactions, resource allocations, and the like, we need to see how it is arriving at its conclusions so that we can judge the impartiality or correctness of its actions. We demand the same accountability of humans. Why not AI? Interpretability, Klein asserts, is a potent way to mitigate some of the prosaic dangers of AI—things like economic crashes, security meltdowns, biased hiring decisions, and more. It may be in the public interest, but it’s not necessarily what the AI companies are after. The hard problems As you can see, even today’s remarkable, but specialized, AI (ANI, for those of you who remember a few thousand words back) suffer from what are broadly defined as alignment problems—a gap between the AI’s actions and human values, goals, intents, preferences, and principles. That’s frightening enough. What happens when we achieve AGI— human-level inorganic intelligence? Poorly aligned AGI poses an existential threat, and AI business leaders and researchers know it. That’s the impetus behind a May 30, 2023 statement that said, “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risk such as pandemics and nuclear war.” This isn’t just public posturing, either. “P-Doom” is a new topic at Silicon Valley parties. P-Doom is the probability that an individual gives to AI bringing about catastrophe for humanity, and, as Casey Newton said in a recent episode of Hard Fork, “In the AI research community, there are people who think that probability is like 10 % or higher. ” As fascinating a technological problem as AI is—and as much potential for good that it has—one must wonder why humanity is GETTING IN ALIGNMENT tinkering with a technology that experts believe has a 10% or higher chance of subjugating or eliminating our species. P-Doom may be something we don’t have to worry about for a while. Even Sam Altman, CEO of OpenAI, the creator of it. It could be decades before AGI debuts. Or years. Or months. As Nick Bostrom points out, the experts are all over the map in predicting its arrival40, but we had better solve the alignment problem long before its birth. The recursive self-improvement baked into AGI will likely cause a rapid intelligence explosion, leading from AGI to artificial superintelligence far faster than we would be able to react. You can imagine for yourself the risk that an unaligned vastly superior intellect would pose to the previous holders of the cognitive crown. This is an enormously difficult problem. Says OpenAI, “Unaligned AGI could pose substantial risks to humanity and solving the AGI alignment problems could be so difficult that it will require all of humanity to work together.41” That’s sobering. OpenAI proposes to solve it by tasking AI with it: “Building and aligning a system that can make faster and better alignment research progress than humans can.”42 What could possibly go wrong? ChatGPT, acknowledges that less is to be gained by training larger LLMs and more to be won by focusing on “rapidly increasing capability.”39 Or we may need different models, such as whole brain emulation, enhanced networks, or brain-computer interfaces, to achieve The promising solutions If this last section has been alarming, good. AI is here to stay, and AGI is coming. An uncontrolled experiment on an existential threat is folly, and even the inventors of the atomic bomb, working at the height of war, took every step they could to keep the world in one piece. We must do the same, policing our own use of AI, demanding that safety research progress even faster than the models do, advocating for robust regulation, providing the economic incentives for sensible AI development, and mitigating the negative effects on those disrupted by this technology. We must do all that not just for our safety but also for our direct benefit. AI could be a source of good unmatched in human history. It can design new drugs, monitor marine mammals, and map the progress society has made toward renewable energy. It’s helping us detect deepfakes, communicate better across languages, and optimize energy usage.43 AI is behind a collaborative program linking NASA and the European Space Agency to gather and analyze crucial data about the earth and help GETTING IN ALIGNMENT the world meet the UN’s Sustainable Development Goals. AI solutions will be critical to commercializing nuclear fusion44, improving global food security, and bring better health to more people for less money. The list goes on; imagine the problem, and you can be sure AI will have a role in solving it—up to and including the big ones like restoring the health of our planet. These are the applications Dickon Laws calls novelty, as you may recall. They are the places where visibility of the AI’s involvement recedes into the background. Is this a fantasy? Perhaps. It’s good to be skeptical, and it’s better to be vigilant. After all, nothing about AI is black and white. Nonetheless, “something quite profound is going on,” as Klein notes, and that’s with just the technology we have today. Properly aligned and with the political adjustments to match, AI could go a long way toward ending scarcity, and that’s about the most profound change imaginable. The AI natives If it isn’t obvious already, this should make it abundantly clear: All of us will have to work with AI in order to accomplish the tasks of daily life—the work of living and the toil of earning. For many of us, this will be a difficult adjustment, much like what those who did not grow up with computers had faced when those wonders came to the workplace and the home. Even digital natives will have to accustom themselves to the fact that their relationship with technology has shifted to something much more bilateral. That’s a fundamental change, one that treats technology as a partner, perhaps even an equal one. In a way, AI will become another member of the creative team that David Reichman spoke about and will help us create the narratives of our own lives. There is one group that will experience no adjustment: those born in the past few years, and those yet to be born. They will be AI natives, and, like Gen Z before them, they are the ones who will define how AI is integrated into culture. They will show us how humanity will adapt to this new, powerful force that it itself has unleashed. Hopefully, we will have given them AI aligned to their interests, their values, and their desires. Hopefully, it is a partner for them and not a master. Hopefully, it brings out another layer of potential in humans in much the same way agriculture, industrialization, and electronics—our previous paradigmshifting technologies—have. Hopefully, because of the unique partnership to be formed between AI, and those who grow up native to its wonders and its perils, those of us who pass before this world is born will have laid the groundwork for a better world for our children.